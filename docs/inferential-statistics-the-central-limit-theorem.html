<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Inferential statistics: The Central Limit Theorem | Advanced Statistics Remix</title>
  <meta name="description" content="A textbook for advanced statistics" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Inferential statistics: The Central Limit Theorem | Advanced Statistics Remix" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="A textbook for advanced statistics" />
  <meta name="github-repo" content="vectrlab/stat-course-pack" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Inferential statistics: The Central Limit Theorem | Advanced Statistics Remix" />
  
  <meta name="twitter:description" content="A textbook for advanced statistics" />
  

<meta name="author" content="David Schuster" />


<meta name="date" content="2021-11-09" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="descriptives.html"/>
<link rel="next" href="hypothesistesting.html"/>
<script src="libs/header-attrs-2.10/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Course Pack for Statistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About this Book</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#attribution"><i class="fa fa-check"></i>Attribution</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i>License</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="statistics-for-research.html"><a href="statistics-for-research.html"><i class="fa fa-check"></i><b>1</b> Statistics for Research</a>
<ul>
<li class="chapter" data-level="1.1" data-path="statistics-for-research.html"><a href="statistics-for-research.html#introduction"><i class="fa fa-check"></i><b>1.1</b> Introduction</a></li>
<li class="chapter" data-level="1.2" data-path="statistics-for-research.html"><a href="statistics-for-research.html#measurement"><i class="fa fa-check"></i><b>1.2</b> Measurement</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="statistics-for-research.html"><a href="statistics-for-research.html#level-of-measurement"><i class="fa fa-check"></i><b>1.2.1</b> Level of Measurement</a></li>
<li class="chapter" data-level="1.2.2" data-path="statistics-for-research.html"><a href="statistics-for-research.html#continuous-or-discrete"><i class="fa fa-check"></i><b>1.2.2</b> Continuous or Discrete</a></li>
<li class="chapter" data-level="1.2.3" data-path="statistics-for-research.html"><a href="statistics-for-research.html#qualitative-or-quantitative"><i class="fa fa-check"></i><b>1.2.3</b> Qualitative or Quantitative</a></li>
<li class="chapter" data-level="1.2.4" data-path="statistics-for-research.html"><a href="statistics-for-research.html#distribution-a-collection-of-our-observations"><i class="fa fa-check"></i><b>1.2.4</b> Distribution: A collection of our observations</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="statistics-for-research.html"><a href="statistics-for-research.html#descriptive-statistics-summarizing-our-observations"><i class="fa fa-check"></i><b>1.3</b> Descriptive Statistics: Summarizing our observations</a></li>
<li class="chapter" data-level="1.4" data-path="statistics-for-research.html"><a href="statistics-for-research.html#inferential-statistics-generalizing-from-our-observations"><i class="fa fa-check"></i><b>1.4</b> Inferential Statistics: Generalizing from our observations</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="statistics-for-research.html"><a href="statistics-for-research.html#populations-and-samples-who-or-what-the-research-is-about"><i class="fa fa-check"></i><b>1.4.1</b> Populations and Samples: Who (or what) the research is about</a></li>
<li class="chapter" data-level="1.4.2" data-path="statistics-for-research.html"><a href="statistics-for-research.html#constructs-provide-the-context"><i class="fa fa-check"></i><b>1.4.2</b> Constructs provide the context</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="statistics-for-research.html"><a href="statistics-for-research.html#the-cautionary-tale-of-simpsons-paradox"><i class="fa fa-check"></i><b>1.5</b> The cautionary tale of Simpson’s paradox</a></li>
<li class="chapter" data-level="1.6" data-path="statistics-for-research.html"><a href="statistics-for-research.html#studydesign"><i class="fa fa-check"></i><b>1.6</b> A brief introduction to research design</a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="statistics-for-research.html"><a href="statistics-for-research.html#some-thoughts-about-psychological-measurement"><i class="fa fa-check"></i><b>1.6.1</b> Some thoughts about psychological measurement</a></li>
<li class="chapter" data-level="1.6.2" data-path="statistics-for-research.html"><a href="statistics-for-research.html#operationalisation-defining-your-measurement"><i class="fa fa-check"></i><b>1.6.2</b> Operationalisation: defining your measurement</a></li>
<li class="chapter" data-level="1.6.3" data-path="statistics-for-research.html"><a href="statistics-for-research.html#ivdv"><i class="fa fa-check"></i><b>1.6.3</b> The “role” of variables: predictors and outcomes</a></li>
<li class="chapter" data-level="1.6.4" data-path="statistics-for-research.html"><a href="statistics-for-research.html#researchdesigns"><i class="fa fa-check"></i><b>1.6.4</b> Experimental and non-experimental research</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="statistics-for-research.html"><a href="statistics-for-research.html#causality-research-and-statistics"><i class="fa fa-check"></i><b>1.7</b> Causality, Research, and Statistics</a>
<ul>
<li class="chapter" data-level="1.7.1" data-path="statistics-for-research.html"><a href="statistics-for-research.html#experimental-quasi-experimental-and-non-experimental-studies"><i class="fa fa-check"></i><b>1.7.1</b> Experimental, Quasi-Experimental, and Non-Experimental Studies</a></li>
<li class="chapter" data-level="1.7.2" data-path="statistics-for-research.html"><a href="statistics-for-research.html#demonstrating-causality"><i class="fa fa-check"></i><b>1.7.2</b> Demonstrating Causality</a></li>
<li class="chapter" data-level="1.7.3" data-path="statistics-for-research.html"><a href="statistics-for-research.html#statistics-and-causality"><i class="fa fa-check"></i><b>1.7.3</b> Statistics and Causality</a></li>
<li class="chapter" data-level="1.7.4" data-path="statistics-for-research.html"><a href="statistics-for-research.html#validity-and-reliability"><i class="fa fa-check"></i><b>1.7.4</b> Validity and Reliability</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introR.html"><a href="introR.html"><i class="fa fa-check"></i><b>2</b> Getting started with R</a>
<ul>
<li class="chapter" data-level="2.1" data-path="introR.html"><a href="introR.html#videos"><i class="fa fa-check"></i><b>2.1</b> Videos</a></li>
<li class="chapter" data-level="2.2" data-path="introR.html"><a href="introR.html#introduction-1"><i class="fa fa-check"></i><b>2.2</b> Introduction</a></li>
<li class="chapter" data-level="2.3" data-path="introR.html"><a href="introR.html#gettingR"><i class="fa fa-check"></i><b>2.3</b> Installing R</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="introR.html"><a href="introR.html#installing-r-on-a-windows-computer"><i class="fa fa-check"></i><b>2.3.1</b> Installing R on a Windows computer</a></li>
<li class="chapter" data-level="2.3.2" data-path="introR.html"><a href="introR.html#installing-r-on-a-mac"><i class="fa fa-check"></i><b>2.3.2</b> Installing R on a Mac</a></li>
<li class="chapter" data-level="2.3.3" data-path="introR.html"><a href="introR.html#installing-r-on-a-linux-computer"><i class="fa fa-check"></i><b>2.3.3</b> Installing R on a Linux computer</a></li>
<li class="chapter" data-level="2.3.4" data-path="introR.html"><a href="introR.html#installingrstudio"><i class="fa fa-check"></i><b>2.3.4</b> Downloading and installing RStudio</a></li>
<li class="chapter" data-level="2.3.5" data-path="introR.html"><a href="introR.html#startingR"><i class="fa fa-check"></i><b>2.3.5</b> Starting up R</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="introR.html"><a href="introR.html#firstcommand"><i class="fa fa-check"></i><b>2.4</b> Typing commands at the R console</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="introR.html"><a href="introR.html#an-important-digression-about-formatting"><i class="fa fa-check"></i><b>2.4.1</b> An important digression about formatting</a></li>
<li class="chapter" data-level="2.4.2" data-path="introR.html"><a href="introR.html#be-very-careful-to-avoid-typos"><i class="fa fa-check"></i><b>2.4.2</b> Be very careful to avoid typos</a></li>
<li class="chapter" data-level="2.4.3" data-path="introR.html"><a href="introR.html#r-is-a-bit-flexible-with-spacing"><i class="fa fa-check"></i><b>2.4.3</b> R is (a bit) flexible with spacing</a></li>
<li class="chapter" data-level="2.4.4" data-path="introR.html"><a href="introR.html#r-can-sometimes-tell-that-youre-not-finished-yet-but-not-often"><i class="fa fa-check"></i><b>2.4.4</b> R can sometimes tell that you’re not finished yet (but not often)</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="introR.html"><a href="introR.html#arithmetic"><i class="fa fa-check"></i><b>2.5</b> Doing simple calculations with R</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="introR.html"><a href="introR.html#adding-subtracting-multiplying-and-dividing"><i class="fa fa-check"></i><b>2.5.1</b> Adding, subtracting, multiplying and dividing</a></li>
<li class="chapter" data-level="2.5.2" data-path="introR.html"><a href="introR.html#taking-powers"><i class="fa fa-check"></i><b>2.5.2</b> Taking powers</a></li>
<li class="chapter" data-level="2.5.3" data-path="introR.html"><a href="introR.html#bedmas"><i class="fa fa-check"></i><b>2.5.3</b> Doing calculations in the right order</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="introR.html"><a href="introR.html#assign"><i class="fa fa-check"></i><b>2.6</b> Storing a number as a variable</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="introR.html"><a href="introR.html#variable-assignment-using---and--"><i class="fa fa-check"></i><b>2.6.1</b> Variable assignment using <code>&lt;-</code> and <code>-&gt;</code></a></li>
<li class="chapter" data-level="2.6.2" data-path="introR.html"><a href="introR.html#doing-calculations-using-variables"><i class="fa fa-check"></i><b>2.6.2</b> Doing calculations using variables</a></li>
<li class="chapter" data-level="2.6.3" data-path="introR.html"><a href="introR.html#rules-and-conventions-for-naming-variables"><i class="fa fa-check"></i><b>2.6.3</b> Rules and conventions for naming variables</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="introR.html"><a href="introR.html#usingfunctions"><i class="fa fa-check"></i><b>2.7</b> Using functions to do calculations</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="introR.html"><a href="introR.html#functionarguments"><i class="fa fa-check"></i><b>2.7.1</b> Function arguments, their names and their defaults</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="introR.html"><a href="introR.html#RStudio1"><i class="fa fa-check"></i><b>2.8</b> Letting RStudio help you with your commands</a>
<ul>
<li class="chapter" data-level="2.8.1" data-path="introR.html"><a href="introR.html#autocomplete-using-tab"><i class="fa fa-check"></i><b>2.8.1</b> Autocomplete using “tab”</a></li>
<li class="chapter" data-level="2.8.2" data-path="introR.html"><a href="introR.html#browsing-your-command-history"><i class="fa fa-check"></i><b>2.8.2</b> Browsing your command history</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="introR.html"><a href="introR.html#vectors"><i class="fa fa-check"></i><b>2.9</b> Storing many numbers as a vector</a>
<ul>
<li class="chapter" data-level="2.9.1" data-path="introR.html"><a href="introR.html#creating-a-vector"><i class="fa fa-check"></i><b>2.9.1</b> Creating a vector</a></li>
<li class="chapter" data-level="2.9.2" data-path="introR.html"><a href="introR.html#a-handy-digression"><i class="fa fa-check"></i><b>2.9.2</b> A handy digression</a></li>
<li class="chapter" data-level="2.9.3" data-path="introR.html"><a href="introR.html#vectorsubset"><i class="fa fa-check"></i><b>2.9.3</b> Getting information out of vectors</a></li>
<li class="chapter" data-level="2.9.4" data-path="introR.html"><a href="introR.html#altering-the-elements-of-a-vector"><i class="fa fa-check"></i><b>2.9.4</b> Altering the elements of a vector</a></li>
<li class="chapter" data-level="2.9.5" data-path="introR.html"><a href="introR.html#veclength"><i class="fa fa-check"></i><b>2.9.5</b> Useful things to know about vectors</a></li>
</ul></li>
<li class="chapter" data-level="2.10" data-path="introR.html"><a href="introR.html#text"><i class="fa fa-check"></i><b>2.10</b> Storing text data</a>
<ul>
<li class="chapter" data-level="2.10.1" data-path="introR.html"><a href="introR.html#simpletext"><i class="fa fa-check"></i><b>2.10.1</b> Working with text</a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="introR.html"><a href="introR.html#logicals"><i class="fa fa-check"></i><b>2.11</b> Storing “true or false” data</a>
<ul>
<li class="chapter" data-level="2.11.1" data-path="introR.html"><a href="introR.html#assessing-mathematical-truths"><i class="fa fa-check"></i><b>2.11.1</b> Assessing mathematical truths</a></li>
<li class="chapter" data-level="2.11.2" data-path="introR.html"><a href="introR.html#logical-operations"><i class="fa fa-check"></i><b>2.11.2</b> Logical operations</a></li>
<li class="chapter" data-level="2.11.3" data-path="introR.html"><a href="introR.html#storing-and-using-logical-data"><i class="fa fa-check"></i><b>2.11.3</b> Storing and using logical data</a></li>
<li class="chapter" data-level="2.11.4" data-path="introR.html"><a href="introR.html#vectors-of-logicals"><i class="fa fa-check"></i><b>2.11.4</b> Vectors of logicals</a></li>
<li class="chapter" data-level="2.11.5" data-path="introR.html"><a href="introR.html#logictext"><i class="fa fa-check"></i><b>2.11.5</b> Applying logical operation to text</a></li>
</ul></li>
<li class="chapter" data-level="2.12" data-path="introR.html"><a href="introR.html#indexing"><i class="fa fa-check"></i><b>2.12</b> Indexing vectors</a>
<ul>
<li class="chapter" data-level="2.12.1" data-path="introR.html"><a href="introR.html#extracting-multiple-elements"><i class="fa fa-check"></i><b>2.12.1</b> Extracting multiple elements</a></li>
<li class="chapter" data-level="2.12.2" data-path="introR.html"><a href="introR.html#logical-indexing"><i class="fa fa-check"></i><b>2.12.2</b> Logical indexing</a></li>
</ul></li>
<li class="chapter" data-level="2.13" data-path="introR.html"><a href="introR.html#quitting-r"><i class="fa fa-check"></i><b>2.13</b> Quitting R</a></li>
<li class="chapter" data-level="2.14" data-path="introR.html"><a href="introR.html#summary"><i class="fa fa-check"></i><b>2.14</b> Summary</a></li>
<li class="chapter" data-level="2.15" data-path="introR.html"><a href="introR.html#mechanics"><i class="fa fa-check"></i><b>2.15</b> Additional R concepts</a></li>
<li class="chapter" data-level="2.16" data-path="introR.html"><a href="introR.html#comments"><i class="fa fa-check"></i><b>2.16</b> Using comments</a></li>
<li class="chapter" data-level="2.17" data-path="introR.html"><a href="introR.html#packageinstall"><i class="fa fa-check"></i><b>2.17</b> Installing and loading packages</a>
<ul>
<li class="chapter" data-level="2.17.1" data-path="introR.html"><a href="introR.html#the-package-panel-in-rstudio"><i class="fa fa-check"></i><b>2.17.1</b> The package panel in RStudio</a></li>
<li class="chapter" data-level="2.17.2" data-path="introR.html"><a href="introR.html#packageload"><i class="fa fa-check"></i><b>2.17.2</b> Loading a package</a></li>
<li class="chapter" data-level="2.17.3" data-path="introR.html"><a href="introR.html#packageunload"><i class="fa fa-check"></i><b>2.17.3</b> Unloading a package</a></li>
<li class="chapter" data-level="2.17.4" data-path="introR.html"><a href="introR.html#a-few-extra-comments"><i class="fa fa-check"></i><b>2.17.4</b> A few extra comments</a></li>
<li class="chapter" data-level="2.17.5" data-path="introR.html"><a href="introR.html#downloading-new-packages"><i class="fa fa-check"></i><b>2.17.5</b> Downloading new packages</a></li>
<li class="chapter" data-level="2.17.6" data-path="introR.html"><a href="introR.html#updating-r-and-r-packages"><i class="fa fa-check"></i><b>2.17.6</b> Updating R and R packages</a></li>
<li class="chapter" data-level="2.17.7" data-path="introR.html"><a href="introR.html#what-packages-does-this-book-use"><i class="fa fa-check"></i><b>2.17.7</b> What packages does this book use?</a></li>
</ul></li>
<li class="chapter" data-level="2.18" data-path="introR.html"><a href="introR.html#workspace"><i class="fa fa-check"></i><b>2.18</b> Managing the workspace</a>
<ul>
<li class="chapter" data-level="2.18.1" data-path="introR.html"><a href="introR.html#listing-the-contents-of-the-workspace"><i class="fa fa-check"></i><b>2.18.1</b> Listing the contents of the workspace</a></li>
<li class="chapter" data-level="2.18.2" data-path="introR.html"><a href="introR.html#removing-variables-from-the-workspace"><i class="fa fa-check"></i><b>2.18.2</b> Removing variables from the workspace</a></li>
</ul></li>
<li class="chapter" data-level="2.19" data-path="introR.html"><a href="introR.html#navigation"><i class="fa fa-check"></i><b>2.19</b> Navigating the file system</a>
<ul>
<li class="chapter" data-level="2.19.1" data-path="introR.html"><a href="introR.html#filesystem"><i class="fa fa-check"></i><b>2.19.1</b> The file system itself</a></li>
<li class="chapter" data-level="2.19.2" data-path="introR.html"><a href="introR.html#navigationR"><i class="fa fa-check"></i><b>2.19.2</b> Navigating the file system using the R console</a></li>
<li class="chapter" data-level="2.19.3" data-path="introR.html"><a href="introR.html#why-do-the-windows-paths-use-the-wrong-slash"><i class="fa fa-check"></i><b>2.19.3</b> Why do the Windows paths use the wrong slash?</a></li>
<li class="chapter" data-level="2.19.4" data-path="introR.html"><a href="introR.html#nav3"><i class="fa fa-check"></i><b>2.19.4</b> Navigating the file system using the RStudio file panel</a></li>
</ul></li>
<li class="chapter" data-level="2.20" data-path="introR.html"><a href="introR.html#load"><i class="fa fa-check"></i><b>2.20</b> Loading and saving data</a>
<ul>
<li class="chapter" data-level="2.20.1" data-path="introR.html"><a href="introR.html#loading-workspace-files-using-r"><i class="fa fa-check"></i><b>2.20.1</b> Loading workspace files using R</a></li>
<li class="chapter" data-level="2.20.2" data-path="introR.html"><a href="introR.html#loading-workspace-files-using-rstudio"><i class="fa fa-check"></i><b>2.20.2</b> Loading workspace files using RStudio</a></li>
<li class="chapter" data-level="2.20.3" data-path="introR.html"><a href="introR.html#loadingcsv"><i class="fa fa-check"></i><b>2.20.3</b> Importing data from CSV files using loadingcsv</a></li>
<li class="chapter" data-level="2.20.4" data-path="introR.html"><a href="introR.html#importing-data-from-csv-files-using-rstudio"><i class="fa fa-check"></i><b>2.20.4</b> Importing data from CSV files using RStudio</a></li>
<li class="chapter" data-level="2.20.5" data-path="introR.html"><a href="introR.html#saving-a-workspace-file-using-save"><i class="fa fa-check"></i><b>2.20.5</b> Saving a workspace file using <code>save</code></a></li>
<li class="chapter" data-level="2.20.6" data-path="introR.html"><a href="introR.html#save1"><i class="fa fa-check"></i><b>2.20.6</b> Saving a workspace file using RStudio</a></li>
<li class="chapter" data-level="2.20.7" data-path="introR.html"><a href="introR.html#other-things-you-might-want-to-save"><i class="fa fa-check"></i><b>2.20.7</b> Other things you might want to save</a></li>
</ul></li>
<li class="chapter" data-level="2.21" data-path="introR.html"><a href="introR.html#useful"><i class="fa fa-check"></i><b>2.21</b> Useful things to know about variables</a>
<ul>
<li class="chapter" data-level="2.21.1" data-path="introR.html"><a href="introR.html#specials"><i class="fa fa-check"></i><b>2.21.1</b> Special values</a></li>
<li class="chapter" data-level="2.21.2" data-path="introR.html"><a href="introR.html#names"><i class="fa fa-check"></i><b>2.21.2</b> Assigning names to vector elements</a></li>
<li class="chapter" data-level="2.21.3" data-path="introR.html"><a href="introR.html#variable-classes"><i class="fa fa-check"></i><b>2.21.3</b> Variable classes</a></li>
</ul></li>
<li class="chapter" data-level="2.22" data-path="introR.html"><a href="introR.html#factors"><i class="fa fa-check"></i><b>2.22</b> Factors</a>
<ul>
<li class="chapter" data-level="2.22.1" data-path="introR.html"><a href="introR.html#introducing-factors"><i class="fa fa-check"></i><b>2.22.1</b> Introducing factors</a></li>
<li class="chapter" data-level="2.22.2" data-path="introR.html"><a href="introR.html#labelling-the-factor-levels"><i class="fa fa-check"></i><b>2.22.2</b> Labelling the factor levels</a></li>
<li class="chapter" data-level="2.22.3" data-path="introR.html"><a href="introR.html#moving-on"><i class="fa fa-check"></i><b>2.22.3</b> Moving on…</a></li>
</ul></li>
<li class="chapter" data-level="2.23" data-path="introR.html"><a href="introR.html#dataframes"><i class="fa fa-check"></i><b>2.23</b> Data frames</a>
<ul>
<li class="chapter" data-level="2.23.1" data-path="introR.html"><a href="introR.html#introducing-data-frames"><i class="fa fa-check"></i><b>2.23.1</b> Introducing data frames</a></li>
<li class="chapter" data-level="2.23.2" data-path="introR.html"><a href="introR.html#pulling-out-the-contents-of-the-data-frame-using"><i class="fa fa-check"></i><b>2.23.2</b> Pulling out the contents of the data frame using <code>$</code></a></li>
<li class="chapter" data-level="2.23.3" data-path="introR.html"><a href="introR.html#getting-information-about-a-data-frame"><i class="fa fa-check"></i><b>2.23.3</b> Getting information about a data frame</a></li>
<li class="chapter" data-level="2.23.4" data-path="introR.html"><a href="introR.html#looking-for-more-on-data-frames"><i class="fa fa-check"></i><b>2.23.4</b> Looking for more on data frames?</a></li>
</ul></li>
<li class="chapter" data-level="2.24" data-path="introR.html"><a href="introR.html#lists"><i class="fa fa-check"></i><b>2.24</b> Lists</a></li>
<li class="chapter" data-level="2.25" data-path="introR.html"><a href="introR.html#formulas"><i class="fa fa-check"></i><b>2.25</b> Formulas</a></li>
<li class="chapter" data-level="2.26" data-path="introR.html"><a href="introR.html#generics"><i class="fa fa-check"></i><b>2.26</b> Generic functions</a></li>
<li class="chapter" data-level="2.27" data-path="introR.html"><a href="introR.html#help"><i class="fa fa-check"></i><b>2.27</b> Getting help</a>
<ul>
<li class="chapter" data-level="2.27.1" data-path="introR.html"><a href="introR.html#how-to-read-the-help-documentation"><i class="fa fa-check"></i><b>2.27.1</b> How to read the help documentation</a></li>
<li class="chapter" data-level="2.27.2" data-path="introR.html"><a href="introR.html#other-resources"><i class="fa fa-check"></i><b>2.27.2</b> Other resources</a></li>
</ul></li>
<li class="chapter" data-level="2.28" data-path="introR.html"><a href="introR.html#summary-1"><i class="fa fa-check"></i><b>2.28</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="descriptives.html"><a href="descriptives.html"><i class="fa fa-check"></i><b>3</b> Descriptive statistics</a>
<ul>
<li class="chapter" data-level="3.1" data-path="descriptives.html"><a href="descriptives.html#videos-1"><i class="fa fa-check"></i><b>3.1</b> Videos</a></li>
<li class="chapter" data-level="3.2" data-path="descriptives.html"><a href="descriptives.html#introduction-2"><i class="fa fa-check"></i><b>3.2</b> Introduction</a></li>
<li class="chapter" data-level="3.3" data-path="descriptives.html"><a href="descriptives.html#centraltendency"><i class="fa fa-check"></i><b>3.3</b> Measures of central tendency</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="descriptives.html"><a href="descriptives.html#mean"><i class="fa fa-check"></i><b>3.3.1</b> The mean</a></li>
<li class="chapter" data-level="3.3.2" data-path="descriptives.html"><a href="descriptives.html#calculating-the-mean-in-r"><i class="fa fa-check"></i><b>3.3.2</b> Calculating the mean in R</a></li>
<li class="chapter" data-level="3.3.3" data-path="descriptives.html"><a href="descriptives.html#median"><i class="fa fa-check"></i><b>3.3.3</b> The median</a></li>
<li class="chapter" data-level="3.3.4" data-path="descriptives.html"><a href="descriptives.html#mean-or-median-whats-the-difference"><i class="fa fa-check"></i><b>3.3.4</b> Mean or median? What’s the difference?</a></li>
<li class="chapter" data-level="3.3.5" data-path="descriptives.html"><a href="descriptives.html#housingpriceexample"><i class="fa fa-check"></i><b>3.3.5</b> A real life example</a></li>
<li class="chapter" data-level="3.3.6" data-path="descriptives.html"><a href="descriptives.html#trimmedmean"><i class="fa fa-check"></i><b>3.3.6</b> Trimmed mean</a></li>
<li class="chapter" data-level="3.3.7" data-path="descriptives.html"><a href="descriptives.html#mode"><i class="fa fa-check"></i><b>3.3.7</b> Mode</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="descriptives.html"><a href="descriptives.html#var"><i class="fa fa-check"></i><b>3.4</b> Measures of variability</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="descriptives.html"><a href="descriptives.html#range"><i class="fa fa-check"></i><b>3.4.1</b> Range</a></li>
<li class="chapter" data-level="3.4.2" data-path="descriptives.html"><a href="descriptives.html#interquartile-range"><i class="fa fa-check"></i><b>3.4.2</b> Interquartile range</a></li>
<li class="chapter" data-level="3.4.3" data-path="descriptives.html"><a href="descriptives.html#aad"><i class="fa fa-check"></i><b>3.4.3</b> Mean absolute deviation</a></li>
<li class="chapter" data-level="3.4.4" data-path="descriptives.html"><a href="descriptives.html#variance"><i class="fa fa-check"></i><b>3.4.4</b> Variance</a></li>
<li class="chapter" data-level="3.4.5" data-path="descriptives.html"><a href="descriptives.html#sd"><i class="fa fa-check"></i><b>3.4.5</b> Standard deviation</a></li>
<li class="chapter" data-level="3.4.6" data-path="descriptives.html"><a href="descriptives.html#mad"><i class="fa fa-check"></i><b>3.4.6</b> Median absolute deviation</a></li>
<li class="chapter" data-level="3.4.7" data-path="descriptives.html"><a href="descriptives.html#which-measure-to-use"><i class="fa fa-check"></i><b>3.4.7</b> Which measure to use?</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="descriptives.html"><a href="descriptives.html#skewandkurtosis"><i class="fa fa-check"></i><b>3.5</b> Skew and kurtosis</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="descriptives.html"><a href="descriptives.html#more-detail-on-skewness-measures"><i class="fa fa-check"></i><b>3.5.1</b> More detail on skewness measures</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="descriptives.html"><a href="descriptives.html#descriptive-summary"><i class="fa fa-check"></i><b>3.6</b> Getting an overall summary of a variable</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="descriptives.html"><a href="descriptives.html#summarising-a-variable"><i class="fa fa-check"></i><b>3.6.1</b> “Summarising” a variable</a></li>
<li class="chapter" data-level="3.6.2" data-path="descriptives.html"><a href="descriptives.html#summarising-a-data-frame"><i class="fa fa-check"></i><b>3.6.2</b> “Summarising” a data frame</a></li>
<li class="chapter" data-level="3.6.3" data-path="descriptives.html"><a href="descriptives.html#describing-a-data-frame"><i class="fa fa-check"></i><b>3.6.3</b> “Describing” a data frame</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="descriptives.html"><a href="descriptives.html#groupdescriptives"><i class="fa fa-check"></i><b>3.7</b> Descriptive statistics separately for each group</a></li>
<li class="chapter" data-level="3.8" data-path="descriptives.html"><a href="descriptives.html#good-descriptive-statistics-are-descriptive"><i class="fa fa-check"></i><b>3.8</b> Good descriptive statistics are descriptive!</a></li>
<li class="chapter" data-level="3.9" data-path="descriptives.html"><a href="descriptives.html#graphics"><i class="fa fa-check"></i><b>3.9</b> Drawing graphs</a>
<ul>
<li class="chapter" data-level="3.9.1" data-path="descriptives.html"><a href="descriptives.html#introplotting"><i class="fa fa-check"></i><b>3.9.1</b> An introduction to plotting</a></li>
<li class="chapter" data-level="3.9.2" data-path="descriptives.html"><a href="descriptives.html#hist"><i class="fa fa-check"></i><b>3.9.2</b> Histograms</a></li>
<li class="chapter" data-level="3.9.3" data-path="descriptives.html"><a href="descriptives.html#boxplots"><i class="fa fa-check"></i><b>3.9.3</b> Boxplots</a></li>
<li class="chapter" data-level="3.9.4" data-path="descriptives.html"><a href="descriptives.html#bargraph"><i class="fa fa-check"></i><b>3.9.4</b> Bar graphs</a></li>
<li class="chapter" data-level="3.9.5" data-path="descriptives.html"><a href="descriptives.html#saveimage"><i class="fa fa-check"></i><b>3.9.5</b> Saving image files using R and Rstudio</a></li>
<li class="chapter" data-level="3.9.6" data-path="descriptives.html"><a href="descriptives.html#summary-2"><i class="fa fa-check"></i><b>3.9.6</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html"><i class="fa fa-check"></i><b>4</b> Inferential statistics: The Central Limit Theorem</a>
<ul>
<li class="chapter" data-level="4.1" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#videos-2"><i class="fa fa-check"></i><b>4.1</b> Videos</a></li>
<li class="chapter" data-level="4.2" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#estimation"><i class="fa fa-check"></i><b>4.2</b> Introduction</a></li>
<li class="chapter" data-level="4.3" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#probstats"><i class="fa fa-check"></i><b>4.3</b> How are probability and statistics different?</a></li>
<li class="chapter" data-level="4.4" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#probmeaning"><i class="fa fa-check"></i><b>4.4</b> What does probability mean?</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#the-frequentist-view"><i class="fa fa-check"></i><b>4.4.1</b> The frequentist view</a></li>
<li class="chapter" data-level="4.4.2" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#the-bayesian-view"><i class="fa fa-check"></i><b>4.4.2</b> The Bayesian view</a></li>
<li class="chapter" data-level="4.4.3" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#whats-the-difference-and-who-is-right"><i class="fa fa-check"></i><b>4.4.3</b> What’s the difference? And who is right?</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#srs"><i class="fa fa-check"></i><b>4.5</b> Samples, populations and sampling</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#pop"><i class="fa fa-check"></i><b>4.5.1</b> Defining a population</a></li>
<li class="chapter" data-level="4.5.2" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#simple-random-samples"><i class="fa fa-check"></i><b>4.5.2</b> Simple random samples</a></li>
<li class="chapter" data-level="4.5.3" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#most-samples-are-not-simple-random-samples"><i class="fa fa-check"></i><b>4.5.3</b> Most samples are not simple random samples</a></li>
<li class="chapter" data-level="4.5.4" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#how-much-does-it-matter-if-you-dont-have-a-simple-random-sample"><i class="fa fa-check"></i><b>4.5.4</b> How much does it matter if you don’t have a simple random sample?</a></li>
<li class="chapter" data-level="4.5.5" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#population-parameters-and-sample-statistics"><i class="fa fa-check"></i><b>4.5.5</b> Population parameters and sample statistics</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#lawlargenumbers"><i class="fa fa-check"></i><b>4.6</b> The law of large numbers</a></li>
<li class="chapter" data-level="4.7" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#samplesandclt"><i class="fa fa-check"></i><b>4.7</b> Sampling distributions and the central limit theorem</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#samplingdists"><i class="fa fa-check"></i><b>4.7.1</b> Sampling distribution of the mean</a></li>
<li class="chapter" data-level="4.7.2" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#sample-size-and-population-size"><i class="fa fa-check"></i><b>4.7.2</b> Sample size and population size</a></li>
<li class="chapter" data-level="4.7.3" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#sampling-error"><i class="fa fa-check"></i><b>4.7.3</b> Sampling error</a></li>
<li class="chapter" data-level="4.7.4" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#another-sampling-distribution"><i class="fa fa-check"></i><b>4.7.4</b> Another sampling distribution</a></li>
<li class="chapter" data-level="4.7.5" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#defining-the-central-limit-theorem"><i class="fa fa-check"></i><b>4.7.5</b> Defining the central limit theorem</a></li>
<li class="chapter" data-level="4.7.6" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#more-on-standard-error"><i class="fa fa-check"></i><b>4.7.6</b> More on standard error</a></li>
<li class="chapter" data-level="4.7.7" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#the-sampling-distribution-tells-us-about-the-probability-of-sample-means"><i class="fa fa-check"></i><b>4.7.7</b> The sampling distribution tells us about the probability of sample means</a></li>
<li class="chapter" data-level="4.7.8" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#sampling-distributions-exist-for-any-sample-statistic"><i class="fa fa-check"></i><b>4.7.8</b> Sampling distributions exist for any sample statistic!</a></li>
<li class="chapter" data-level="4.7.9" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#clt"><i class="fa fa-check"></i><b>4.7.9</b> The central limit theorem</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#pointestimates"><i class="fa fa-check"></i><b>4.8</b> Estimating population parameters</a>
<ul>
<li class="chapter" data-level="4.8.1" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#estimating-the-population-mean"><i class="fa fa-check"></i><b>4.8.1</b> Estimating the population mean</a></li>
<li class="chapter" data-level="4.8.2" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#estimating-the-population-standard-deviation"><i class="fa fa-check"></i><b>4.8.2</b> Estimating the population standard deviation</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#ci"><i class="fa fa-check"></i><b>4.9</b> Estimating a confidence interval</a>
<ul>
<li class="chapter" data-level="4.9.1" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#a-slight-mistake-in-the-formula"><i class="fa fa-check"></i><b>4.9.1</b> A slight mistake in the formula</a></li>
<li class="chapter" data-level="4.9.2" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#interpreting-a-confidence-interval"><i class="fa fa-check"></i><b>4.9.2</b> Interpreting a confidence interval</a></li>
<li class="chapter" data-level="4.9.3" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#calculating-confidence-intervals-in-r"><i class="fa fa-check"></i><b>4.9.3</b> Calculating confidence intervals in R</a></li>
<li class="chapter" data-level="4.9.4" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#ciplots"><i class="fa fa-check"></i><b>4.9.4</b> Plotting confidence intervals in R</a></li>
</ul></li>
<li class="chapter" data-level="4.10" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#summary-3"><i class="fa fa-check"></i><b>4.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="hypothesistesting.html"><a href="hypothesistesting.html"><i class="fa fa-check"></i><b>5</b> Hypothesis testing</a>
<ul>
<li class="chapter" data-level="5.1" data-path="hypothesistesting.html"><a href="hypothesistesting.html#videos-3"><i class="fa fa-check"></i><b>5.1</b> Videos</a></li>
<li class="chapter" data-level="5.2" data-path="hypothesistesting.html"><a href="hypothesistesting.html#introduction-3"><i class="fa fa-check"></i><b>5.2</b> Introduction</a></li>
<li class="chapter" data-level="5.3" data-path="hypothesistesting.html"><a href="hypothesistesting.html#hypotheses"><i class="fa fa-check"></i><b>5.3</b> A menagerie of hypotheses</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="hypothesistesting.html"><a href="hypothesistesting.html#research-hypotheses-versus-statistical-hypotheses"><i class="fa fa-check"></i><b>5.3.1</b> Research hypotheses versus statistical hypotheses</a></li>
<li class="chapter" data-level="5.3.2" data-path="hypothesistesting.html"><a href="hypothesistesting.html#null-hypotheses-and-alternative-hypotheses"><i class="fa fa-check"></i><b>5.3.2</b> Null hypotheses and alternative hypotheses</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="hypothesistesting.html"><a href="hypothesistesting.html#errortypes"><i class="fa fa-check"></i><b>5.4</b> Two types of errors</a></li>
<li class="chapter" data-level="5.5" data-path="hypothesistesting.html"><a href="hypothesistesting.html#teststatistics"><i class="fa fa-check"></i><b>5.5</b> Test statistics and sampling distributions</a></li>
<li class="chapter" data-level="5.6" data-path="hypothesistesting.html"><a href="hypothesistesting.html#decisionmaking"><i class="fa fa-check"></i><b>5.6</b> Making decisions</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="hypothesistesting.html"><a href="hypothesistesting.html#critical-regions-and-critical-values"><i class="fa fa-check"></i><b>5.6.1</b> Critical regions and critical values</a></li>
<li class="chapter" data-level="5.6.2" data-path="hypothesistesting.html"><a href="hypothesistesting.html#a-note-on-statistical-significance"><i class="fa fa-check"></i><b>5.6.2</b> A note on statistical “significance”</a></li>
<li class="chapter" data-level="5.6.3" data-path="hypothesistesting.html"><a href="hypothesistesting.html#onesidedtests"><i class="fa fa-check"></i><b>5.6.3</b> The difference between one sided and two sided tests</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="hypothesistesting.html"><a href="hypothesistesting.html#pvalue"><i class="fa fa-check"></i><b>5.7</b> The <span class="math inline">\(p\)</span> value of a test</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="hypothesistesting.html"><a href="hypothesistesting.html#a-softer-view-of-decision-making"><i class="fa fa-check"></i><b>5.7.1</b> A softer view of decision making</a></li>
<li class="chapter" data-level="5.7.2" data-path="hypothesistesting.html"><a href="hypothesistesting.html#the-probability-of-extreme-data"><i class="fa fa-check"></i><b>5.7.2</b> The probability of extreme data</a></li>
<li class="chapter" data-level="5.7.3" data-path="hypothesistesting.html"><a href="hypothesistesting.html#a-common-mistake"><i class="fa fa-check"></i><b>5.7.3</b> A common mistake</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="hypothesistesting.html"><a href="hypothesistesting.html#writeup"><i class="fa fa-check"></i><b>5.8</b> Reporting the results of a hypothesis test</a>
<ul>
<li class="chapter" data-level="5.8.1" data-path="hypothesistesting.html"><a href="hypothesistesting.html#the-issue"><i class="fa fa-check"></i><b>5.8.1</b> The issue</a></li>
<li class="chapter" data-level="5.8.2" data-path="hypothesistesting.html"><a href="hypothesistesting.html#two-proposed-solutions"><i class="fa fa-check"></i><b>5.8.2</b> Two proposed solutions</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="hypothesistesting.html"><a href="hypothesistesting.html#running-the-hypothesis-test-in-practice"><i class="fa fa-check"></i><b>5.9</b> Running the hypothesis test in practice</a></li>
<li class="chapter" data-level="5.10" data-path="hypothesistesting.html"><a href="hypothesistesting.html#effectsize"><i class="fa fa-check"></i><b>5.10</b> Effect size, sample size and power</a>
<ul>
<li class="chapter" data-level="5.10.1" data-path="hypothesistesting.html"><a href="hypothesistesting.html#the-power-function"><i class="fa fa-check"></i><b>5.10.1</b> The power function</a></li>
<li class="chapter" data-level="5.10.2" data-path="hypothesistesting.html"><a href="hypothesistesting.html#effect-size"><i class="fa fa-check"></i><b>5.10.2</b> Effect size</a></li>
<li class="chapter" data-level="5.10.3" data-path="hypothesistesting.html"><a href="hypothesistesting.html#increasing-the-power-of-your-study"><i class="fa fa-check"></i><b>5.10.3</b> Increasing the power of your study</a></li>
</ul></li>
<li class="chapter" data-level="5.11" data-path="hypothesistesting.html"><a href="hypothesistesting.html#nhstmess"><i class="fa fa-check"></i><b>5.11</b> Some issues to consider</a>
<ul>
<li class="chapter" data-level="5.11.1" data-path="hypothesistesting.html"><a href="hypothesistesting.html#neyman-versus-fisher"><i class="fa fa-check"></i><b>5.11.1</b> Neyman versus Fisher</a></li>
<li class="chapter" data-level="5.11.2" data-path="hypothesistesting.html"><a href="hypothesistesting.html#bayesians-versus-frequentists"><i class="fa fa-check"></i><b>5.11.2</b> Bayesians versus frequentists</a></li>
<li class="chapter" data-level="5.11.3" data-path="hypothesistesting.html"><a href="hypothesistesting.html#traps"><i class="fa fa-check"></i><b>5.11.3</b> Traps</a></li>
</ul></li>
<li class="chapter" data-level="5.12" data-path="hypothesistesting.html"><a href="hypothesistesting.html#summary-4"><i class="fa fa-check"></i><b>5.12</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="issues-in-hypothesis-testing.html"><a href="issues-in-hypothesis-testing.html"><i class="fa fa-check"></i><b>6</b> Issues in Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="6.1" data-path="issues-in-hypothesis-testing.html"><a href="issues-in-hypothesis-testing.html#videos-4"><i class="fa fa-check"></i><b>6.1</b> Videos</a></li>
<li class="chapter" data-level="6.2" data-path="issues-in-hypothesis-testing.html"><a href="issues-in-hypothesis-testing.html#introduction-4"><i class="fa fa-check"></i><b>6.2</b> Introduction</a></li>
<li class="chapter" data-level="6.3" data-path="issues-in-hypothesis-testing.html"><a href="issues-in-hypothesis-testing.html#the-researcher-affects-nhst-outcomes"><i class="fa fa-check"></i><b>6.3</b> The researcher affects NHST outcomes</a></li>
<li class="chapter" data-level="6.4" data-path="issues-in-hypothesis-testing.html"><a href="issues-in-hypothesis-testing.html#nhst-misunderstandings"><i class="fa fa-check"></i><b>6.4</b> NHST Misunderstandings</a></li>
<li class="chapter" data-level="6.5" data-path="issues-in-hypothesis-testing.html"><a href="issues-in-hypothesis-testing.html#nhst-issues"><i class="fa fa-check"></i><b>6.5</b> NHST Issues</a></li>
<li class="chapter" data-level="6.6" data-path="issues-in-hypothesis-testing.html"><a href="issues-in-hypothesis-testing.html#conclusions"><i class="fa fa-check"></i><b>6.6</b> Conclusions</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="data-cleaning-and-missing-values-analysis.html"><a href="data-cleaning-and-missing-values-analysis.html"><i class="fa fa-check"></i><b>7</b> Data Cleaning and Missing Values Analysis</a>
<ul>
<li class="chapter" data-level="7.1" data-path="data-cleaning-and-missing-values-analysis.html"><a href="data-cleaning-and-missing-values-analysis.html#videos-5"><i class="fa fa-check"></i><b>7.1</b> Videos</a></li>
<li class="chapter" data-level="7.2" data-path="data-cleaning-and-missing-values-analysis.html"><a href="data-cleaning-and-missing-values-analysis.html#introduction-dealing-with-the-unexpected"><i class="fa fa-check"></i><b>7.2</b> Introduction: Dealing with the Unexpected</a></li>
<li class="chapter" data-level="7.3" data-path="data-cleaning-and-missing-values-analysis.html"><a href="data-cleaning-and-missing-values-analysis.html#data-cleaning"><i class="fa fa-check"></i><b>7.3</b> Data Cleaning</a></li>
<li class="chapter" data-level="7.4" data-path="data-cleaning-and-missing-values-analysis.html"><a href="data-cleaning-and-missing-values-analysis.html#a-general-plan-for-data-cleaning"><i class="fa fa-check"></i><b>7.4</b> A General Plan for Data Cleaning</a></li>
<li class="chapter" data-level="7.5" data-path="data-cleaning-and-missing-values-analysis.html"><a href="data-cleaning-and-missing-values-analysis.html#step-0.-design-your-research-to-mimimize-data-problems"><i class="fa fa-check"></i><b>7.5</b> Step 0. Design your Research to Mimimize Data Problems</a></li>
<li class="chapter" data-level="7.6" data-path="data-cleaning-and-missing-values-analysis.html"><a href="data-cleaning-and-missing-values-analysis.html#step-1.-examine-your-data"><i class="fa fa-check"></i><b>7.6</b> Step 1. Examine Your Data</a>
<ul>
<li class="chapter" data-level="7.6.1" data-path="data-cleaning-and-missing-values-analysis.html"><a href="data-cleaning-and-missing-values-analysis.html#sort"><i class="fa fa-check"></i><b>7.6.1</b> Sorting, flipping and merging data</a></li>
<li class="chapter" data-level="7.6.2" data-path="data-cleaning-and-missing-values-analysis.html"><a href="data-cleaning-and-missing-values-analysis.html#binding-vectors-together"><i class="fa fa-check"></i><b>7.6.2</b> Binding vectors together</a></li>
<li class="chapter" data-level="7.6.3" data-path="data-cleaning-and-missing-values-analysis.html"><a href="data-cleaning-and-missing-values-analysis.html#reshape"><i class="fa fa-check"></i><b>7.6.3</b> Reshaping a data frame</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="data-cleaning-and-missing-values-analysis.html"><a href="data-cleaning-and-missing-values-analysis.html#step-2.-outlier-analysis"><i class="fa fa-check"></i><b>7.7</b> Step 2. Outlier Analysis</a></li>
<li class="chapter" data-level="7.8" data-path="data-cleaning-and-missing-values-analysis.html"><a href="data-cleaning-and-missing-values-analysis.html#step-3.-missing-values-analysis"><i class="fa fa-check"></i><b>7.8</b> Step 3. Missing values analysis</a>
<ul>
<li class="chapter" data-level="7.8.1" data-path="introR.html"><a href="introR.html#specials"><i class="fa fa-check"></i><b>7.8.1</b> Special values in R</a></li>
<li class="chapter" data-level="7.8.2" data-path="data-cleaning-and-missing-values-analysis.html"><a href="data-cleaning-and-missing-values-analysis.html#missing"><i class="fa fa-check"></i><b>7.8.2</b> Handling missing values in R</a></li>
<li class="chapter" data-level="7.8.3" data-path="data-cleaning-and-missing-values-analysis.html"><a href="data-cleaning-and-missing-values-analysis.html#why-values-are-missing-mcar-mar-and-mnar"><i class="fa fa-check"></i><b>7.8.3</b> Why values are missing: MCAR, MAR, and MNAR</a></li>
</ul></li>
<li class="chapter" data-level="7.9" data-path="data-cleaning-and-missing-values-analysis.html"><a href="data-cleaning-and-missing-values-analysis.html#step-4.-test-specific-assumption-checking"><i class="fa fa-check"></i><b>7.9</b> Step 4. Test-specific assumption checking</a></li>
<li class="chapter" data-level="7.10" data-path="data-cleaning-and-missing-values-analysis.html"><a href="data-cleaning-and-missing-values-analysis.html#communicate-results-of-data-cleaning-in-apa-style"><i class="fa fa-check"></i><b>7.10</b> Communicate results of data cleaning in APA style</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>8</b> Regression</a>
<ul>
<li class="chapter" data-level="8.1" data-path="regression.html"><a href="regression.html#videos-6"><i class="fa fa-check"></i><b>8.1</b> Videos</a></li>
<li class="chapter" data-level="8.2" data-path="regression.html"><a href="regression.html#introduction-5"><i class="fa fa-check"></i><b>8.2</b> Introduction</a></li>
<li class="chapter" data-level="8.3" data-path="regression.html"><a href="regression.html#the-general-linear-model-glm"><i class="fa fa-check"></i><b>8.3</b> The General Linear Model (GLM)</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="regression.html"><a href="regression.html#the-traditional-approach-two-kinds-of-parametric-statistical-tests"><i class="fa fa-check"></i><b>8.3.1</b> The Traditional approach: Two kinds of parametric statistical tests</a></li>
<li class="chapter" data-level="8.3.2" data-path="regression.html"><a href="regression.html#the-glm-approach"><i class="fa fa-check"></i><b>8.3.2</b> The GLM approach</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="regression.html"><a href="regression.html#correl"><i class="fa fa-check"></i><b>8.4</b> Correlations</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="regression.html"><a href="regression.html#the-data"><i class="fa fa-check"></i><b>8.4.1</b> The data</a></li>
<li class="chapter" data-level="8.4.2" data-path="regression.html"><a href="regression.html#the-strength-and-direction-of-a-relationship"><i class="fa fa-check"></i><b>8.4.2</b> The strength and direction of a relationship</a></li>
<li class="chapter" data-level="8.4.3" data-path="regression.html"><a href="regression.html#the-correlation-coefficient"><i class="fa fa-check"></i><b>8.4.3</b> The correlation coefficient</a></li>
<li class="chapter" data-level="8.4.4" data-path="regression.html"><a href="regression.html#calculating-correlations-in-r"><i class="fa fa-check"></i><b>8.4.4</b> Calculating correlations in R</a></li>
<li class="chapter" data-level="8.4.5" data-path="regression.html"><a href="regression.html#interpretingcorrelations"><i class="fa fa-check"></i><b>8.4.5</b> Interpreting a correlation</a></li>
<li class="chapter" data-level="8.4.6" data-path="regression.html"><a href="regression.html#spearmans-rank-correlations"><i class="fa fa-check"></i><b>8.4.6</b> Spearman’s rank correlations</a></li>
<li class="chapter" data-level="8.4.7" data-path="regression.html"><a href="regression.html#the-correlate-function"><i class="fa fa-check"></i><b>8.4.7</b> The <code>correlate()</code> function</a></li>
<li class="chapter" data-level="8.4.8" data-path="regression.html"><a href="regression.html#missing-values-in-pairwise-calculations-1"><i class="fa fa-check"></i><b>8.4.8</b> Missing values in pairwise calculations</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="regression.html"><a href="regression.html#introregression"><i class="fa fa-check"></i><b>8.5</b> Linear regression</a></li>
<li class="chapter" data-level="8.6" data-path="regression.html"><a href="regression.html#regressionestimation"><i class="fa fa-check"></i><b>8.6</b> Estimating a linear regression model</a>
<ul>
<li class="chapter" data-level="8.6.1" data-path="regression.html"><a href="regression.html#lm"><i class="fa fa-check"></i><b>8.6.1</b> Using the <code>lm()</code> function</a></li>
<li class="chapter" data-level="8.6.2" data-path="regression.html"><a href="regression.html#interpreting-the-estimated-model"><i class="fa fa-check"></i><b>8.6.2</b> Interpreting the estimated model</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="regression.html"><a href="regression.html#multipleregression"><i class="fa fa-check"></i><b>8.7</b> Multiple linear regression</a>
<ul>
<li class="chapter" data-level="8.7.1" data-path="regression.html"><a href="regression.html#doing-it-in-r"><i class="fa fa-check"></i><b>8.7.1</b> Doing it in R</a></li>
<li class="chapter" data-level="8.7.2" data-path="regression.html"><a href="regression.html#formula-for-the-general-case"><i class="fa fa-check"></i><b>8.7.2</b> Formula for the general case</a></li>
</ul></li>
<li class="chapter" data-level="8.8" data-path="regression.html"><a href="regression.html#r2"><i class="fa fa-check"></i><b>8.8</b> Quantifying the fit of the regression model</a>
<ul>
<li class="chapter" data-level="8.8.1" data-path="regression.html"><a href="regression.html#the-r2-value"><i class="fa fa-check"></i><b>8.8.1</b> The <span class="math inline">\(R^2\)</span> value</a></li>
<li class="chapter" data-level="8.8.2" data-path="regression.html"><a href="regression.html#the-relationship-between-regression-and-correlation"><i class="fa fa-check"></i><b>8.8.2</b> The relationship between regression and correlation</a></li>
<li class="chapter" data-level="8.8.3" data-path="regression.html"><a href="regression.html#the-adjusted-r2-value"><i class="fa fa-check"></i><b>8.8.3</b> The adjusted <span class="math inline">\(R^2\)</span> value</a></li>
</ul></li>
<li class="chapter" data-level="8.9" data-path="regression.html"><a href="regression.html#regressiontests"><i class="fa fa-check"></i><b>8.9</b> Hypothesis tests for regression models</a>
<ul>
<li class="chapter" data-level="8.9.1" data-path="regression.html"><a href="regression.html#testing-the-model-as-a-whole-the-omnibus-test"><i class="fa fa-check"></i><b>8.9.1</b> Testing the model as a whole: The omnibus test</a></li>
<li class="chapter" data-level="8.9.2" data-path="regression.html"><a href="regression.html#tests-for-individual-coefficients"><i class="fa fa-check"></i><b>8.9.2</b> Tests for individual coefficients</a></li>
<li class="chapter" data-level="8.9.3" data-path="regression.html"><a href="regression.html#regressionsummary"><i class="fa fa-check"></i><b>8.9.3</b> Running the hypothesis tests in R</a></li>
</ul></li>
<li class="chapter" data-level="8.10" data-path="regression.html"><a href="regression.html#corrhyp"><i class="fa fa-check"></i><b>8.10</b> Testing the significance of a correlation</a>
<ul>
<li class="chapter" data-level="8.10.1" data-path="regression.html"><a href="regression.html#hypothesis-tests-for-a-single-correlation"><i class="fa fa-check"></i><b>8.10.1</b> Hypothesis tests for a single correlation</a></li>
<li class="chapter" data-level="8.10.2" data-path="regression.html"><a href="regression.html#corrhyp2"><i class="fa fa-check"></i><b>8.10.2</b> Hypothesis tests for all pairwise correlations</a></li>
</ul></li>
<li class="chapter" data-level="8.11" data-path="regression.html"><a href="regression.html#regressioncoefs"><i class="fa fa-check"></i><b>8.11</b> Regarding regression coefficients</a>
<ul>
<li class="chapter" data-level="8.11.1" data-path="regression.html"><a href="regression.html#confidence-intervals-for-the-coefficients"><i class="fa fa-check"></i><b>8.11.1</b> Confidence intervals for the coefficients</a></li>
<li class="chapter" data-level="8.11.2" data-path="regression.html"><a href="regression.html#calculating-standardised-regression-coefficients"><i class="fa fa-check"></i><b>8.11.2</b> Calculating standardised regression coefficients</a></li>
</ul></li>
<li class="chapter" data-level="8.12" data-path="regression.html"><a href="regression.html#regressionassumptions"><i class="fa fa-check"></i><b>8.12</b> Assumptions of regression</a></li>
<li class="chapter" data-level="8.13" data-path="regression.html"><a href="regression.html#regressiondiagnostics"><i class="fa fa-check"></i><b>8.13</b> Model checking</a>
<ul>
<li class="chapter" data-level="8.13.1" data-path="regression.html"><a href="regression.html#three-kinds-of-residuals"><i class="fa fa-check"></i><b>8.13.1</b> Three kinds of residuals</a></li>
<li class="chapter" data-level="8.13.2" data-path="regression.html"><a href="regression.html#regressionoutliers"><i class="fa fa-check"></i><b>8.13.2</b> Three kinds of anomalous data</a></li>
<li class="chapter" data-level="8.13.3" data-path="regression.html"><a href="regression.html#regressionnormality"><i class="fa fa-check"></i><b>8.13.3</b> Checking the normality of the residuals</a></li>
<li class="chapter" data-level="8.13.4" data-path="regression.html"><a href="regression.html#regressionlinearity"><i class="fa fa-check"></i><b>8.13.4</b> Checking the linearity of the relationship</a></li>
<li class="chapter" data-level="8.13.5" data-path="regression.html"><a href="regression.html#regressionhomogeneity"><i class="fa fa-check"></i><b>8.13.5</b> Checking the homogeneity of variance</a></li>
<li class="chapter" data-level="8.13.6" data-path="regression.html"><a href="regression.html#regressioncollinearity"><i class="fa fa-check"></i><b>8.13.6</b> Checking for collinearity</a></li>
</ul></li>
<li class="chapter" data-level="8.14" data-path="regression.html"><a href="regression.html#modelselreg"><i class="fa fa-check"></i><b>8.14</b> Model selection</a>
<ul>
<li class="chapter" data-level="8.14.1" data-path="regression.html"><a href="regression.html#backward-elimination"><i class="fa fa-check"></i><b>8.14.1</b> Backward elimination</a></li>
<li class="chapter" data-level="8.14.2" data-path="regression.html"><a href="regression.html#forward-selection"><i class="fa fa-check"></i><b>8.14.2</b> Forward selection</a></li>
<li class="chapter" data-level="8.14.3" data-path="regression.html"><a href="regression.html#a-caveat"><i class="fa fa-check"></i><b>8.14.3</b> A caveat</a></li>
<li class="chapter" data-level="8.14.4" data-path="regression.html"><a href="regression.html#comparing-two-regression-models"><i class="fa fa-check"></i><b>8.14.4</b> Comparing two regression models</a></li>
</ul></li>
<li class="chapter" data-level="8.15" data-path="regression.html"><a href="regression.html#practical-issues-in-correlation-and-regression"><i class="fa fa-check"></i><b>8.15</b> Practical Issues in Correlation and Regression</a>
<ul>
<li class="chapter" data-level="8.15.1" data-path="regression.html"><a href="regression.html#correlation-is-not-causation"><i class="fa fa-check"></i><b>8.15.1</b> Correlation is not causation</a></li>
<li class="chapter" data-level="8.15.2" data-path="regression.html"><a href="regression.html#interpreting-nhst-in-big-data"><i class="fa fa-check"></i><b>8.15.2</b> Interpreting NHST in Big Data</a></li>
<li class="chapter" data-level="8.15.3" data-path="regression.html"><a href="regression.html#outliers"><i class="fa fa-check"></i><b>8.15.3</b> Outliers</a></li>
<li class="chapter" data-level="8.15.4" data-path="regression.html"><a href="regression.html#restriction-of-range"><i class="fa fa-check"></i><b>8.15.4</b> Restriction of Range</a></li>
<li class="chapter" data-level="8.15.5" data-path="regression.html"><a href="regression.html#regression-toward-the-mean"><i class="fa fa-check"></i><b>8.15.5</b> Regression Toward the Mean</a></li>
<li class="chapter" data-level="8.15.6" data-path="regression.html"><a href="regression.html#report-effect-size"><i class="fa fa-check"></i><b>8.15.6</b> Report Effect Size</a></li>
<li class="chapter" data-level="8.15.7" data-path="regression.html"><a href="regression.html#what-are-degrees-of-freedom-again"><i class="fa fa-check"></i><b>8.15.7</b> What are Degrees of Freedom, again?</a></li>
</ul></li>
<li class="chapter" data-level="8.16" data-path="regression.html"><a href="regression.html#summary-5"><i class="fa fa-check"></i><b>8.16</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="ttest.html"><a href="ttest.html"><i class="fa fa-check"></i><b>9</b> T-Tests</a>
<ul>
<li class="chapter" data-level="9.1" data-path="ttest.html"><a href="ttest.html#videos-7"><i class="fa fa-check"></i><b>9.1</b> Videos</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="ttest.html"><a href="ttest.html#t-tests"><i class="fa fa-check"></i><b>9.1.1</b> T-Tests</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="ttest.html"><a href="ttest.html#groupcomparisonscompared"><i class="fa fa-check"></i><b>9.2</b> Comparison of tests that compare groups</a></li>
<li class="chapter" data-level="9.3" data-path="ttest.html"><a href="ttest.html#introduction-6"><i class="fa fa-check"></i><b>9.3</b> Introduction</a></li>
<li class="chapter" data-level="9.4" data-path="ttest.html"><a href="ttest.html#onesamplettest"><i class="fa fa-check"></i><b>9.4</b> The one-sample <span class="math inline">\(t\)</span>-test</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="ttest.html"><a href="ttest.html#introducing-the-t-test"><i class="fa fa-check"></i><b>9.4.1</b> Introducing the <span class="math inline">\(t\)</span>-test</a></li>
<li class="chapter" data-level="9.4.2" data-path="ttest.html"><a href="ttest.html#doing-the-test-in-r"><i class="fa fa-check"></i><b>9.4.2</b> Doing the test in R</a></li>
<li class="chapter" data-level="9.4.3" data-path="ttest.html"><a href="ttest.html#ttestoneassumptions"><i class="fa fa-check"></i><b>9.4.3</b> Assumptions of the one sample <span class="math inline">\(t\)</span>-test</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="ttest.html"><a href="ttest.html#studentttest"><i class="fa fa-check"></i><b>9.5</b> The independent samples <span class="math inline">\(t\)</span>-test (Student test)</a>
<ul>
<li class="chapter" data-level="9.5.1" data-path="ttest.html"><a href="ttest.html#the-data-1"><i class="fa fa-check"></i><b>9.5.1</b> The data</a></li>
<li class="chapter" data-level="9.5.2" data-path="ttest.html"><a href="ttest.html#introducing-the-test"><i class="fa fa-check"></i><b>9.5.2</b> Introducing the test</a></li>
<li class="chapter" data-level="9.5.3" data-path="ttest.html"><a href="ttest.html#a-pooled-estimate-of-the-standard-deviation"><i class="fa fa-check"></i><b>9.5.3</b> A “pooled estimate” of the standard deviation</a></li>
<li class="chapter" data-level="9.5.4" data-path="ttest.html"><a href="ttest.html#the-same-pooled-estimate-described-differently"><i class="fa fa-check"></i><b>9.5.4</b> The same pooled estimate, described differently</a></li>
<li class="chapter" data-level="9.5.5" data-path="ttest.html"><a href="ttest.html#completing-the-test"><i class="fa fa-check"></i><b>9.5.5</b> Completing the test</a></li>
<li class="chapter" data-level="9.5.6" data-path="ttest.html"><a href="ttest.html#doing-the-test-in-r-1"><i class="fa fa-check"></i><b>9.5.6</b> Doing the test in R</a></li>
<li class="chapter" data-level="9.5.7" data-path="ttest.html"><a href="ttest.html#positive-and-negative-t-values"><i class="fa fa-check"></i><b>9.5.7</b> Positive and negative <span class="math inline">\(t\)</span> values</a></li>
<li class="chapter" data-level="9.5.8" data-path="ttest.html"><a href="ttest.html#studentassumptions"><i class="fa fa-check"></i><b>9.5.8</b> Assumptions of the test</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="ttest.html"><a href="ttest.html#welchttest"><i class="fa fa-check"></i><b>9.6</b> The independent samples <span class="math inline">\(t\)</span>-test (Welch test)</a>
<ul>
<li class="chapter" data-level="9.6.1" data-path="ttest.html"><a href="ttest.html#doing-the-test-in-r-2"><i class="fa fa-check"></i><b>9.6.1</b> Doing the test in R</a></li>
<li class="chapter" data-level="9.6.2" data-path="ttest.html"><a href="ttest.html#assumptions-of-the-test"><i class="fa fa-check"></i><b>9.6.2</b> Assumptions of the test</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="ttest.html"><a href="ttest.html#pairedsamplesttest"><i class="fa fa-check"></i><b>9.7</b> The paired-samples <span class="math inline">\(t\)</span>-test</a>
<ul>
<li class="chapter" data-level="9.7.1" data-path="ttest.html"><a href="ttest.html#the-data-2"><i class="fa fa-check"></i><b>9.7.1</b> The data</a></li>
<li class="chapter" data-level="9.7.2" data-path="ttest.html"><a href="ttest.html#what-is-the-paired-samples-t-test"><i class="fa fa-check"></i><b>9.7.2</b> What is the paired samples <span class="math inline">\(t\)</span>-test?</a></li>
<li class="chapter" data-level="9.7.3" data-path="ttest.html"><a href="ttest.html#doing-the-test-in-r-part-1"><i class="fa fa-check"></i><b>9.7.3</b> Doing the test in R, part 1</a></li>
<li class="chapter" data-level="9.7.4" data-path="ttest.html"><a href="ttest.html#doing-the-test-in-r-part-2"><i class="fa fa-check"></i><b>9.7.4</b> Doing the test in R, part 2</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="ttest.html"><a href="ttest.html#one-sided-tests"><i class="fa fa-check"></i><b>9.8</b> One sided tests</a></li>
<li class="chapter" data-level="9.9" data-path="ttest.html"><a href="ttest.html#ttestfunction"><i class="fa fa-check"></i><b>9.9</b> Using the t.test() function</a></li>
<li class="chapter" data-level="9.10" data-path="ttest.html"><a href="ttest.html#cohensd"><i class="fa fa-check"></i><b>9.10</b> Effect size</a>
<ul>
<li class="chapter" data-level="9.10.1" data-path="ttest.html"><a href="ttest.html#cohens-d-from-one-sample"><i class="fa fa-check"></i><b>9.10.1</b> Cohen’s <span class="math inline">\(d\)</span> from one sample</a></li>
<li class="chapter" data-level="9.10.2" data-path="ttest.html"><a href="ttest.html#cohens-d-from-a-student-t-test"><i class="fa fa-check"></i><b>9.10.2</b> Cohen’s <span class="math inline">\(d\)</span> from a Student <span class="math inline">\(t\)</span> test</a></li>
<li class="chapter" data-level="9.10.3" data-path="ttest.html"><a href="ttest.html#cohens-d-from-a-welch-test"><i class="fa fa-check"></i><b>9.10.3</b> Cohen’s <span class="math inline">\(d\)</span> from a Welch test</a></li>
<li class="chapter" data-level="9.10.4" data-path="ttest.html"><a href="ttest.html#cohens-d-from-a-paired-samples-test"><i class="fa fa-check"></i><b>9.10.4</b> Cohen’s <span class="math inline">\(d\)</span> from a paired-samples test</a></li>
</ul></li>
<li class="chapter" data-level="9.11" data-path="ttest.html"><a href="ttest.html#shapiro"><i class="fa fa-check"></i><b>9.11</b> Checking the normality of a sample</a>
<ul>
<li class="chapter" data-level="9.11.1" data-path="ttest.html"><a href="ttest.html#qq-plots"><i class="fa fa-check"></i><b>9.11.1</b> QQ plots</a></li>
<li class="chapter" data-level="9.11.2" data-path="ttest.html"><a href="ttest.html#shapiro-wilk-tests"><i class="fa fa-check"></i><b>9.11.2</b> Shapiro-Wilk tests</a></li>
</ul></li>
<li class="chapter" data-level="9.12" data-path="ttest.html"><a href="ttest.html#wilcox"><i class="fa fa-check"></i><b>9.12</b> Testing non-normal data with Wilcoxon tests</a>
<ul>
<li class="chapter" data-level="9.12.1" data-path="ttest.html"><a href="ttest.html#two-sample-wilcoxon-test"><i class="fa fa-check"></i><b>9.12.1</b> Two sample Wilcoxon test</a></li>
<li class="chapter" data-level="9.12.2" data-path="ttest.html"><a href="ttest.html#one-sample-wilcoxon-test"><i class="fa fa-check"></i><b>9.12.2</b> One sample Wilcoxon test</a></li>
</ul></li>
<li class="chapter" data-level="9.13" data-path="ttest.html"><a href="ttest.html#summary-6"><i class="fa fa-check"></i><b>9.13</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="anova.html"><a href="anova.html"><i class="fa fa-check"></i><b>10</b> One-way ANOVA</a>
<ul>
<li class="chapter" data-level="10.1" data-path="anova.html"><a href="anova.html#videos-8"><i class="fa fa-check"></i><b>10.1</b> Videos</a>
<ul>
<li class="chapter" data-level="10.1.1" data-path="anova.html"><a href="anova.html#one-way-between-subjects-anova"><i class="fa fa-check"></i><b>10.1.1</b> One-Way, Between-Subjects ANOVA</a></li>
<li class="chapter" data-level="10.1.2" data-path="anova.html"><a href="anova.html#repeated-measures-anova"><i class="fa fa-check"></i><b>10.1.2</b> Repeated Measures ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="anova.html"><a href="anova.html#anxifree"><i class="fa fa-check"></i><b>10.2</b> An illustrative data set</a></li>
<li class="chapter" data-level="10.3" data-path="anova.html"><a href="anova.html#anovaintro"><i class="fa fa-check"></i><b>10.3</b> How ANOVA works</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="anova.html"><a href="anova.html#two-formulas-for-the-variance-of-y"><i class="fa fa-check"></i><b>10.3.1</b> Two formulas for the variance of <span class="math inline">\(Y\)</span></a></li>
<li class="chapter" data-level="10.3.2" data-path="anova.html"><a href="anova.html#from-variances-to-sums-of-squares"><i class="fa fa-check"></i><b>10.3.2</b> From variances to sums of squares</a></li>
<li class="chapter" data-level="10.3.3" data-path="anova.html"><a href="anova.html#from-sums-of-squares-to-the-f-test"><i class="fa fa-check"></i><b>10.3.3</b> From sums of squares to the <span class="math inline">\(F\)</span>-test</a></li>
<li class="chapter" data-level="10.3.4" data-path="anova.html"><a href="anova.html#anovamodel"><i class="fa fa-check"></i><b>10.3.4</b> The model for the data and the meaning of <span class="math inline">\(F\)</span> (advanced)</a></li>
<li class="chapter" data-level="10.3.5" data-path="anova.html"><a href="anova.html#anovacalc"><i class="fa fa-check"></i><b>10.3.5</b> A worked example</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="anova.html"><a href="anova.html#introduceaov"><i class="fa fa-check"></i><b>10.4</b> Running an ANOVA in R</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="anova.html"><a href="anova.html#using-the-aov-function-to-specify-your-anova"><i class="fa fa-check"></i><b>10.4.1</b> Using the <code>aov()</code> function to specify your ANOVA</a></li>
<li class="chapter" data-level="10.4.2" data-path="anova.html"><a href="anova.html#aovobjects"><i class="fa fa-check"></i><b>10.4.2</b> Understanding what the <code>aov()</code> function produces</a></li>
<li class="chapter" data-level="10.4.3" data-path="anova.html"><a href="anova.html#running-the-hypothesis-tests-for-the-anova"><i class="fa fa-check"></i><b>10.4.3</b> Running the hypothesis tests for the ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="anova.html"><a href="anova.html#etasquared"><i class="fa fa-check"></i><b>10.5</b> Effect size</a></li>
<li class="chapter" data-level="10.6" data-path="anova.html"><a href="anova.html#anovaandt"><i class="fa fa-check"></i><b>10.6</b> On the relationship between ANOVA and the Student <span class="math inline">\(t\)</span> test</a></li>
<li class="chapter" data-level="10.7" data-path="anova.html"><a href="anova.html#summary-7"><i class="fa fa-check"></i><b>10.7</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="multiple-comparisons.html"><a href="multiple-comparisons.html"><i class="fa fa-check"></i><b>11</b> Multiple comparisons</a>
<ul>
<li class="chapter" data-level="11.1" data-path="multiple-comparisons.html"><a href="multiple-comparisons.html#posthoc"><i class="fa fa-check"></i><b>11.1</b> Multiple comparisons and post hoc tests</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="multiple-comparisons.html"><a href="multiple-comparisons.html#running-pairwise-t-tests"><i class="fa fa-check"></i><b>11.1.1</b> Running “pairwise” <span class="math inline">\(t\)</span>-tests</a></li>
<li class="chapter" data-level="11.1.2" data-path="multiple-comparisons.html"><a href="multiple-comparisons.html#corrections-for-multiple-testing"><i class="fa fa-check"></i><b>11.1.2</b> Corrections for multiple testing</a></li>
<li class="chapter" data-level="11.1.3" data-path="multiple-comparisons.html"><a href="multiple-comparisons.html#bonferroni-corrections"><i class="fa fa-check"></i><b>11.1.3</b> Bonferroni corrections</a></li>
<li class="chapter" data-level="11.1.4" data-path="multiple-comparisons.html"><a href="multiple-comparisons.html#holm-corrections"><i class="fa fa-check"></i><b>11.1.4</b> Holm corrections</a></li>
<li class="chapter" data-level="11.1.5" data-path="multiple-comparisons.html"><a href="multiple-comparisons.html#writing-up-the-post-hoc-test"><i class="fa fa-check"></i><b>11.1.5</b> Writing up the post hoc test</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="multiple-comparisons.html"><a href="multiple-comparisons.html#anovaassumptions"><i class="fa fa-check"></i><b>11.2</b> Assumptions of one-way ANOVA</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="multiple-comparisons.html"><a href="multiple-comparisons.html#how-robust-is-anova"><i class="fa fa-check"></i><b>11.2.1</b> How robust is ANOVA?</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="multiple-comparisons.html"><a href="multiple-comparisons.html#levene"><i class="fa fa-check"></i><b>11.3</b> Checking the homogeneity of variance assumption</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="multiple-comparisons.html"><a href="multiple-comparisons.html#running-the-levenes-test-in-r"><i class="fa fa-check"></i><b>11.3.1</b> Running the Levene’s test in R</a></li>
<li class="chapter" data-level="11.3.2" data-path="multiple-comparisons.html"><a href="multiple-comparisons.html#additional-comments"><i class="fa fa-check"></i><b>11.3.2</b> Additional comments</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="multiple-comparisons.html"><a href="multiple-comparisons.html#welchoneway"><i class="fa fa-check"></i><b>11.4</b> Removing the homogeneity of variance assumption</a></li>
<li class="chapter" data-level="11.5" data-path="multiple-comparisons.html"><a href="multiple-comparisons.html#anovanormality"><i class="fa fa-check"></i><b>11.5</b> Checking the normality assumption</a></li>
<li class="chapter" data-level="11.6" data-path="multiple-comparisons.html"><a href="multiple-comparisons.html#kruskalwallis"><i class="fa fa-check"></i><b>11.6</b> Removing the normality assumption</a>
<ul>
<li class="chapter" data-level="11.6.1" data-path="multiple-comparisons.html"><a href="multiple-comparisons.html#the-logic-behind-the-kruskal-wallis-test"><i class="fa fa-check"></i><b>11.6.1</b> The logic behind the Kruskal-Wallis test</a></li>
<li class="chapter" data-level="11.6.2" data-path="multiple-comparisons.html"><a href="multiple-comparisons.html#additional-details"><i class="fa fa-check"></i><b>11.6.2</b> Additional details</a></li>
<li class="chapter" data-level="11.6.3" data-path="multiple-comparisons.html"><a href="multiple-comparisons.html#how-to-run-the-kruskal-wallis-test-in-r"><i class="fa fa-check"></i><b>11.6.3</b> How to run the Kruskal-Wallis test in R</a></li>
</ul></li>
<li class="chapter" data-level="11.7" data-path="multiple-comparisons.html"><a href="multiple-comparisons.html#summary-8"><i class="fa fa-check"></i><b>11.7</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="factorial-analysis-of-variance-anova.html"><a href="factorial-analysis-of-variance-anova.html"><i class="fa fa-check"></i><b>12</b> Factorial Analysis of Variance (ANOVA)</a>
<ul>
<li class="chapter" data-level="12.1" data-path="factorial-analysis-of-variance-anova.html"><a href="factorial-analysis-of-variance-anova.html#video"><i class="fa fa-check"></i><b>12.1</b> Video</a></li>
<li class="chapter" data-level="12.2" data-path="factorial-analysis-of-variance-anova.html"><a href="factorial-analysis-of-variance-anova.html#introduction-7"><i class="fa fa-check"></i><b>12.2</b> Introduction</a></li>
<li class="chapter" data-level="12.3" data-path="factorial-analysis-of-variance-anova.html"><a href="factorial-analysis-of-variance-anova.html#why-do-a-factorial-anova"><i class="fa fa-check"></i><b>12.3</b> Why do a factorial ANOVA?</a></li>
<li class="chapter" data-level="12.4" data-path="factorial-analysis-of-variance-anova.html"><a href="factorial-analysis-of-variance-anova.html#interactions"><i class="fa fa-check"></i><b>12.4</b> Interactions</a></li>
<li class="chapter" data-level="12.5" data-path="factorial-analysis-of-variance-anova.html"><a href="factorial-analysis-of-variance-anova.html#main-effects"><i class="fa fa-check"></i><b>12.5</b> Main Effects</a></li>
<li class="chapter" data-level="12.6" data-path="factorial-analysis-of-variance-anova.html"><a href="factorial-analysis-of-variance-anova.html#simple-effects"><i class="fa fa-check"></i><b>12.6</b> Simple Effects</a></li>
<li class="chapter" data-level="12.7" data-path="factorial-analysis-of-variance-anova.html"><a href="factorial-analysis-of-variance-anova.html#interaction-effect"><i class="fa fa-check"></i><b>12.7</b> Interaction Effect</a></li>
<li class="chapter" data-level="12.8" data-path="factorial-analysis-of-variance-anova.html"><a href="factorial-analysis-of-variance-anova.html#factorial-anova-is-really-3-anovas"><i class="fa fa-check"></i><b>12.8</b> Factorial ANOVA is really 3 ANOVAs</a></li>
<li class="chapter" data-level="12.9" data-path="factorial-analysis-of-variance-anova.html"><a href="factorial-analysis-of-variance-anova.html#interpret-interaction-effects-first"><i class="fa fa-check"></i><b>12.9</b> Interpret Interaction Effects First</a></li>
<li class="chapter" data-level="12.10" data-path="factorial-analysis-of-variance-anova.html"><a href="factorial-analysis-of-variance-anova.html#graphing-factorial-anova-means"><i class="fa fa-check"></i><b>12.10</b> Graphing Factorial ANOVA Means</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="statistics-reference.html"><a href="statistics-reference.html"><i class="fa fa-check"></i><b>13</b> Statistics Reference</a>
<ul>
<li class="chapter" data-level="13.1" data-path="statistics-reference.html"><a href="statistics-reference.html#one-sample-z-test"><i class="fa fa-check"></i><b>13.1</b> One-Sample <em>z</em>-Test</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="statistics-reference.html"><a href="statistics-reference.html#definition"><i class="fa fa-check"></i><b>13.1.1</b> Definition</a></li>
<li class="chapter" data-level="13.1.2" data-path="statistics-reference.html"><a href="statistics-reference.html#test-statistic"><i class="fa fa-check"></i><b>13.1.2</b> Test Statistic</a></li>
<li class="chapter" data-level="13.1.3" data-path="statistics-reference.html"><a href="statistics-reference.html#assumptions-required-data"><i class="fa fa-check"></i><b>13.1.3</b> Assumptions &amp; Required Data</a></li>
<li class="chapter" data-level="13.1.4" data-path="statistics-reference.html"><a href="statistics-reference.html#when-to-use-it"><i class="fa fa-check"></i><b>13.1.4</b> When to use it</a></li>
<li class="chapter" data-level="13.1.5" data-path="statistics-reference.html"><a href="statistics-reference.html#example"><i class="fa fa-check"></i><b>13.1.5</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="statistics-reference.html"><a href="statistics-reference.html#correlation"><i class="fa fa-check"></i><b>13.2</b> Correlation</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="statistics-reference.html"><a href="statistics-reference.html#definition-1"><i class="fa fa-check"></i><b>13.2.1</b> Definition</a></li>
<li class="chapter" data-level="13.2.2" data-path="statistics-reference.html"><a href="statistics-reference.html#test-statistic-1"><i class="fa fa-check"></i><b>13.2.2</b> Test Statistic</a></li>
<li class="chapter" data-level="13.2.3" data-path="statistics-reference.html"><a href="statistics-reference.html#assumptions-required-data-1"><i class="fa fa-check"></i><b>13.2.3</b> Assumptions &amp; Required Data</a></li>
<li class="chapter" data-level="13.2.4" data-path="statistics-reference.html"><a href="statistics-reference.html#when-to-use-it-1"><i class="fa fa-check"></i><b>13.2.4</b> When to use it</a></li>
<li class="chapter" data-level="13.2.5" data-path="statistics-reference.html"><a href="statistics-reference.html#example-1"><i class="fa fa-check"></i><b>13.2.5</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="statistics-reference.html"><a href="statistics-reference.html#linear-regression"><i class="fa fa-check"></i><b>13.3</b> Linear Regression</a>
<ul>
<li class="chapter" data-level="13.3.1" data-path="statistics-reference.html"><a href="statistics-reference.html#definition-2"><i class="fa fa-check"></i><b>13.3.1</b> Definition</a></li>
<li class="chapter" data-level="13.3.2" data-path="statistics-reference.html"><a href="statistics-reference.html#test-statistic-2"><i class="fa fa-check"></i><b>13.3.2</b> Test Statistic</a></li>
<li class="chapter" data-level="13.3.3" data-path="statistics-reference.html"><a href="statistics-reference.html#assumptions-required-data-2"><i class="fa fa-check"></i><b>13.3.3</b> Assumptions &amp; Required Data</a></li>
<li class="chapter" data-level="13.3.4" data-path="statistics-reference.html"><a href="statistics-reference.html#when-to-use-it-2"><i class="fa fa-check"></i><b>13.3.4</b> When to use it</a></li>
<li class="chapter" data-level="13.3.5" data-path="statistics-reference.html"><a href="statistics-reference.html#example-2"><i class="fa fa-check"></i><b>13.3.5</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="statistics-reference.html"><a href="statistics-reference.html#one-sample-t-test"><i class="fa fa-check"></i><b>13.4</b> One-Sample <em>t</em>-Test</a>
<ul>
<li class="chapter" data-level="13.4.1" data-path="statistics-reference.html"><a href="statistics-reference.html#definition-3"><i class="fa fa-check"></i><b>13.4.1</b> Definition</a></li>
<li class="chapter" data-level="13.4.2" data-path="statistics-reference.html"><a href="statistics-reference.html#test-statistic-3"><i class="fa fa-check"></i><b>13.4.2</b> Test Statistic</a></li>
<li class="chapter" data-level="13.4.3" data-path="statistics-reference.html"><a href="statistics-reference.html#assumptions-required-data-3"><i class="fa fa-check"></i><b>13.4.3</b> Assumptions &amp; Required Data</a></li>
<li class="chapter" data-level="13.4.4" data-path="statistics-reference.html"><a href="statistics-reference.html#when-to-use-it-3"><i class="fa fa-check"></i><b>13.4.4</b> When to use it</a></li>
<li class="chapter" data-level="13.4.5" data-path="statistics-reference.html"><a href="statistics-reference.html#example-3"><i class="fa fa-check"></i><b>13.4.5</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="statistics-reference.html"><a href="statistics-reference.html#paired-samples-t-test"><i class="fa fa-check"></i><b>13.5</b> Paired Samples T-Test</a>
<ul>
<li class="chapter" data-level="13.5.1" data-path="statistics-reference.html"><a href="statistics-reference.html#definition-4"><i class="fa fa-check"></i><b>13.5.1</b> Definition</a></li>
<li class="chapter" data-level="13.5.2" data-path="statistics-reference.html"><a href="statistics-reference.html#test-statistic-4"><i class="fa fa-check"></i><b>13.5.2</b> Test Statistic</a></li>
<li class="chapter" data-level="13.5.3" data-path="statistics-reference.html"><a href="statistics-reference.html#assumptions-required-data-4"><i class="fa fa-check"></i><b>13.5.3</b> Assumptions &amp; Required Data</a></li>
<li class="chapter" data-level="13.5.4" data-path="statistics-reference.html"><a href="statistics-reference.html#when-to-use-it-4"><i class="fa fa-check"></i><b>13.5.4</b> When to use it</a></li>
<li class="chapter" data-level="13.5.5" data-path="statistics-reference.html"><a href="statistics-reference.html#example-4"><i class="fa fa-check"></i><b>13.5.5</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="statistics-reference.html"><a href="statistics-reference.html#independent-samples-t-test"><i class="fa fa-check"></i><b>13.6</b> Independent Samples <span class="math inline">\(t\)</span>-Test</a>
<ul>
<li class="chapter" data-level="13.6.1" data-path="statistics-reference.html"><a href="statistics-reference.html#definition-5"><i class="fa fa-check"></i><b>13.6.1</b> Definition</a></li>
<li class="chapter" data-level="13.6.2" data-path="statistics-reference.html"><a href="statistics-reference.html#test-statistic-5"><i class="fa fa-check"></i><b>13.6.2</b> Test Statistic</a></li>
<li class="chapter" data-level="13.6.3" data-path="statistics-reference.html"><a href="statistics-reference.html#assumptions-required-data-5"><i class="fa fa-check"></i><b>13.6.3</b> Assumptions &amp; Required Data</a></li>
<li class="chapter" data-level="13.6.4" data-path="statistics-reference.html"><a href="statistics-reference.html#when-to-use-it-5"><i class="fa fa-check"></i><b>13.6.4</b> When to use it</a></li>
<li class="chapter" data-level="13.6.5" data-path="statistics-reference.html"><a href="statistics-reference.html#example-5"><i class="fa fa-check"></i><b>13.6.5</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="statistics-reference.html"><a href="statistics-reference.html#one-way-anova"><i class="fa fa-check"></i><b>13.7</b> One-Way ANOVA</a>
<ul>
<li class="chapter" data-level="13.7.1" data-path="statistics-reference.html"><a href="statistics-reference.html#definition-6"><i class="fa fa-check"></i><b>13.7.1</b> Definition</a></li>
<li class="chapter" data-level="13.7.2" data-path="statistics-reference.html"><a href="statistics-reference.html#test-statistic-6"><i class="fa fa-check"></i><b>13.7.2</b> Test Statistic</a></li>
<li class="chapter" data-level="13.7.3" data-path="statistics-reference.html"><a href="statistics-reference.html#assumptions-required-data-6"><i class="fa fa-check"></i><b>13.7.3</b> Assumptions &amp; Required Data</a></li>
<li class="chapter" data-level="13.7.4" data-path="statistics-reference.html"><a href="statistics-reference.html#when-to-use-it-6"><i class="fa fa-check"></i><b>13.7.4</b> When to use it</a></li>
<li class="chapter" data-level="13.7.5" data-path="statistics-reference.html"><a href="statistics-reference.html#example-1-between-subjects"><i class="fa fa-check"></i><b>13.7.5</b> Example 1: Between-Subjects</a></li>
<li class="chapter" data-level="13.7.6" data-path="statistics-reference.html"><a href="statistics-reference.html#example-2-within-subjects"><i class="fa fa-check"></i><b>13.7.6</b> Example 2: Within-Subjects</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Advanced Statistics Remix</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="inferential-statistics-the-central-limit-theorem" class="section level1" number="4">
<h1><span class="header-section-number">Chapter 4</span> Inferential statistics: The Central Limit Theorem</h1>
<div id="videos-2" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> Videos</h2>
<p><a href="https://www.youtube.com/watch?v=GEFxFVESQXc">Video: Are you a Bayesian or a Frequentist?</a></p>
<p><a href="https://www.youtube.com/watch?v=be9e-Q-jC-0">Video: Sampling Methods</a></p>
<p><a href="https://www.youtube.com/watch?v=_BiVb6neUP0">Video: Estimation and Confidence Intervals</a></p>
<p><a href="https://youtu.be/qpioCUPvyHY">Video: The central limit theorem</a></p>
</div>
<div id="estimation" class="section level2" number="4.2">
<h2><span class="header-section-number">4.2</span> Introduction</h2>
<p>Text by <span class="citation"><a href="#ref-Navarro2018" role="doc-biblioref">Navarro</a> (<a href="#ref-Navarro2018" role="doc-biblioref">2018</a>)</span></p>
<p>The role of descriptive statistics is to concisely summarise what we <em>do</em> know. In contrast, the purpose of inferential statistics is to “learn what we do not know from what we do.” We are in a good position to think about the problem of statistical inference. What kinds of things would we like to learn about? And how do we learn them? These are the questions that lie at the heart of inferential statistics, and they are traditionally divided into two “big ideas”: estimation and hypothesis testing. The goal in this chapter is to introduce the first of these big ideas, estimation theory, but I’m going to witter on about probability and sampling theory first because estimation theory doesn’t make sense until you understand probability and sampling . As a consequence, this chapter divides naturally into two parts Sections <a href="inferential-statistics-the-central-limit-theorem.html#srs">4.5</a> through <a href="inferential-statistics-the-central-limit-theorem.html#samplesandclt">4.7</a> are focused on sampling theory, and Sections <a href="inferential-statistics-the-central-limit-theorem.html#pointestimates">4.8</a> and <a href="inferential-statistics-the-central-limit-theorem.html#ci">4.9</a> make use of sampling theory to discuss how statisticians think about estimation.</p>
</div>
<div id="probstats" class="section level2" number="4.3">
<h2><span class="header-section-number">4.3</span> How are probability and statistics different?</h2>
<p>Text by <span class="citation"><a href="#ref-Navarro2018" role="doc-biblioref">Navarro</a> (<a href="#ref-Navarro2018" role="doc-biblioref">2018</a>)</span>
Before we start talking about probability theory, it’s helpful to spend a moment thinking about the relationship between probability and statistics. The two disciplines are closely related but they’re not identical. Probability theory is “the doctrine of chances.” It’s a branch of mathematics that tells you how often different kinds of events will happen. For example, all of these questions are things you can answer using probability theory:</p>
<ul>
<li>What are the chances of a fair coin coming up heads 10 times in a row?</li>
<li>If I roll two six sided dice, how likely is it that I’ll roll two sixes?</li>
<li>How likely is it that five cards drawn from a perfectly shuffled deck will all be hearts?</li>
<li>What are the chances that I’ll win the lottery?</li>
</ul>
<p>Notice that all of these questions have something in common. In each case the “truth of the world” is known, and my question relates to the “what kind of events” will happen. In the first question I <em>know</em> that the coin is fair, so there’s a 50% chance that any individual coin flip will come up heads. In the second question, I <em>know</em> that the chance of rolling a 6 on a single die is 1 in 6. In the third question I <em>know</em> that the deck is shuffled properly. And in the fourth question, I <em>know</em> that the lottery follows specific rules. You get the idea. The critical point is that probabilistic questions start with a known <strong><em>model</em></strong> of the world, and we use that model to do some calculations. The underlying model can be quite simple. For instance, in the coin flipping example, we can write down the model like this:
<span class="math display">\[
P(\mbox{heads}) = 0.5
\]</span>
which you can read as “the probability of heads is 0.5.” As we’ll see later, in the same way that percentages are numbers that range from 0% to 100%, probabilities are just numbers that range from 0 to 1. When using this probability model to answer the first question, I don’t actually know exactly what’s going to happen. Maybe I’ll get 10 heads, like the question says. But maybe I’ll get three heads. That’s the key thing: in probability theory, the <em>model</em> is known, but the <em>data</em> are not.</p>
<p>So that’s probability. What about statistics? Statistical questions work the other way around. In statistics, we <em>do not</em> know the truth about the world. All we have is the data, and it is from the data that we want to <em>learn</em> the truth about the world. Statistical questions tend to look more like these:</p>
<ul>
<li>If my friend flips a coin 10 times and gets 10 heads, are they playing a trick on me?</li>
<li>If five cards off the top of the deck are all hearts, how likely is it that the deck was shuffled? - If the lottery commissioner’s spouse wins the lottery, how likely is it that the lottery was rigged?</li>
</ul>
<p>This time around, the only thing we have are data. What I <em>know</em> is that I saw my friend flip the coin 10 times and it came up heads every time. And what I want to <strong><em>infer</em></strong> is whether or not I should conclude that what I just saw was actually a fair coin being flipped 10 times in a row, or whether I should suspect that my friend is playing a trick on me. The data I have look like this:</p>
<pre><code>H H H H H H H H H H H</code></pre>
<p>and what I’m trying to do is work out which “model of the world” I should put my trust in. If the coin is fair, then the model I should adopt is one that says that the probability of heads is 0.5; that is, <span class="math inline">\(P(\mbox{heads}) = 0.5\)</span>. If the coin is not fair, then I should conclude that the probability of heads is <em>not</em> 0.5, which we would write as <span class="math inline">\(P(\mbox{heads}) \neq 0.5\)</span>. In other words, the statistical inference problem is to figure out which of these probability models is right. Clearly, the statistical question isn’t the same as the probability question, but they’re deeply connected to one another. Because of this, a good introduction to statistical theory will start with a discussion of what probability is and how it works.</p>
</div>
<div id="probmeaning" class="section level2" number="4.4">
<h2><span class="header-section-number">4.4</span> What does probability mean?</h2>
<p>Text by <span class="citation"><a href="#ref-Navarro2018" role="doc-biblioref">Navarro</a> (<a href="#ref-Navarro2018" role="doc-biblioref">2018</a>)</span></p>
<p>Let’s start with the first of these questions. What is “probability?” It might seem surprising to you, but while statisticians and mathematicians (mostly) agree on what the <em>rules</em> of probability are, there’s much less of a consensus on what the word really <em>means</em>. It seems weird because we’re all very comfortable using words like “chance,” “likely,” “possible” and “probable,” and it doesn’t seem like it should be a very difficult question to answer. If you had to explain “probability” to a five year old, you could do a pretty good job. But if you’ve ever had that experience in real life, you might walk away from the conversation feeling like you didn’t quite get it right, and that (like many everyday concepts) it turns out that you don’t <em>really</em> know what it’s all about.</p>
<p>So I’ll have a go at it. Let’s suppose I want to bet on a soccer game between two teams of robots, <em>Arduino Arsenal</em> and <em>C Milan</em>. After thinking about it, I decide that there is an 80% probability that <em>Arduino Arsenal</em> winning. What do I mean by that? Here are three possibilities…</p>
<ul>
<li>They’re robot teams, so I can make them play over and over again, and if I did that, <em>Arduino Arsenal</em> would win 8 out of every 10 games on average.</li>
<li>For any given game, I would only agree that betting on this game is only “fair” if a $1 bet on <em>C Milan</em> gives a $5 payoff (i.e. I get my $1 back plus a $4 reward for being correct), as would a $4 bet on <em>Arduino Arsenal</em> (i.e., my $4 bet plus a $1 reward).</li>
<li>My subjective “belief” or “confidence” in an <em>Arduino Arsenal</em> victory is four times as strong as my belief in a <em>C Milan</em> victory.</li>
</ul>
<p>Each of these seems sensible. However they’re not identical, and not every statistician would endorse all of them. The reason is that there are different statistical ideologies (yes, really!) and depending on which one you subscribe to, you might say that some of those statements are meaningless or irrelevant. In this section, I give a brief introduction the two main approaches that exist in the literature. These are by no means the only approaches, but they’re the two big ones.</p>
<div id="the-frequentist-view" class="section level3" number="4.4.1">
<h3><span class="header-section-number">4.4.1</span> The frequentist view</h3>
<p>The first of the two major approaches to probability, and the more dominant one in statistics, is referred to as the <strong><em>frequentist view</em></strong>, and it defines probability as a <strong><em>long-run frequency</em></strong>. Suppose we were to try flipping a fair coin, over and over again. By definition, this is a coin that has <span class="math inline">\(P(H) = 0.5\)</span>. What might we observe? One possibility is that the first 20 flips might look like this:</p>
<pre><code>T,H,H,H,H,T,T,H,H,H,H,T,H,H,T,T,T,T,T,H</code></pre>
<p>In this case 11 of these 20 coin flips (55%) came up heads. Now suppose that I’d been keeping a running tally of the number of heads (which I’ll call <span class="math inline">\(N_H\)</span>) that I’ve seen, across the first <span class="math inline">\(N\)</span> flips, and calculate the proportion of heads <span class="math inline">\(N_H / N\)</span> every time. Here’s what I’d get (I did literally flip coins to produce this!):</p>
<table>
<thead>
<tr class="header">
<th align="right">number.of.flips</th>
<th align="right">number.of.heads</th>
<th align="right">proportion</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="right">0</td>
<td align="right">0.00</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="right">1</td>
<td align="right">0.50</td>
</tr>
<tr class="odd">
<td align="right">3</td>
<td align="right">2</td>
<td align="right">0.67</td>
</tr>
<tr class="even">
<td align="right">4</td>
<td align="right">3</td>
<td align="right">0.75</td>
</tr>
<tr class="odd">
<td align="right">5</td>
<td align="right">4</td>
<td align="right">0.80</td>
</tr>
<tr class="even">
<td align="right">6</td>
<td align="right">4</td>
<td align="right">0.67</td>
</tr>
<tr class="odd">
<td align="right">7</td>
<td align="right">4</td>
<td align="right">0.57</td>
</tr>
<tr class="even">
<td align="right">8</td>
<td align="right">5</td>
<td align="right">0.63</td>
</tr>
<tr class="odd">
<td align="right">9</td>
<td align="right">6</td>
<td align="right">0.67</td>
</tr>
<tr class="even">
<td align="right">10</td>
<td align="right">7</td>
<td align="right">0.70</td>
</tr>
<tr class="odd">
<td align="right">11</td>
<td align="right">8</td>
<td align="right">0.73</td>
</tr>
<tr class="even">
<td align="right">12</td>
<td align="right">8</td>
<td align="right">0.67</td>
</tr>
<tr class="odd">
<td align="right">13</td>
<td align="right">9</td>
<td align="right">0.69</td>
</tr>
<tr class="even">
<td align="right">14</td>
<td align="right">10</td>
<td align="right">0.71</td>
</tr>
<tr class="odd">
<td align="right">15</td>
<td align="right">10</td>
<td align="right">0.67</td>
</tr>
<tr class="even">
<td align="right">16</td>
<td align="right">10</td>
<td align="right">0.63</td>
</tr>
<tr class="odd">
<td align="right">17</td>
<td align="right">10</td>
<td align="right">0.59</td>
</tr>
<tr class="even">
<td align="right">18</td>
<td align="right">10</td>
<td align="right">0.56</td>
</tr>
<tr class="odd">
<td align="right">19</td>
<td align="right">10</td>
<td align="right">0.53</td>
</tr>
<tr class="even">
<td align="right">20</td>
<td align="right">11</td>
<td align="right">0.55</td>
</tr>
</tbody>
</table>
<p>Notice that at the start of the sequence, the <em>proportion</em> of heads fluctuates wildly, starting at .00 and rising as high as .80. Later on, one gets the impression that it dampens out a bit, with more and more of the values actually being pretty close to the “right” answer of .50. This is the frequentist definition of probability in a nutshell: flip a fair coin over and over again, and as <span class="math inline">\(N\)</span> grows large (approaches infinity, denoted <span class="math inline">\(N\rightarrow \infty\)</span>), the proportion of heads will converge to 50%. There are some subtle technicalities that the mathematicians care about, but qualitatively speaking, that’s how the frequentists define probability. Unfortunately, I don’t have an infinite number of coins, or the infinite patience required to flip a coin an infinite number of times. However, I do have a computer, and computers excel at mindless repetitive tasks. So I asked my computer to simulate flipping a coin 1000 times, and then drew a picture of what happens to the proportion <span class="math inline">\(N_H / N\)</span> as <span class="math inline">\(N\)</span> increases. Actually, I did it four times, just to make sure it wasn’t a fluke. The results are shown in Figure <a href="inferential-statistics-the-central-limit-theorem.html#fig:frequentistprobability">4.1</a>. As you can see, the <em>proportion of observed heads</em> eventually stops fluctuating, and settles down; when it does, the number at which it finally settles is the true probability of heads.</p>
<div class="figure"><span style="display:block;" id="fig:frequentistprobability"></span>
<img src="schuster-statistics-remix_files/figure-html/frequentistprobability-1.png" alt="An illustration of how frequentist probability works. If you flip a fair coin over and over again, the proportion of heads that you've seen eventually settles down, and converges to the true probability of 0.5. Each panel shows four different simulated experiments: in each case, we pretend we flipped a coin 1000 times, and kept track of the proportion of flips that were heads as we went along. Although none of these sequences actually ended up with an exact value of .5, if we'd extended the experiment for an infinite number of coin flips they would have." width="672" />
<p class="caption">
Figure 4.1: An illustration of how frequentist probability works. If you flip a fair coin over and over again, the proportion of heads that you’ve seen eventually settles down, and converges to the true probability of 0.5. Each panel shows four different simulated experiments: in each case, we pretend we flipped a coin 1000 times, and kept track of the proportion of flips that were heads as we went along. Although none of these sequences actually ended up with an exact value of .5, if we’d extended the experiment for an infinite number of coin flips they would have.
</p>
</div>
<p>The frequentist definition of probability has some desirable characteristics. Firstly, it is objective: the probability of an event is <em>necessarily</em> grounded in the world. The only way that probability statements can make sense is if they refer to (a sequence of) events that occur in the physical universe.<a href="#fn80" class="footnote-ref" id="fnref80"><sup>80</sup></a> Secondly, it is unambiguous: any two people watching the same sequence of events unfold, trying to calculate the probability of an event, must inevitably come up with the same answer. However, it also has undesirable characteristics. Firstly, infinite sequences don’t exist in the physical world. Suppose you picked up a coin from your pocket and started to flip it. Every time it lands, it impacts on the ground. Each impact wears the coin down a bit; eventually, the coin will be destroyed. So, one might ask whether it really makes sense to pretend that an “infinite” sequence of coin flips is even a meaningful concept, or an objective one. We can’t say that an “infinite sequence” of events is a real thing in the physical universe, because the physical universe doesn’t allow infinite anything. More seriously, the frequentist definition has a narrow scope. There are lots of things out there that human beings are happy to assign probability to in everyday language, but cannot (even in theory) be mapped onto a hypothetical sequence of events. For instance, if a meteorologist comes on TV and says, “the probability of rain in Adelaide on 2 November 2048 is 60%” we humans are happy to accept this. But it’s not clear how to define this in frequentist terms. There’s only one city of Adelaide, and only 2 November 2048. There’s no infinite sequence of events here, just a once-off thing. Frequentist probability genuinely <em>forbids</em> us from making probability statements about a single event. From the frequentist perspective, it will either rain tomorrow or it will not; there is no “probability” that attaches to a single non-repeatable event. Now, it should be said that there are some very clever tricks that frequentists can use to get around this. One possibility is that what the meteorologist means is something like this: “There is a category of days for which I predict a 60% chance of rain; if we look only across those days for which I make this prediction, then on 60% of those days it will actually rain.” It’s very weird and counterintuitive to think of it this way, but you do see frequentists do this sometimes. And it <em>will</em> come up later in this book (see Section <a href="inferential-statistics-the-central-limit-theorem.html#ci">4.9</a>).</p>
</div>
<div id="the-bayesian-view" class="section level3" number="4.4.2">
<h3><span class="header-section-number">4.4.2</span> The Bayesian view</h3>
<p>The <strong><em>Bayesian view</em></strong> of probability is often called the subjectivist view, and it is a minority view among statisticians, but one that has been steadily gaining traction for the last several decades. There are many flavours of Bayesianism, making hard to say exactly what “the” Bayesian view is. The most common way of thinking about subjective probability is to define the probability of an event as the <strong><em>degree of belief</em></strong> that an intelligent and rational agent assigns to that truth of that event. From that perspective, probabilities don’t exist in the world, but rather in the thoughts and assumptions of people and other intelligent beings.
However, in order for this approach to work, we need some way of operationalising “degree of belief.” One way that you can do this is to formalise it in terms of “rational gambling,” though there are many other ways. Suppose that I believe that there’s a 60% probability of rain tomorrow. If someone offers me a bet: if it rains tomorrow, then I win $5, but if it doesn’t rain then I lose $5. Clearly, from my perspective, this is a pretty good bet. On the other hand, if I think that the probability of rain is only 40%, then it’s a bad bet to take. Thus, we can operationalise the notion of a “subjective probability” in terms of what bets I’m willing to accept.</p>
<p>What are the advantages and disadvantages to the Bayesian approach? The main advantage is that it allows you to assign probabilities to any event you want to. You don’t need to be limited to those events that are repeatable. The main disadvantage (to many people) is that we can’t be purely objective – specifying a probability requires us to specify an entity that has the relevant degree of belief. This entity might be a human, an alien, a robot, or even a statistician, but there has to be an intelligent agent out there that believes in things. To many people this is uncomfortable: it seems to make probability arbitrary. While the Bayesian approach does require that the agent in question be rational (i.e., obey the rules of probability), it does allow everyone to have their own beliefs; I can believe the coin is fair and you don’t have to, even though we’re both rational. The frequentist view doesn’t allow any two observers to attribute different probabilities to the same event: when that happens, then at least one of them must be wrong. The Bayesian view does not prevent this from occurring. Two observers with different background knowledge can legitimately hold different beliefs about the same event. In short, where the frequentist view is sometimes considered to be too narrow (forbids lots of things that that we want to assign probabilities to), the Bayesian view is sometimes thought to be too broad (allows too many differences between observers).</p>
</div>
<div id="whats-the-difference-and-who-is-right" class="section level3" number="4.4.3">
<h3><span class="header-section-number">4.4.3</span> What’s the difference? And who is right?</h3>
<p>Now that you’ve seen each of these two views independently, it’s useful to make sure you can compare the two. Go back to the hypothetical robot soccer game at the start of the section. What do you think a frequentist and a Bayesian would say about these three statements? Which statement would a frequentist say is the correct definition of probability? Which one would a Bayesian do? Would some of these statements be meaningless to a frequentist or a Bayesian? If you’ve understood the two perspectives, you should have some sense of how to answer those questions.</p>
<p>Okay, assuming you understand the different, you might be wondering which of them is <em>right</em>? Honestly, I don’t know that there is a right answer. As far as I can tell there’s nothing mathematically incorrect about the way frequentists think about sequences of events, and there’s nothing mathematically incorrect about the way that Bayesians define the beliefs of a rational agent. In fact, when you dig down into the details, Bayesians and frequentists actually agree about a lot of things. Many frequentist methods lead to decisions that Bayesians agree a rational agent would make. Many Bayesian methods have very good frequentist properties.</p>
<p>For the most part, I’m a pragmatist so I’ll use any statistical method that I trust. As it turns out, that makes me prefer Bayesian methods, for reasons I’ll explain towards the end of the book, but I’m not fundamentally opposed to frequentist methods. Not everyone is quite so relaxed. For instance, consider Sir Ronald Fisher, one of the towering figures of 20th century statistics and a vehement opponent to all things Bayesian, whose paper on the mathematical foundations of statistics referred to Bayesian probability as “an impenetrable jungle [that] arrests progress towards precision of statistical concepts” <span class="citation"><a href="#ref-Fisher1922b" role="doc-biblioref">Fisher</a> (<a href="#ref-Fisher1922b" role="doc-biblioref">1922</a>)</span>. Or the psychologist Paul Meehl, who suggests that relying on frequentist methods could turn you into “a potent but sterile intellectual rake who leaves in his merry path a long train of ravished maidens but no viable scientific offspring” <span class="citation"><a href="#ref-Meehl1967" role="doc-biblioref">Meehl</a> (<a href="#ref-Meehl1967" role="doc-biblioref">1967</a>)</span>. The history of statistics, as you might gather, is not devoid of entertainment.</p>
<p>In any case, while I personally prefer the Bayesian view, the majority of statistical analyses are based on the frequentist approach. My reasoning is pragmatic: the goal of this book is to cover roughly the same territory as a typical undergraduate stats class in psychology, and if you want to understand the statistical tools used by most psychologists, you’ll need a good grasp of frequentist methods. I promise you that this isn’t wasted effort. Even if you end up wanting to switch to the Bayesian perspective, you really should read through at least one book on the “orthodox” frequentist view. And since R is the most widely used statistical language for Bayesians, you might as well read a book that uses R. Besides, I won’t completely ignore the Bayesian perspective. Every now and then I’ll add some commentary from a Bayesian point of view.</p>
</div>
</div>
<div id="srs" class="section level2" number="4.5">
<h2><span class="header-section-number">4.5</span> Samples, populations and sampling</h2>
<p>Text by <span class="citation"><a href="#ref-Navarro2018" role="doc-biblioref">Navarro</a> (<a href="#ref-Navarro2018" role="doc-biblioref">2018</a>)</span></p>
<p>In the prelude to Part I discussed the riddle of induction, and highlighted the fact that <em>all</em> learning requires you to make assumptions. Accepting that this is true, our first task to come up with some fairly general assumptions about data that make sense. This is where <strong><em>sampling theory</em></strong> comes in. If probability theory is the foundations upon which all statistical theory builds, sampling theory is the frame around which you can build the rest of the house. Sampling theory plays a huge role in specifying the assumptions upon which your statistical inferences rely. And in order to talk about “making inferences” the way statisticians think about it, we need to be a bit more explicit about what it is that we’re drawing inferences <em>from</em> (the sample) and what it is that we’re drawing inferences <em>about</em> (the population).</p>
<p>In almost every situation of interest, what we have available to us as researchers is a <strong><em>sample</em></strong> of data. We might have run experiment with some number of participants; a polling company might have phoned some number of people to ask questions about voting intentions; etc. Regardless: the data set available to us is finite, and incomplete. We can’t possibly get every person in the world to do our experiment; a polling company doesn’t have the time or the money to ring up every voter in the country etc. In our earlier discussion of descriptive statistics (Chapter <a href="descriptives.html#descriptives">3</a>, this sample was the only thing we were interested in. Our only goal was to find ways of describing, summarising and graphing that sample. This is about to change.</p>
<div id="pop" class="section level3" number="4.5.1">
<h3><span class="header-section-number">4.5.1</span> Defining a population</h3>
<p>A sample is a concrete thing. You can open up a data file, and there’s the data from your sample. A <strong><em>population</em></strong>, on the other hand, is a more abstract idea. It refers to the set of all possible people, or all possible observations, that you want to draw conclusions about, and is generally <em>much</em> bigger than the sample. In an ideal world, the researcher would begin the study with a clear idea of what the population of interest is, since the process of designing a study and testing hypotheses about the data that it produces does depend on the population about which you want to make statements. However, that doesn’t always happen in practice: usually the researcher has a fairly vague idea of what the population is and designs the study as best he/she can on that basis.</p>
<p>Sometimes it’s easy to state the population of interest. For instance, in the “polling company” example that opened the chapter, the population consisted of all voters enrolled at the a time of the study – millions of people. The sample was a set of 1000 people who all belong to that population. In most situations the situation is much less simple. In a typical a psychological experiment, determining the population of interest is a bit more complicated. Suppose I run an experiment using 100 undergraduate students as my participants. My goal, as a cognitive scientist, is to try to learn something about how the mind works. So, which of the following would count as “the population”:</p>
<ul>
<li>All of the undergraduate psychology students at the University of Adelaide?</li>
<li>Undergraduate psychology students in general, anywhere in the world?</li>
<li>Australians currently living?</li>
<li>Australians of similar ages to my sample?</li>
<li>Anyone currently alive?</li>
<li>Any human being, past, present or future?</li>
<li>Any biological organism with a sufficient degree of intelligence operating in a terrestrial environment?</li>
<li>Any intelligent being?</li>
</ul>
<p>Each of these defines a real group of mind-possessing entities, all of which might be of interest to me as a cognitive scientist, and it’s not at all clear which one ought to be the true population of interest. As another example, consider the Wellesley-Croker game that we discussed in the prelude. The sample here is a specific sequence of 12 wins and 0 losses for Wellesley. What is the population?</p>
<ul>
<li>All outcomes until Wellesley and Croker arrived at their destination?</li>
<li>All outcomes if Wellesley and Croker had played the game for the rest of their lives?</li>
<li>All outcomes if Wellseley and Croker lived forever and played the game until the world ran out of hills?</li>
<li>All outcomes if we created an infinite set of parallel universes and the Wellesely/Croker pair made guesses about the same 12 hills in each universe?</li>
</ul>
<p>Again, it’s not obvious what the population is.</p>
</div>
<div id="simple-random-samples" class="section level3" number="4.5.2">
<h3><span class="header-section-number">4.5.2</span> Simple random samples</h3>
<div class="figure"><span style="display:block;" id="fig:srs1"></span>
<img src="img/estimation/srs1.png" alt="Simple random sampling without replacement from a finite population" width="584" />
<p class="caption">
Figure 4.2: Simple random sampling without replacement from a finite population
</p>
</div>
<p>Irrespective of how I define the population, the critical point is that the sample is a subset of the population, and our goal is to use our knowledge of the sample to draw inferences about the properties of the population. The relationship between the two depends on the <em>procedure</em> by which the sample was selected. This procedure is referred to as a <strong><em>sampling method</em></strong>, and it is important to understand why it matters.</p>
<p>To keep things simple, let’s imagine that we have a bag containing 10 chips. Each chip has a unique letter printed on it, so we can distinguish between the 10 chips. The chips come in two colours, black and white. This set of chips is the population of interest, and it is depicted graphically on the left of Figure <a href="inferential-statistics-the-central-limit-theorem.html#fig:srs1">4.2</a>. As you can see from looking at the picture, there are 4 black chips and 6 white chips, but of course in real life we wouldn’t know that unless we looked in the bag. Now imagine you run the following “experiment”: you shake up the bag, close your eyes, and pull out 4 chips without putting any of them back into the bag. First out comes the <span class="math inline">\(a\)</span> chip (black), then the <span class="math inline">\(c\)</span> chip (white), then <span class="math inline">\(j\)</span> (white) and then finally <span class="math inline">\(b\)</span> (black). If you wanted, you could then put all the chips back in the bag and repeat the experiment, as depicted on the right hand side of Figure <a href="inferential-statistics-the-central-limit-theorem.html#fig:srs1">4.2</a>. Each time you get different results, but the procedure is identical in each case. The fact that the same procedure can lead to different results each time, we refer to it as a <em>random</em> process.<a href="#fn81" class="footnote-ref" id="fnref81"><sup>81</sup></a> However, because we shook the bag before pulling any chips out, it seems reasonable to think that every chip has the same chance of being selected. A procedure in which every member of the population has the same chance of being selected is called a <strong><em>simple random sample</em></strong>. The fact that we did <em>not</em> put the chips back in the bag after pulling them out means that you can’t observe the same thing twice, and in such cases the observations are said to have been sampled <strong><em>without replacement</em></strong>.</p>
<p>To help make sure you understand the importance of the sampling procedure, consider an alternative way in which the experiment could have been run. Suppose that my 5-year old son had opened the bag, and decided to pull out four black chips without putting any of them back in the bag. This <em>biased</em> sampling scheme is depicted in Figure <a href="inferential-statistics-the-central-limit-theorem.html#fig:brs">4.3</a>. Now consider the evidentiary value of seeing 4 black chips and 0 white chips. Clearly, it depends on the sampling scheme, does it not? If you know that the sampling scheme is biased to select only black chips, then a sample that consists of only black chips doesn’t tell you very much about the population! For this reason, statisticians really like it when a data set can be considered a simple random sample, because it makes the data analysis <em>much</em> easier.</p>
<div class="figure"><span style="display:block;" id="fig:brs"></span>
<img src="img/estimation/brs.png" alt="Biased sampling without replacement from a finite population" width="572" />
<p class="caption">
Figure 4.3: Biased sampling without replacement from a finite population
</p>
</div>
<div class="figure"><span style="display:block;" id="fig:srs2"></span>
<img src="img/estimation/srs2.png" alt="Simple random sampling *with* replacement from a finite population" width="584" />
<p class="caption">
Figure 4.4: Simple random sampling <em>with</em> replacement from a finite population
</p>
</div>
<p>A third procedure is worth mentioning. This time around we close our eyes, shake the bag, and pull out a chip. This time, however, we record the observation and then put the chip back in the bag. Again we close our eyes, shake the bag, and pull out a chip. We then repeat this procedure until we have 4 chips. Data sets generated in this way are still simple random samples, but because we put the chips back in the bag immediately after drawing them it is referred to as a sample <strong><em>with replacement</em></strong>. The difference between this situation and the first one is that it is possible to observe the same population member multiple times, as illustrated in Figure <a href="inferential-statistics-the-central-limit-theorem.html#fig:srs2">4.4</a>.</p>
<p>In my experience, most psychology experiments tend to be sampling without replacement, because the same person is not allowed to participate in the experiment twice. However, most statistical theory is based on the assumption that the data arise from a simple random sample <em>with</em> replacement. In real life, this very rarely matters. If the population of interest is large (e.g., has more than 10 entities!) the difference between sampling with- and without- replacement is too small to be concerned with. The difference between simple random samples and biased samples, on the other hand, is not such an easy thing to dismiss.</p>
</div>
<div id="most-samples-are-not-simple-random-samples" class="section level3" number="4.5.3">
<h3><span class="header-section-number">4.5.3</span> Most samples are not simple random samples</h3>
<p>As you can see from looking at the list of possible populations that I showed above, it is almost impossible to obtain a simple random sample from most populations of interest. When I run experiments, I’d consider it a minor miracle if my participants turned out to be a random sampling of the undergraduate psychology students at Adelaide university, even though this is by far the narrowest population that I might want to generalise to. A thorough discussion of other types of sampling schemes is beyond the scope of this book, but to give you a sense of what’s out there I’ll list a few of the more important ones:</p>
<ul>
<li><em>Stratified sampling</em>. Suppose your population is (or can be) divided into several different subpopulations, or <em>strata</em>. Perhaps you’re running a study at several different sites, for example. Instead of trying to sample randomly from the population as a whole, you instead try to collect a separate random sample from each of the strata. Stratified sampling is sometimes easier to do than simple random sampling, especially when the population is already divided into the distinct strata. It can also be more efficient that simple random sampling, especially when some of the subpopulations are rare. For instance, when studying schizophrenia it would be much better to divide the population into two<a href="#fn82" class="footnote-ref" id="fnref82"><sup>82</sup></a> strata (schizophrenic and not-schizophrenic), and then sample an equal number of people from each group. If you selected people randomly, you would get so few schizophrenic people in the sample that your study would be useless. This specific kind of of stratified sampling is referred to as <em>oversampling</em> because it makes a deliberate attempt to over-represent rare groups.</li>
<li><em>Snowball sampling</em> is a technique that is especially useful when sampling from a “hidden” or hard to access population, and is especially common in social sciences. For instance, suppose the researchers want to conduct an opinion poll among transgender people. The research team might only have contact details for a few trans folks, so the survey starts by asking them to participate (stage 1). At the end of the survey, the participants are asked to provide contact details for other people who might want to participate. In stage 2, those new contacts are surveyed. The process continues until the researchers have sufficient data. The big advantage to snowball sampling is that it gets you data in situations that might otherwise be impossible to get any. On the statistical side, the main disadvantage is that the sample is highly non-random, and non-random in ways that are difficult to address. On the real life side, the disadvantage is that the procedure can be unethical if not handled well, because hidden populations are often hidden for a reason. I chose transgender people as an example here to highlight this: if you weren’t careful you might end up outing people who don’t want to be outed (very, very bad form), and even if you don’t make that mistake it can still be intrusive to use people’s social networks to study them. It’s certainly very hard to get people’s informed consent <em>before</em> contacting them, yet in many cases the simple act of contacting them and saying “hey we want to study you” can be hurtful. Social networks are complex things, and just because you can use them to get data doesn’t always mean you should.</li>
<li><em>Convenience sampling</em> is more or less what it sounds like. The samples are chosen in a way that is convenient to the researcher, and not selected at random from the population of interest. Snowball sampling is one type of convenience sampling, but there are many others. A common example in psychology are studies that rely on undergraduate psychology students. These samples are generally non-random in two respects: firstly, reliance on undergraduate psychology students automatically means that your data are restricted to a single subpopulation. Secondly, the students usually get to pick which studies they participate in, so the sample is a self selected subset of psychology students not a randomly selected subset. In real life, most studies are convenience samples of one form or another. This is sometimes a severe limitation, but not always.</li>
</ul>
<p><strong>Dave here</strong>, adding two more sampling methods:</p>
<ul>
<li><p><em>Systematic sampling</em>: Starting from a random point, select every Nth participant.</p></li>
<li><p><em>Cluster sampling</em>: Divide population into clusters or units (such as schools), take a random sample of the clusters (i.e., randomly select a school) and then measure all the participants within the cluster (i.e., measure every student in the school).</p></li>
</ul>
</div>
<div id="how-much-does-it-matter-if-you-dont-have-a-simple-random-sample" class="section level3" number="4.5.4">
<h3><span class="header-section-number">4.5.4</span> How much does it matter if you don’t have a simple random sample?</h3>
<p>Okay, so real world data collection tends not to involve nice simple random samples. Does that matter? A little thought should make it clear to you that it <em>can</em> matter if your data are not a simple random sample: just think about the difference between Figures <a href="inferential-statistics-the-central-limit-theorem.html#fig:srs1">4.2</a> and <a href="inferential-statistics-the-central-limit-theorem.html#fig:brs">4.3</a>. However, it’s not quite as bad as it sounds. Some types of biased samples are entirely unproblematic. For instance, when using a stratified sampling technique you actually <em>know</em> what the bias is because you created it deliberately, often to <em>increase</em> the effectiveness of your study, and there are statistical techniques that you can use to adjust for the biases you’ve introduced (not covered in this book!). So in those situations it’s not a problem.</p>
<p>More generally though, it’s important to remember that random sampling is a means to an end, not the end in itself. Let’s assume you’ve relied on a convenience sample, and as such you can assume it’s biased. A bias in your sampling method is only a problem if it causes you to draw the wrong conclusions. When viewed from that perspective, I’d argue that we don’t need the sample to be randomly generated in <em>every</em> respect: we only need it to be random with respect to the psychologically-relevant phenomenon of interest. Suppose I’m doing a study looking at working memory capacity. In study 1, I actually have the ability to sample randomly from all human beings currently alive, with one exception: I can only sample people born on a Monday. In study 2, I am able to sample randomly from the Australian population. I want to generalise my results to the population of all living humans. Which study is better? The answer, obviously, is study 1. Why? Because we have no reason to think that being “born on a Monday” has any interesting relationship to working memory capacity. In contrast, I can think of several reasons why “being Australian” might matter. Australia is a wealthy, industrialised country with a very well-developed education system. People growing up in that system will have had life experiences much more similar to the experiences of the people who designed the tests for working memory capacity. This shared experience might easily translate into similar beliefs about how to “take a test,” a shared assumption about how psychological experimentation works, and so on. These things might actually matter. For instance, “test taking” style might have taught the Australian participants how to direct their attention exclusively on fairly abstract test materials relative to people that haven’t grown up in a similar environment; leading to a misleading picture of what working memory capacity is.</p>
<p>There are two points hidden in this discussion. Firstly, when designing your own studies, it’s important to think about what population you care about, and try hard to sample in a way that is appropriate to that population. In practice, you’re usually forced to put up with a “sample of convenience” (e.g., psychology lecturers sample psychology students because that’s the least expensive way to collect data, and our coffers aren’t exactly overflowing with gold), but if so you should at least spend some time thinking about what the dangers of this practice might be.</p>
<p>Secondly, if you’re going to criticise someone else’s study because they’ve used a sample of convenience rather than laboriously sampling randomly from the entire human population, at least have the courtesy to offer a specific theory as to <em>how</em> this might have distorted the results. Remember, everyone in science is aware of this issue, and does what they can to alleviate it. Merely pointing out that “the study only included people from group BLAH” is entirely unhelpful, and borders on being insulting to the researchers, who are <em>of course</em> aware of the issue. They just don’t happen to be in possession of the infinite supply of time and money required to construct the perfect sample. In short, if you want to offer a responsible critique of the sampling process, then be <em>helpful</em>. Rehashing the blindingly obvious truisms that I’ve been rambling on about in this section isn’t helpful.</p>
</div>
<div id="population-parameters-and-sample-statistics" class="section level3" number="4.5.5">
<h3><span class="header-section-number">4.5.5</span> Population parameters and sample statistics</h3>
<p>Okay. Setting aside the thorny methodological issues associated with obtaining a random sample and my rather unfortunate tendency to rant about lazy methodological criticism, let’s consider a slightly different issue. Up to this point we have been talking about populations the way a scientist might. To a psychologist, a population might be a group of people. To an ecologist, a population might be a group of bears. In most cases the populations that scientists care about are concrete things that actually exist in the real world. Statisticians, however, are a funny lot. On the one hand, they <em>are</em> interested in real world data and real science in the same way that scientists are. On the other hand, they also operate in the realm of pure abstraction in the way that mathematicians do. As a consequence, statistical theory tends to be a bit abstract in how a population is defined. In much the same way that psychological researchers operationalise our abstract theoretical ideas in terms of concrete measurements (Section <a href="statistics-for-research.html#measurement">1.2</a>, statisticians operationalise the concept of a “population” in terms of mathematical objects that they know how to work with. They’re called probability distributions.</p>
<p>The idea is quite simple. Let’s say we’re talking about IQ scores. To a psychologist, the population of interest is a group of actual humans who have IQ scores. A statistician “simplifies” this by operationally defining the population as the probability distribution depicted in Figure <a href="#fig:IQdist"><strong>??</strong></a>. IQ tests are designed so that the average IQ is 100, the standard deviation of IQ scores is 15, and the distribution of IQ scores is normal. These values are referred to as the <strong><em>population parameters</em></strong> because they are characteristics of the entire population. That is, we say that the population mean <span class="math inline">\(\mu\)</span> is 100, and the population standard deviation <span class="math inline">\(\sigma\)</span> is 15.</p>
<div class="figure"><span style="display:block;" id="fig:IQdist-1"></span>
<img src="schuster-statistics-remix_files/figure-html/IQdist-1.png" alt="The population distribution of IQ scores (panel a) and two samples drawn randomly from it. In panel b we have a sample of 100 observations, and panel c we have a sample of 10,000 observations." width="672" />
<p class="caption">
Figure 4.5: The population distribution of IQ scores (panel a) and two samples drawn randomly from it. In panel b we have a sample of 100 observations, and panel c we have a sample of 10,000 observations.
</p>
</div>
<div class="figure"><span style="display:block;" id="fig:IQdist-2"></span>
<img src="schuster-statistics-remix_files/figure-html/IQdist-2.png" alt="The population distribution of IQ scores (panel a) and two samples drawn randomly from it. In panel b we have a sample of 100 observations, and panel c we have a sample of 10,000 observations." width="672" />
<p class="caption">
Figure 4.6: The population distribution of IQ scores (panel a) and two samples drawn randomly from it. In panel b we have a sample of 100 observations, and panel c we have a sample of 10,000 observations.
</p>
</div>
<pre><code>## [1] &quot;n= 100 mean= 102.514476752168 sd= 15.7372430972857&quot;</code></pre>
<div class="figure"><span style="display:block;" id="fig:IQdist-3"></span>
<img src="schuster-statistics-remix_files/figure-html/IQdist-3.png" alt="The population distribution of IQ scores (panel a) and two samples drawn randomly from it. In panel b we have a sample of 100 observations, and panel c we have a sample of 10,000 observations." width="672" />
<p class="caption">
Figure 4.7: The population distribution of IQ scores (panel a) and two samples drawn randomly from it. In panel b we have a sample of 100 observations, and panel c we have a sample of 10,000 observations.
</p>
</div>
<pre><code>## [1] &quot;n= 10000 mean= 99.922922759723 sd= 14.7673900768644&quot;</code></pre>
<p>Now suppose I run an experiment. I select 100 people at random and administer an IQ test, giving me a simple random sample from the population. My sample would consist of a collection of numbers like this:</p>
<pre><code>                          106 101 98 80 74 ... 107 72 100</code></pre>
<p>Each of these IQ scores is sampled from a normal distribution with mean 100 and standard deviation 15. So if I plot a histogram of the sample, I get something like the one shown in Figure <a href="#fig:IQdist"><strong>??</strong></a>b. As you can see, the histogram is <em>roughly</em> the right shape, but it’s a very crude approximation to the true population distribution shown in Figure <a href="#fig:IQdist"><strong>??</strong></a>a. When I calculate the mean of my sample, I get a number that is fairly close to the population mean 100 but not identical. In this case, it turns out that the people in my sample have a mean IQ of 98.5, and the standard deviation of their IQ scores is 15.9. These <strong><em>sample statistics</em></strong> are properties of my data set, and although they are fairly similar to the true population values, they are not the same. In general, sample statistics are the things you can calculate from your data set, and the population parameters are the things you want to learn about. Later on in this chapter I’ll talk about how you can estimate population parameters using your sample statistics (Section <a href="inferential-statistics-the-central-limit-theorem.html#pointestimates">4.8</a> and how to work out how confident you are in your estimates (Section <a href="inferential-statistics-the-central-limit-theorem.html#ci">4.9</a> but before we get to that there’s a few more ideas in sampling theory that you need to know about.</p>
</div>
</div>
<div id="lawlargenumbers" class="section level2" number="4.6">
<h2><span class="header-section-number">4.6</span> The law of large numbers</h2>
<p>Text by <span class="citation"><a href="#ref-Navarro2018" role="doc-biblioref">Navarro</a> (<a href="#ref-Navarro2018" role="doc-biblioref">2018</a>)</span></p>
<p>In the previous section I showed you the results of one fictitious IQ experiment with a sample size of <span class="math inline">\(N=100\)</span>. The results were somewhat encouraging: the true population mean is 100, and the sample mean of 98.5 is a pretty reasonable approximation to it. In many scientific studies that level of precision is perfectly acceptable, but in other situations you need to be a lot more precise. If we want our sample statistics to be much closer to the population parameters, what can we do about it?</p>
<p>The obvious answer is to collect more data. Suppose that we ran a much larger experiment, this time measuring the IQs of 10,000 people. We can simulate the results of this experiment using R. The <code>rnorm()</code> function generates random numbers sampled from a normal distribution. For an experiment with a sample size of <code>n = 10000</code>, and a population with <code>mean = 100</code> and <code>sd = 15</code>, R produces our fake IQ data using these commands:</p>
<div class="sourceCode" id="cb485"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb485-1"><a href="inferential-statistics-the-central-limit-theorem.html#cb485-1" aria-hidden="true" tabindex="-1"></a>IQ <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="at">n =</span> <span class="dv">10000</span>, <span class="at">mean =</span> <span class="dv">100</span>, <span class="at">sd =</span> <span class="dv">15</span>) <span class="co"># generate IQ scores</span></span>
<span id="cb485-2"><a href="inferential-statistics-the-central-limit-theorem.html#cb485-2" aria-hidden="true" tabindex="-1"></a>IQ <span class="ot">&lt;-</span> <span class="fu">round</span>(IQ) <span class="co"># IQs are whole numbers!</span></span>
<span id="cb485-3"><a href="inferential-statistics-the-central-limit-theorem.html#cb485-3" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">head</span>(IQ))</span></code></pre></div>
<pre><code>## [1] 105  93 104 104 108  91</code></pre>
<p>I can compute the mean IQ using the command <code>mean(IQ)</code> and the standard deviation using the command <code>sd(IQ)</code>, and I can draw a histgram using <code>hist()</code>. The histogram of this much larger sample is shown in Figure <a href="#fig:IQdist"><strong>??</strong></a> c. Even a moment’s inspections makes clear that the larger sample is a much better approximation to the true population distribution than the smaller one. This is reflected in the sample statistics: the mean IQ for the larger sample turns out to be 99.9, and the standard deviation is 15.1. These values are now very close to the true population.</p>
<p>I feel a bit silly saying this, but the thing I want you to take away from this is that large samples generally give you better information. I feel silly saying it because it’s so bloody obvious that it shouldn’t need to be said. In fact, it’s such an obvious point that when Jacob Bernoulli – one of the founders of probability theory – formalised this idea back in 1713, he was kind of a jerk about it. Here’s how he described the fact that we all share this intuition:</p>
<blockquote>
<p><em>For even the most stupid of men, by some instinct of nature, by himself and without any instruction (which is a remarkable thing), is convinced that the more observations have been made, the less danger there is of wandering from one’s goal</em> <span class="citation"><a href="#ref-Stigler1986" role="doc-biblioref">Stigler</a> (<a href="#ref-Stigler1986" role="doc-biblioref">1986</a>)</span></p>
</blockquote>
<p>Okay, so the passage comes across as a bit condescending (not to mention sexist), but his main point is correct: it really does feel obvious that more data will give you better answers. The question is, why is this so? Not surprisingly, this intuition that we all share turns out to be correct, and statisticians refer to it as the <strong><em>law of large numbers</em></strong>. The law of large numbers is a mathematical law that applies to many different sample statistics, but the simplest way to think about it is as a law about averages. The sample mean is the most obvious example of a statistic that relies on averaging (because that’s what the mean is… an average), so let’s look at that. When applied to the sample mean, what the law of large numbers states is that as the sample gets larger, the sample mean tends to get closer to the true population mean. Or, to say it a little bit more precisely, as the sample size “approaches” infinity (written as <span class="math inline">\(N \rightarrow \infty\)</span>) the sample mean approaches the population mean (<span class="math inline">\(\bar{X} \rightarrow \mu\)</span>).<a href="#fn83" class="footnote-ref" id="fnref83"><sup>83</sup></a></p>
<p>I don’t intend to subject you to a proof that the law of large numbers is true, but it’s one of the most important tools for statistical theory. The law of large numbers is the thing we can use to justify our belief that collecting more and more data will eventually lead us to the truth. For any particular data set, the sample statistics that we calculate from it will be wrong, but the law of large numbers tells us that if we keep collecting more data those sample statistics will tend to get closer and closer to the true population parameters.</p>
</div>
<div id="samplesandclt" class="section level2" number="4.7">
<h2><span class="header-section-number">4.7</span> Sampling distributions and the central limit theorem</h2>
<p>Text by <span class="citation"><a href="#ref-Navarro2018" role="doc-biblioref">Navarro</a> (<a href="#ref-Navarro2018" role="doc-biblioref">2018</a>)</span></p>
<p>The law of large numbers is a very powerful tool, but it’s not going to be good enough to answer all our questions. Among other things, all it gives us is a “long run guarantee.” In the long run, if we were somehow able to collect an infinite amount of data, then the law of large numbers guarantees that our sample statistics will be correct. But as John Maynard Keynes famously argued in economics, a long run guarantee is of little use in real life:</p>
<blockquote>
<p><em>[The] long run is a misleading guide to current affairs. In the long run we are all dead. Economists set themselves too easy, too useless a task, if in tempestuous seasons they can only tell us, that when the storm is long past, the ocean is flat again.</em> <span class="citation"><a href="#ref-Keynes1923" role="doc-biblioref">Keynes</a> (<a href="#ref-Keynes1923" role="doc-biblioref">1923</a>)</span></p>
</blockquote>
<p>As in economics, so too in psychology and statistics. It is not enough to know that we will <em>eventually</em> arrive at the right answer when calculating the sample mean. Knowing that an infinitely large data set will tell me the exact value of the population mean is cold comfort when my <em>actual</em> data set has a sample size of <span class="math inline">\(N=100\)</span>. In real life, then, we must know something about the behaviour of the sample mean when it is calculated from a more modest data set!</p>
<div id="samplingdists" class="section level3" number="4.7.1">
<h3><span class="header-section-number">4.7.1</span> Sampling distribution of the mean</h3>
<p>With this in mind, let’s abandon the idea that our studies will have sample sizes of 10000, and consider a very modest experiment indeed. This time around we’ll sample <span class="math inline">\(N=5\)</span> people and measure their IQ scores. As before, I can simulate this experiment in R using the <code>rnorm()</code> function:</p>
<pre><code>&gt; IQ.1 &lt;- round( rnorm(n=5, mean=100, sd=15 ))
&gt; IQ.1
[1]  90  82  94  99 110</code></pre>
<p>The mean IQ in this sample turns out to be exactly 95. Not surprisingly, this is much less accurate than the previous experiment. Now imagine that I decided to <strong><em>replicate</em></strong> the experiment. That is, I repeat the procedure as closely as possible: I randomly sample 5 new people and measure their IQ. Again, R allows me to simulate the results of this procedure:</p>
<pre><code>&gt; IQ.2 &lt;- round( rnorm(n=5, mean=100, sd=15 ))
&gt; IQ.2
[1]  78  88 111 111 117</code></pre>
<p>This time around, the mean IQ in my sample is 101. If I repeat the experiment 10 times I obtain the results shown in Table <a href="#tab:replications"><strong>??</strong></a>, and as you can see the sample mean varies from one replication to the next.</p>
<table>
<colgroup>
<col width="10%" />
<col width="6%" />
<col width="6%" />
<col width="6%" />
<col width="6%" />
<col width="6%" />
<col width="8%" />
<col width="50%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">NANA</th>
<th align="right">Person.1</th>
<th align="right">Person.2</th>
<th align="right">Person.3</th>
<th align="right">Person.4</th>
<th align="right">Person.5</th>
<th align="right">Sample.Mean</th>
<th align="left">caption</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Replication 1</td>
<td align="right">90</td>
<td align="right">82</td>
<td align="right">94</td>
<td align="right">99</td>
<td align="right">110</td>
<td align="right">95.0</td>
<td align="left">Ten replications of the IQ experiment, each with a sample size of <span class="math inline">\(N=5\)</span>.</td>
</tr>
<tr class="even">
<td align="left">Replication 2</td>
<td align="right">78</td>
<td align="right">88</td>
<td align="right">111</td>
<td align="right">111</td>
<td align="right">117</td>
<td align="right">101.0</td>
<td align="left">Ten replications of the IQ experiment, each with a sample size of <span class="math inline">\(N=5\)</span>.</td>
</tr>
<tr class="odd">
<td align="left">Replication 3</td>
<td align="right">111</td>
<td align="right">122</td>
<td align="right">91</td>
<td align="right">98</td>
<td align="right">86</td>
<td align="right">101.6</td>
<td align="left">Ten replications of the IQ experiment, each with a sample size of <span class="math inline">\(N=5\)</span>.</td>
</tr>
<tr class="even">
<td align="left">Replication 4</td>
<td align="right">98</td>
<td align="right">96</td>
<td align="right">119</td>
<td align="right">99</td>
<td align="right">107</td>
<td align="right">103.8</td>
<td align="left">Ten replications of the IQ experiment, each with a sample size of <span class="math inline">\(N=5\)</span>.</td>
</tr>
<tr class="odd">
<td align="left">Replication 5</td>
<td align="right">105</td>
<td align="right">113</td>
<td align="right">103</td>
<td align="right">103</td>
<td align="right">98</td>
<td align="right">104.4</td>
<td align="left">Ten replications of the IQ experiment, each with a sample size of <span class="math inline">\(N=5\)</span>.</td>
</tr>
<tr class="even">
<td align="left">Replication 6</td>
<td align="right">81</td>
<td align="right">89</td>
<td align="right">93</td>
<td align="right">85</td>
<td align="right">114</td>
<td align="right">92.4</td>
<td align="left">Ten replications of the IQ experiment, each with a sample size of <span class="math inline">\(N=5\)</span>.</td>
</tr>
<tr class="odd">
<td align="left">Replication 7</td>
<td align="right">100</td>
<td align="right">93</td>
<td align="right">108</td>
<td align="right">98</td>
<td align="right">133</td>
<td align="right">106.4</td>
<td align="left">Ten replications of the IQ experiment, each with a sample size of <span class="math inline">\(N=5\)</span>.</td>
</tr>
<tr class="even">
<td align="left">Replication 8</td>
<td align="right">107</td>
<td align="right">100</td>
<td align="right">105</td>
<td align="right">117</td>
<td align="right">85</td>
<td align="right">102.8</td>
<td align="left">Ten replications of the IQ experiment, each with a sample size of <span class="math inline">\(N=5\)</span>.</td>
</tr>
<tr class="odd">
<td align="left">Replication 9</td>
<td align="right">86</td>
<td align="right">119</td>
<td align="right">108</td>
<td align="right">73</td>
<td align="right">116</td>
<td align="right">100.4</td>
<td align="left">Ten replications of the IQ experiment, each with a sample size of <span class="math inline">\(N=5\)</span>.</td>
</tr>
<tr class="even">
<td align="left">Replication 10</td>
<td align="right">95</td>
<td align="right">126</td>
<td align="right">112</td>
<td align="right">120</td>
<td align="right">76</td>
<td align="right">105.8</td>
<td align="left">Ten replications of the IQ experiment, each with a sample size of <span class="math inline">\(N=5\)</span>.</td>
</tr>
</tbody>
</table>
<p>Now suppose that I decided to keep going in this fashion, replicating this “five IQ scores” experiment over and over again. Every time I replicate the experiment I write down the sample mean. Over time, I’d be amassing a new data set, in which every experiment generates a single data point. The first 10 observations from my data set are the sample means listed in Table <a href="#tab:replications"><strong>??</strong></a>, so my data set starts out like this:</p>
<pre><code>                      95.0 101.0 101.6 103.8 104.4 ...</code></pre>
<p>What if I continued like this for 10,000 replications, and then drew a histogram? Using the magical powers of R that’s exactly what I did, and you can see the results in Figure <a href="inferential-statistics-the-central-limit-theorem.html#fig:sampdistmean">4.8</a>. As this picture illustrates, the average of 5 IQ scores is usually between 90 and 110. But more importantly, what it highlights is that if we replicate an experiment over and over again, what we end up with is a <em>distribution</em> of sample means! This distribution has a special name in statistics: it’s called the <strong><em>sampling distribution of the mean</em></strong>.</p>
<p>Sampling distributions are another important theoretical idea in statistics, and they’re crucial for understanding the behaviour of small samples. For instance, when I ran the very first “five IQ scores” experiment, the sample mean turned out to be 95. What the sampling distribution in Figure <a href="inferential-statistics-the-central-limit-theorem.html#fig:sampdistmean">4.8</a> tells us, though, is that the “five IQ scores” experiment is not very accurate. If I repeat the experiment, the sampling distribution tells me that I can expect to see a sample mean anywhere between 80 and 120.</p>
<div class="figure"><span style="display:block;" id="fig:sampdistmean"></span>
<img src="img/estimation/sampleDist4.png" alt="The sampling distribution of the mean for the &quot;five IQ scores experiment&quot;. If you sample 5 people at random and calculate their *average* IQ, you'll almost certainly get a number between 80 and 120, even though there are quite a lot of individuals who have IQs above 120 or below 80. For comparison, the black line plots the population distribution of IQ scores." width="338" />
<p class="caption">
Figure 4.8: The sampling distribution of the mean for the “five IQ scores experiment.” If you sample 5 people at random and calculate their <em>average</em> IQ, you’ll almost certainly get a number between 80 and 120, even though there are quite a lot of individuals who have IQs above 120 or below 80. For comparison, the black line plots the population distribution of IQ scores.
</p>
</div>
<div class="figure"><span style="display:block;" id="fig:sampdistmax"></span>
<img src="img/estimation/sampleDistMax.png" alt="The sampling distribution of the *maximum* for the &quot;five IQ scores experiment&quot;. If you sample 5 people at random and select the one with the highest IQ score, you'll probably see someone with an IQ between 100 and 140." width="338" />
<p class="caption">
Figure 4.9: The sampling distribution of the <em>maximum</em> for the “five IQ scores experiment.” If you sample 5 people at random and select the one with the highest IQ score, you’ll probably see someone with an IQ between 100 and 140.
</p>
</div>
<p>With an explanation of sampling distributions out of the way, Dave will now explain some additional detail behind this important concept.</p>
</div>
<div id="sample-size-and-population-size" class="section level3" number="4.7.2">
<h3><span class="header-section-number">4.7.2</span> Sample size and population size</h3>
<p>Text by David Schuster</p>
<p>The size of a distribution is the number of units it contains. In mathematics, sample size is typically represented as <span class="math inline">\(n\)</span> and population size is typically represented as <span class="math inline">\(N\)</span>. In APA-style writing, however, sample size is represented as <span class="math inline">\(N\)</span>, and <span class="math inline">\(n\)</span> is used to represent a subsample (a part of a sample, such as the units in one condition). There does not seem to be a recommended symbol for population size in APA style. Why not? Many times, the population size is unknown.</p>
</div>
<div id="sampling-error" class="section level3" number="4.7.3">
<h3><span class="header-section-number">4.7.3</span> Sampling error</h3>
<p>Text by David Schuster</p>
<p>Sampling error is the mismatch between a sample statistic and a population parameter. As we have seen, taking a single random sample does not guarantee perfect representation of a population. Let’s generate a population distribution, take a single random sample, and then compare the population and sample distributions:</p>
<div class="sourceCode" id="cb490"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb490-1"><a href="inferential-statistics-the-central-limit-theorem.html#cb490-1" aria-hidden="true" tabindex="-1"></a>population <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">1000</span>, <span class="dv">1</span>, <span class="dv">100</span>) <span class="co"># Generate a variable called &#39;population&#39; with 1000 values chosen at random between 1 and 100</span></span>
<span id="cb490-2"><a href="inferential-statistics-the-central-limit-theorem.html#cb490-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(population) <span class="co"># Display the mean of the population distribution</span></span>
<span id="cb490-3"><a href="inferential-statistics-the-central-limit-theorem.html#cb490-3" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 50.84139</span></span>
<span id="cb490-4"><a href="inferential-statistics-the-central-limit-theorem.html#cb490-4" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(population) <span class="co"># Display a histogram of the population distribution</span></span></code></pre></div>
<p><img src="schuster-statistics-remix_files/figure-html/unnamed-chunk-240-1.png" width="672" /></p>
<div class="sourceCode" id="cb491"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb491-1"><a href="inferential-statistics-the-central-limit-theorem.html#cb491-1" aria-hidden="true" tabindex="-1"></a>onesample <span class="ot">&lt;-</span> <span class="fu">sample</span>(population, <span class="at">size =</span> <span class="dv">3</span>, <span class="at">replace =</span> <span class="cn">TRUE</span>) <span class="co"># Take a random sample of size 3 from the population distribution with replacement</span></span>
<span id="cb491-2"><a href="inferential-statistics-the-central-limit-theorem.html#cb491-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(onesample) <span class="co"># Display the mean of the sample distribution</span></span>
<span id="cb491-3"><a href="inferential-statistics-the-central-limit-theorem.html#cb491-3" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 46.50254</span></span>
<span id="cb491-4"><a href="inferential-statistics-the-central-limit-theorem.html#cb491-4" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(onesample) <span class="co"># Display a histogram of the sample distribution</span></span></code></pre></div>
<p><img src="schuster-statistics-remix_files/figure-html/unnamed-chunk-240-2.png" width="672" /></p>
<div class="sourceCode" id="cb492"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb492-1"><a href="inferential-statistics-the-central-limit-theorem.html#cb492-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(onesample) <span class="sc">-</span> <span class="fu">mean</span>(population) <span class="co"># The difference between the sample mean and the population mean</span></span>
<span id="cb492-2"><a href="inferential-statistics-the-central-limit-theorem.html#cb492-2" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] -4.338842</span></span></code></pre></div>
<p>This required a few R functions you may not have seen before. <code>runif()</code> randomly generates uniform distributions (all values have equal probability) of any specified size and between any specified values. <code>sample()</code> takes a random sample from a specified distribution.</p>
<p>What was the difference between the sample mean and the population mean? If random sampling was perfect, it would be zero. Although these values were randomly generated, I am fairly confident the difference was not zero. Next, how do the <em>shapes</em> of the two distributions compare?</p>
<p>Okay, the sample is not perfect. But sampling error is not dichotomous. Sometimes we can observe larger or smaller sampling error. What causes sampling error to be larger or smaller? First, we’ll change <em>sample size</em>:</p>
<div class="sourceCode" id="cb493"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb493-1"><a href="inferential-statistics-the-central-limit-theorem.html#cb493-1" aria-hidden="true" tabindex="-1"></a>population <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">1000</span>, <span class="dv">1</span>, <span class="dv">100</span>) <span class="co"># Generate a variable called &#39;population&#39; with 1000 values chosen at random between 1 and 100</span></span>
<span id="cb493-2"><a href="inferential-statistics-the-central-limit-theorem.html#cb493-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(population) <span class="co"># Display the mean of the population distribution</span></span>
<span id="cb493-3"><a href="inferential-statistics-the-central-limit-theorem.html#cb493-3" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 51.40494</span></span>
<span id="cb493-4"><a href="inferential-statistics-the-central-limit-theorem.html#cb493-4" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(population) <span class="co"># Display a histogram of the population distribution</span></span></code></pre></div>
<p><img src="schuster-statistics-remix_files/figure-html/unnamed-chunk-241-1.png" width="672" /></p>
<div class="sourceCode" id="cb494"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb494-1"><a href="inferential-statistics-the-central-limit-theorem.html#cb494-1" aria-hidden="true" tabindex="-1"></a>onesample <span class="ot">&lt;-</span> <span class="fu">sample</span>(population, <span class="at">size =</span> <span class="dv">900</span>, <span class="at">replace =</span> <span class="cn">TRUE</span>) <span class="co"># Take a random sample of size 900 from the population distribution with replacement</span></span>
<span id="cb494-2"><a href="inferential-statistics-the-central-limit-theorem.html#cb494-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(onesample) <span class="co"># Display the mean of the sample distribution</span></span>
<span id="cb494-3"><a href="inferential-statistics-the-central-limit-theorem.html#cb494-3" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 50.03159</span></span>
<span id="cb494-4"><a href="inferential-statistics-the-central-limit-theorem.html#cb494-4" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(onesample) <span class="co"># Display a histogram of the sample distribution</span></span></code></pre></div>
<p><img src="schuster-statistics-remix_files/figure-html/unnamed-chunk-241-2.png" width="672" /></p>
<div class="sourceCode" id="cb495"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb495-1"><a href="inferential-statistics-the-central-limit-theorem.html#cb495-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(onesample) <span class="sc">-</span> <span class="fu">mean</span>(population) <span class="co"># The difference between the sample mean and the population mean</span></span>
<span id="cb495-2"><a href="inferential-statistics-the-central-limit-theorem.html#cb495-2" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] -1.37335</span></span></code></pre></div>
<p>Did increasing the sample size to 900 increase or decrease sampling error? It decreased it. That hints that using a larger sample size can give us less sampling error. What do you think would happen if we put sample size back to 3 and, instead, changed the population so that all values were between 1 and 2?</p>
<p>Samples have variability because populations have variability; the sample mean ultimately depends on who gets selected to be in the sample. The larger the sample, the smaller the sampling error. The greater the variability in the population, the larger the sampling error.</p>
<p>Next, we will expand this discussion to consider what would happen if we construct a sampling distribution. We will take one sample, then take another and another. Our units will be sample means instead of scores.</p>
</div>
<div id="another-sampling-distribution" class="section level3" number="4.7.4">
<h3><span class="header-section-number">4.7.4</span> Another sampling distribution</h3>
<p>Text by David Schuster</p>
<p>A <strong>sampling distribution</strong> is a distribution of sample means.</p>
<p>If you take repeated samples, you can plot the mean of each sample. A collection of sample means forms a sampling distribution of the mean. Sampling distributions are made of many samples.</p>
<p>We will modify our prior example slightly. This time, we will create a loop (technically, a for…next loop where we tell R how many times we want a command repeated) to repeat taking the sample mean. Loops are common in computer programming languages, but we don’t use them very often when we do statistics in R. All you really need to know is that the loop causes the computer to repeat the commands inside the brackets.</p>
<div class="sourceCode" id="cb496"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb496-1"><a href="inferential-statistics-the-central-limit-theorem.html#cb496-1" aria-hidden="true" tabindex="-1"></a>population <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">1000</span>, <span class="dv">1</span>, <span class="dv">100</span>) <span class="co"># Generate a variable called &#39;population&#39; with 1000 values chosen at random between 1 and 100</span></span>
<span id="cb496-2"><a href="inferential-statistics-the-central-limit-theorem.html#cb496-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(population) <span class="co"># Display the mean of the population distribution</span></span>
<span id="cb496-3"><a href="inferential-statistics-the-central-limit-theorem.html#cb496-3" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 51.06564</span></span>
<span id="cb496-4"><a href="inferential-statistics-the-central-limit-theorem.html#cb496-4" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(population) <span class="co"># Display a histogram of the population distribution</span></span></code></pre></div>
<p><img src="schuster-statistics-remix_files/figure-html/unnamed-chunk-242-1.png" width="672" /></p>
<div class="sourceCode" id="cb497"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb497-1"><a href="inferential-statistics-the-central-limit-theorem.html#cb497-1" aria-hidden="true" tabindex="-1"></a>sampling <span class="ot">=</span> <span class="fu">rep</span>(<span class="cn">NA</span>, <span class="dv">2000</span>) <span class="co"># create a variable with 2000 values of NA</span></span>
<span id="cb497-2"><a href="inferential-statistics-the-central-limit-theorem.html#cb497-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">2000</span>){</span>
<span id="cb497-3"><a href="inferential-statistics-the-central-limit-theorem.html#cb497-3" aria-hidden="true" tabindex="-1"></a>  onesample <span class="ot">&lt;-</span> <span class="fu">sample</span>(population, <span class="at">size =</span> <span class="dv">5</span>, <span class="at">replace =</span> <span class="cn">TRUE</span>) <span class="co"># take a random sample of size 5 from the population distribution with replacement  </span></span>
<span id="cb497-4"><a href="inferential-statistics-the-central-limit-theorem.html#cb497-4" aria-hidden="true" tabindex="-1"></a>  sampling[i] <span class="ot">=</span> <span class="fu">mean</span>(onesample) <span class="co"># add the sample&#39;s mean to the sampling distribution</span></span>
<span id="cb497-5"><a href="inferential-statistics-the-central-limit-theorem.html#cb497-5" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb497-6"><a href="inferential-statistics-the-central-limit-theorem.html#cb497-6" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(sampling) <span class="co"># Display the mean of the SAMPLING distribution</span></span>
<span id="cb497-7"><a href="inferential-statistics-the-central-limit-theorem.html#cb497-7" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 50.86804</span></span>
<span id="cb497-8"><a href="inferential-statistics-the-central-limit-theorem.html#cb497-8" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(sampling) <span class="co"># Display a histogram of the SAMPLING distribution</span></span></code></pre></div>
<p><img src="schuster-statistics-remix_files/figure-html/unnamed-chunk-242-2.png" width="672" /></p>
<div class="sourceCode" id="cb498"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb498-1"><a href="inferential-statistics-the-central-limit-theorem.html#cb498-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(sampling) <span class="sc">-</span> <span class="fu">mean</span>(population) <span class="co"># The difference between the SAMPLING distribution mean and the population mean</span></span>
<span id="cb498-2"><a href="inferential-statistics-the-central-limit-theorem.html#cb498-2" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] -0.1976031</span></span></code></pre></div>
<p>Okay, some <em>really</em> interesting things happened in this last example. Before we continue, make sure you understand the steps what we’ve taken to create a sampling distribution:</p>
<ol style="list-style-type: decimal">
<li>We started with a population distribution. The shape of the population distribution is not important (did you notice that all the populations were close to a uniform distribution? None of the populations were normally distributed). While we’re at it, the population <em>size</em> is also not important. This example would have worked with a population of 50 or a population of 300,000.</li>
<li>We took a random sample from the population, with replacement. We found the mean of our random sample. We stored the mean in a variable called sampling.</li>
<li>We repeated Step 2 many times. Following this, we had a list of 2000 sample means stored in a variable called sampling. This is our sampling distribution. <strong>Sampling distributions are made of sample means</strong>. Put another way, the units of a sampling distribution are sample means.</li>
</ol>
<p>What did you you notice when you look at the histogram of the sampling distribution? It’s normally distributed! We started with a non-normal population and ended up with a normally distributed sampling distribution. This is one of the outcomes specified by the central limit theorem.</p>
<p>What did you notice about the difference between the mean of the sampling distribution and the mean of the population? It’s small. It is probably the smallest value of all the examples in this section. This is another outcome specified by the central limit theorem. As we collect more and more sample means, the mean of the sampling distribution will approach the mean of the population.</p>
<p>It has taken us a lot of steps and several examples to get here.</p>
</div>
<div id="defining-the-central-limit-theorem" class="section level3" number="4.7.5">
<h3><span class="header-section-number">4.7.5</span> Defining the central limit theorem</h3>
<p>Text by David Schuster</p>
<p>The central limit theorem (CLT) says that sampling distributions have special properties.</p>
<p>The CLT says that: (1) assuming two things, (2) if you do a series of steps, then (3) you will obtain an outcome. The outcome has implications for us.</p>
<ul>
<li>The two <strong>assumptions</strong> are a random sample and a variable that is continuous.</li>
<li>The <strong>steps</strong> are to take repeated random samples of the population and calculate the mean of each of those samples. Construct a sampling distribution from the sample means.</li>
<li>The <strong>outcome</strong> is that the histogram of the sample means is normally distributed. We call this the sampling distribution of the mean. It will always be normally distributed under the CLT, as long as we have a sufficiently large sample size.</li>
<li>This frequency distribution, like all frequency distributions, has a standard deviation called the standard error of the mean.</li>
</ul>
</div>
<div id="more-on-standard-error" class="section level3" number="4.7.6">
<h3><span class="header-section-number">4.7.6</span> More on standard error</h3>
<p>Text by David Schuster</p>
<p>Sampling distributions have a mean and standard deviation, just like any other distribution we have seen. However, the standard deviation of a sampling distribution has a special name: the standard error.</p>
<p>Standard error is calculated using this formula: <span class="math inline">\(\sigma_{\bar{X}}=\frac{\sigma}{\sqrt{n}}\)</span></p>
<p>In words: divide the standard deviation of the population by the square root of the sample size. Let’s run the example one more time, this time calculating the standard error two ways:</p>
<div class="sourceCode" id="cb499"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb499-1"><a href="inferential-statistics-the-central-limit-theorem.html#cb499-1" aria-hidden="true" tabindex="-1"></a>population <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">1000</span>, <span class="dv">1</span>, <span class="dv">100</span>) <span class="co"># Generate a variable called &#39;population&#39; with 1000 values chosen at random between 1 and 100</span></span>
<span id="cb499-2"><a href="inferential-statistics-the-central-limit-theorem.html#cb499-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb499-3"><a href="inferential-statistics-the-central-limit-theorem.html#cb499-3" aria-hidden="true" tabindex="-1"></a>sampling <span class="ot">=</span> <span class="fu">rep</span>(<span class="cn">NA</span>, <span class="dv">2000</span>) <span class="co"># create a variable with 2000 values of NA to store our sampling distribution</span></span>
<span id="cb499-4"><a href="inferential-statistics-the-central-limit-theorem.html#cb499-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">2000</span>){</span>
<span id="cb499-5"><a href="inferential-statistics-the-central-limit-theorem.html#cb499-5" aria-hidden="true" tabindex="-1"></a>  onesample <span class="ot">&lt;-</span> <span class="fu">sample</span>(population, <span class="at">size =</span> <span class="dv">5</span>, <span class="at">replace =</span> <span class="cn">TRUE</span>) <span class="co"># take a random sample of size 5 from the population distribution with replacement  </span></span>
<span id="cb499-6"><a href="inferential-statistics-the-central-limit-theorem.html#cb499-6" aria-hidden="true" tabindex="-1"></a>  sampling[i] <span class="ot">=</span> <span class="fu">mean</span>(onesample) <span class="co"># add the sample&#39;s mean to the sampling distribution</span></span>
<span id="cb499-7"><a href="inferential-statistics-the-central-limit-theorem.html#cb499-7" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb499-8"><a href="inferential-statistics-the-central-limit-theorem.html#cb499-8" aria-hidden="true" tabindex="-1"></a>sampling_sd <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">var</span>(sampling) <span class="sc">*</span> (<span class="fu">length</span>(sampling)<span class="sc">-</span><span class="dv">1</span>)<span class="sc">/</span><span class="fu">length</span>(sampling)) <span class="co"># Calculate the standard deviation of our observed (generated) sampling distribution</span></span>
<span id="cb499-9"><a href="inferential-statistics-the-central-limit-theorem.html#cb499-9" aria-hidden="true" tabindex="-1"></a>sampling_sd <span class="co"># Display the standard deviation of our observed (generated) sampling distribution</span></span></code></pre></div>
<pre><code>## [1] 12.51985</code></pre>
<div class="sourceCode" id="cb501"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb501-1"><a href="inferential-statistics-the-central-limit-theorem.html#cb501-1" aria-hidden="true" tabindex="-1"></a>pop_sd <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">var</span>(population) <span class="sc">*</span> (<span class="fu">length</span>(population)<span class="sc">-</span><span class="dv">1</span>)<span class="sc">/</span><span class="fu">length</span>(population)) <span class="co"># find population standard deviation (the var() function gives sample variance and this converts it into population standard deviation)</span></span>
<span id="cb501-2"><a href="inferential-statistics-the-central-limit-theorem.html#cb501-2" aria-hidden="true" tabindex="-1"></a>standard_error <span class="ot">&lt;-</span> pop_sd <span class="sc">/</span> <span class="fu">sqrt</span>(<span class="dv">5</span>) <span class="co"># Use the central limit theorem to calculate standard error using the formula</span></span>
<span id="cb501-3"><a href="inferential-statistics-the-central-limit-theorem.html#cb501-3" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(standard_error) <span class="co"># Display the standard error</span></span></code></pre></div>
<pre><code>## [1] 12.50781</code></pre>
<div class="sourceCode" id="cb503"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb503-1"><a href="inferential-statistics-the-central-limit-theorem.html#cb503-1" aria-hidden="true" tabindex="-1"></a>standard_error <span class="sc">-</span> sampling_sd <span class="co"># Display the difference between the calculated standard error and our observed standard deviation</span></span></code></pre></div>
<pre><code>## [1] -0.01203723</code></pre>
<p>They are pretty close. Why do we need the standard error formula if we could just find the standard deviation of the sampling distribution? Well, we typically do not work with the sampling distribution directly. We simply understand that it exists. Creating a sampling distribution requires the population distribution to be available to us, and this isn’t usually the case when we are doing research. Further, the central limit theorem specifies taking an <em>unlimited</em> number of samples in order for the sampling distribution mean to equal the population mean. It also requires a sample size of <span class="math inline">\(N\ge30\)</span> when the population is not normally distributed. We have violated that rule (we used a uniform population distribution and we set our sample size as low as 5), but the numbers still came out pretty close.</p>
<p>Notice that if we assume that the central limit theorem applies, we already know the shape, mean, and standard deviation of a sampling distribution without having to construct it. This is one key to inferential statistics, and, specifically, <strong>parametric statistics</strong>. Parametric statistics that are methods that are based on known (or assumed) probability distributions. The sampling distribution of the mean is one such example.</p>
</div>
<div id="the-sampling-distribution-tells-us-about-the-probability-of-sample-means" class="section level3" number="4.7.7">
<h3><span class="header-section-number">4.7.7</span> The sampling distribution tells us about the probability of sample means</h3>
<p>Where this gets useful is using the sampling distribution to make statements about the probability of obtaining a single sample mean. In many research contexts, we work with a single sample distribution. We do not have access to the population distribution nor the sampling distribution. But, we can use the central limit theorem to imagine what the sampling distribution looks like (it’s normal with it’s mean equal to the population mean and a standard error based on population standard deviation and sample size). Because the sampling distribution is made of sample means, it tells us about what we can expect if we take one single random sample from a population.</p>
<p>To summarize, the central limit theorem allows us to say useful things for research:</p>
<ul>
<li><p>A single random sample will have a mean that approximates the population mean. We can use samples in place of having to measure every member of the population.</p></li>
<li><p>Each time we take a random sample and calculate the mean, we are most likely to get the population mean.</p></li>
<li><p>Our sample means will vary. We can predict how much they vary by calculating the standard error.</p></li>
<li><p>It is possible to take a random sample and calculate the mean only to get a sample mean that is far away from the population mean, but this is unlikely to happen.</p></li>
<li><p>A larger sample size reduces the standard error of the mean. Larger sample sizes give us better estimates of the mean.</p></li>
</ul>
</div>
<div id="sampling-distributions-exist-for-any-sample-statistic" class="section level3" number="4.7.8">
<h3><span class="header-section-number">4.7.8</span> Sampling distributions exist for any sample statistic!</h3>
<p>Text by <span class="citation"><a href="#ref-Navarro2018" role="doc-biblioref">Navarro</a> (<a href="#ref-Navarro2018" role="doc-biblioref">2018</a>)</span></p>
<p>One thing to keep in mind when thinking about sampling distributions is that <em>any</em> sample statistic you might care to calculate has a sampling distribution. For example, suppose that each time I replicated the “five IQ scores” experiment I wrote down the largest IQ score in the experiment. This would give me a data set that started out like this:</p>
<pre><code>                      110 117 122 119 113 ... </code></pre>
<p>Doing this over and over again would give me a very different sampling distribution, namely the <em>sampling distribution of the maximum</em>. The sampling distribution of the maximum of 5 IQ scores is shown in Figure <a href="inferential-statistics-the-central-limit-theorem.html#fig:sampdistmax">4.9</a>. Not surprisingly, if you pick 5 people at random and then find the person with the highest IQ score, they’re going to have an above average IQ. Most of the time you’ll end up with someone whose IQ is measured in the 100 to 140 range.</p>
</div>
<div id="clt" class="section level3" number="4.7.9">
<h3><span class="header-section-number">4.7.9</span> The central limit theorem</h3>
<p>An illustration of the how sampling distribution of the mean depends on sample size. In each panel, I generated 10,000 samples of IQ data, and calculated the mean IQ observed within each of these data sets. The histograms in these plots show the distribution of these means (i.e., the sampling distribution of the mean). Each individual IQ score was drawn from a normal distribution with mean 100 and standard deviation 15, which is shown as the solid black line).</p>
<div class="figure"><span style="display:block;" id="fig:IQsampa"></span>
<img src="schuster-statistics-remix_files/figure-html/IQsampa-1.png" alt="Each data set contained only a single observation, so the mean of each sample is just one person's IQ score. As a consequence, the sampling distribution of the mean is of course identical to the population distribution of IQ scores." width="672" />
<p class="caption">
Figure 4.10: Each data set contained only a single observation, so the mean of each sample is just one person’s IQ score. As a consequence, the sampling distribution of the mean is of course identical to the population distribution of IQ scores.
</p>
</div>
<div class="figure"><span style="display:block;" id="fig:IQsampb"></span>
<img src="schuster-statistics-remix_files/figure-html/IQsampb-1.png" alt="When we raise the sample size to 2, the mean of any one sample tends to be closer to the population mean than a one person's IQ score, and so the histogram (i.e., the sampling distribution) is a bit narrower than the population distribution." width="672" />
<p class="caption">
Figure 4.11: When we raise the sample size to 2, the mean of any one sample tends to be closer to the population mean than a one person’s IQ score, and so the histogram (i.e., the sampling distribution) is a bit narrower than the population distribution.
</p>
</div>
<div class="figure"><span style="display:block;" id="fig:IQsampc"></span>
<img src="schuster-statistics-remix_files/figure-html/IQsampc-1.png" alt="By the time we raise the sample size to 10, we can see that the distribution of sample means tend to be fairly tightly clustered around the true population mean." width="672" />
<p class="caption">
Figure 4.12: By the time we raise the sample size to 10, we can see that the distribution of sample means tend to be fairly tightly clustered around the true population mean.
</p>
</div>
<p>At this point I hope you have a pretty good sense of what sampling distributions are, and in particular what the sampling distribution of the mean is. In this section I want to talk about how the sampling distribution of the mean changes as a function of sample size. Intuitively, you already know part of the answer: if you only have a few observations, the sample mean is likely to be quite inaccurate: if you replicate a small experiment and recalculate the mean you’ll get a very different answer. In other words, the sampling distribution is quite wide. If you replicate a large experiment and recalculate the sample mean you’ll probably get the same answer you got last time, so the sampling distribution will be very narrow. You can see this visually in Figures <a href="inferential-statistics-the-central-limit-theorem.html#fig:IQsampa">4.10</a>, <a href="inferential-statistics-the-central-limit-theorem.html#fig:IQsampb">4.11</a> and <a href="inferential-statistics-the-central-limit-theorem.html#fig:IQsampc">4.12</a>: the bigger the sample size, the narrower the sampling distribution gets. We can quantify this effect by calculating the standard deviation of the sampling distribution, which is referred to as the <strong><em>standard error</em></strong>. The standard error of a statistic is often denoted SE, and since we’re usually interested in the standard error of the sample <em>mean</em>, we often use the acronym SEM. As you can see just by looking at the picture, as the sample size <span class="math inline">\(N\)</span> increases, the SEM decreases.</p>
<p>Okay, so that’s one part of the story. However, there’s something I’ve been glossing over so far. All my examples up to this point have been based on the “IQ scores” experiments, and because IQ scores are roughly normally distributed, I’ve assumed that the population distribution is normal. What if it isn’t normal? What happens to the sampling distribution of the mean? The remarkable thing is this: no matter what shape your population distribution is, as <span class="math inline">\(N\)</span> increases the sampling distribution of the mean starts to look more like a normal distribution. To give you a sense of this, I ran some simulations using R. To do this, I started with the “ramped” distribution shown in the histogram in Figure <a href="#fig:cltdemo"><strong>??</strong></a>. As you can see by comparing the triangular shaped histogram to the bell curve plotted by the black line, the population distribution doesn’t look very much like a normal distribution at all. Next, I used R to simulate the results of a large number of experiments. In each experiment I took <span class="math inline">\(N=2\)</span> samples from this distribution, and then calculated the sample mean. Figure <a href="#fig:cltdemo"><strong>??</strong></a> plots the histogram of these sample means (i.e., the sampling distribution of the mean for <span class="math inline">\(N=2\)</span>). This time, the histogram produces a <span class="math inline">\(\cap\)</span>-shaped distribution: it’s still not normal, but it’s a lot closer to the black line than the population distribution in Figure <a href="#fig:cltdemo"><strong>??</strong></a>. When I increase the sample size to <span class="math inline">\(N=4\)</span>, the sampling distribution of the mean is very close to normal (Figure <a href="#fig:cltdemo"><strong>??</strong></a>, and by the time we reach a sample size of <span class="math inline">\(N=8\)</span> it’s almost perfectly normal. In other words, as long as your sample size isn’t tiny, the sampling distribution of the mean will be approximately normal no matter what your population distribution looks like!</p>
<div class="sourceCode" id="cb506"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb506-1"><a href="inferential-statistics-the-central-limit-theorem.html#cb506-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># needed for printing</span></span>
<span id="cb506-2"><a href="inferential-statistics-the-central-limit-theorem.html#cb506-2" aria-hidden="true" tabindex="-1"></a>    width <span class="ot">&lt;-</span> <span class="dv">6</span></span>
<span id="cb506-3"><a href="inferential-statistics-the-central-limit-theorem.html#cb506-3" aria-hidden="true" tabindex="-1"></a>    height <span class="ot">&lt;-</span> <span class="dv">6</span> </span>
<span id="cb506-4"><a href="inferential-statistics-the-central-limit-theorem.html#cb506-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb506-5"><a href="inferential-statistics-the-central-limit-theorem.html#cb506-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># parameters of the beta</span></span>
<span id="cb506-6"><a href="inferential-statistics-the-central-limit-theorem.html#cb506-6" aria-hidden="true" tabindex="-1"></a>    a <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb506-7"><a href="inferential-statistics-the-central-limit-theorem.html#cb506-7" aria-hidden="true" tabindex="-1"></a>    b <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb506-8"><a href="inferential-statistics-the-central-limit-theorem.html#cb506-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb506-9"><a href="inferential-statistics-the-central-limit-theorem.html#cb506-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># mean and standard deviation of the beta</span></span>
<span id="cb506-10"><a href="inferential-statistics-the-central-limit-theorem.html#cb506-10" aria-hidden="true" tabindex="-1"></a>    s <span class="ot">&lt;-</span> <span class="fu">sqrt</span>( a<span class="sc">*</span>b <span class="sc">/</span> (a<span class="sc">+</span>b)<span class="sc">^</span><span class="dv">2</span> <span class="sc">/</span> (a<span class="sc">+</span>b<span class="sc">+</span><span class="dv">1</span>) )</span>
<span id="cb506-11"><a href="inferential-statistics-the-central-limit-theorem.html#cb506-11" aria-hidden="true" tabindex="-1"></a>    m <span class="ot">&lt;-</span> a <span class="sc">/</span> (a<span class="sc">+</span>b)</span>
<span id="cb506-12"><a href="inferential-statistics-the-central-limit-theorem.html#cb506-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb506-13"><a href="inferential-statistics-the-central-limit-theorem.html#cb506-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># define function to draw a plot</span></span>
<span id="cb506-14"><a href="inferential-statistics-the-central-limit-theorem.html#cb506-14" aria-hidden="true" tabindex="-1"></a>    plotOne <span class="ot">&lt;-</span> <span class="cf">function</span>(n,<span class="at">N=</span><span class="dv">50000</span>) {</span>
<span id="cb506-15"><a href="inferential-statistics-the-central-limit-theorem.html#cb506-15" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb506-16"><a href="inferential-statistics-the-central-limit-theorem.html#cb506-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># generate N random sample means of size n</span></span>
<span id="cb506-17"><a href="inferential-statistics-the-central-limit-theorem.html#cb506-17" aria-hidden="true" tabindex="-1"></a>        X <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rbeta</span>(n<span class="sc">*</span>N,a,b),n,N)</span>
<span id="cb506-18"><a href="inferential-statistics-the-central-limit-theorem.html#cb506-18" aria-hidden="true" tabindex="-1"></a>        X <span class="ot">&lt;-</span> <span class="fu">colMeans</span>(X)</span>
<span id="cb506-19"><a href="inferential-statistics-the-central-limit-theorem.html#cb506-19" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb506-20"><a href="inferential-statistics-the-central-limit-theorem.html#cb506-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># plot the data</span></span>
<span id="cb506-21"><a href="inferential-statistics-the-central-limit-theorem.html#cb506-21" aria-hidden="true" tabindex="-1"></a>        <span class="fu">hist</span>( X, <span class="at">breaks=</span><span class="fu">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>,.<span class="dv">025</span>), <span class="at">border=</span><span class="st">&quot;white&quot;</span>, <span class="at">freq=</span><span class="cn">FALSE</span>,</span>
<span id="cb506-22"><a href="inferential-statistics-the-central-limit-theorem.html#cb506-22" aria-hidden="true" tabindex="-1"></a>            <span class="at">col=</span><span class="fu">ifelse</span>(colour,emphColLight,emphGrey),</span>
<span id="cb506-23"><a href="inferential-statistics-the-central-limit-theorem.html#cb506-23" aria-hidden="true" tabindex="-1"></a>            <span class="at">xlab=</span><span class="st">&quot;Sample Mean&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;&quot;</span>, <span class="at">xlim=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="fl">1.2</span>),</span>
<span id="cb506-24"><a href="inferential-statistics-the-central-limit-theorem.html#cb506-24" aria-hidden="true" tabindex="-1"></a>            <span class="at">main=</span><span class="fu">paste</span>(<span class="st">&quot;Sample Size =&quot;</span>,n), <span class="at">axes=</span><span class="cn">FALSE</span>,</span>
<span id="cb506-25"><a href="inferential-statistics-the-central-limit-theorem.html#cb506-25" aria-hidden="true" tabindex="-1"></a>            <span class="at">font.main=</span><span class="dv">1</span>, <span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">5</span>)</span>
<span id="cb506-26"><a href="inferential-statistics-the-central-limit-theorem.html#cb506-26" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb506-27"><a href="inferential-statistics-the-central-limit-theorem.html#cb506-27" aria-hidden="true" tabindex="-1"></a>        <span class="fu">box</span>()</span>
<span id="cb506-28"><a href="inferential-statistics-the-central-limit-theorem.html#cb506-28" aria-hidden="true" tabindex="-1"></a>        <span class="fu">axis</span>(<span class="dv">1</span>)</span>
<span id="cb506-29"><a href="inferential-statistics-the-central-limit-theorem.html#cb506-29" aria-hidden="true" tabindex="-1"></a>        <span class="co">#axis(2)</span></span>
<span id="cb506-30"><a href="inferential-statistics-the-central-limit-theorem.html#cb506-30" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb506-31"><a href="inferential-statistics-the-central-limit-theorem.html#cb506-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># plot the theoretical distribution</span></span>
<span id="cb506-32"><a href="inferential-statistics-the-central-limit-theorem.html#cb506-32" aria-hidden="true" tabindex="-1"></a>        <span class="fu">lines</span>( x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>,<span class="fl">1.2</span>,.<span class="dv">01</span>), <span class="fu">dnorm</span>(x,m,s<span class="sc">/</span><span class="fu">sqrt</span>(n)), </span>
<span id="cb506-33"><a href="inferential-statistics-the-central-limit-theorem.html#cb506-33" aria-hidden="true" tabindex="-1"></a>            <span class="at">lwd=</span><span class="dv">2</span>, <span class="at">col=</span><span class="st">&quot;black&quot;</span>, <span class="at">type=</span><span class="st">&quot;l&quot;</span></span>
<span id="cb506-34"><a href="inferential-statistics-the-central-limit-theorem.html#cb506-34" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb506-35"><a href="inferential-statistics-the-central-limit-theorem.html#cb506-35" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb506-36"><a href="inferential-statistics-the-central-limit-theorem.html#cb506-36" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb506-37"><a href="inferential-statistics-the-central-limit-theorem.html#cb506-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span>( i <span class="cf">in</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">8</span>)) {</span>
<span id="cb506-38"><a href="inferential-statistics-the-central-limit-theorem.html#cb506-38" aria-hidden="true" tabindex="-1"></a>        <span class="fu">plotOne</span>(i)}</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:cltdemo-1"></span>
<img src="schuster-statistics-remix_files/figure-html/cltdemo-1.png" alt="A demonstration of the central limit theorem. In panel a, we have a non-normal population distribution; and panels b-d show the sampling distribution of the mean for samples of size 2,4 and 8, for data drawn from the distribution in panel a. As you can see, even though the original population distribution is non-normal, the sampling distribution of the mean becomes pretty close to normal by the time you have a sample of even 4 observations. " width="672" />
<p class="caption">
Figure 4.13: A demonstration of the central limit theorem. In panel a, we have a non-normal population distribution; and panels b-d show the sampling distribution of the mean for samples of size 2,4 and 8, for data drawn from the distribution in panel a. As you can see, even though the original population distribution is non-normal, the sampling distribution of the mean becomes pretty close to normal by the time you have a sample of even 4 observations.
</p>
</div>
<div class="figure"><span style="display:block;" id="fig:cltdemo-2"></span>
<img src="schuster-statistics-remix_files/figure-html/cltdemo-2.png" alt="A demonstration of the central limit theorem. In panel a, we have a non-normal population distribution; and panels b-d show the sampling distribution of the mean for samples of size 2,4 and 8, for data drawn from the distribution in panel a. As you can see, even though the original population distribution is non-normal, the sampling distribution of the mean becomes pretty close to normal by the time you have a sample of even 4 observations. " width="672" />
<p class="caption">
Figure 4.14: A demonstration of the central limit theorem. In panel a, we have a non-normal population distribution; and panels b-d show the sampling distribution of the mean for samples of size 2,4 and 8, for data drawn from the distribution in panel a. As you can see, even though the original population distribution is non-normal, the sampling distribution of the mean becomes pretty close to normal by the time you have a sample of even 4 observations.
</p>
</div>
<div class="figure"><span style="display:block;" id="fig:cltdemo-3"></span>
<img src="schuster-statistics-remix_files/figure-html/cltdemo-3.png" alt="A demonstration of the central limit theorem. In panel a, we have a non-normal population distribution; and panels b-d show the sampling distribution of the mean for samples of size 2,4 and 8, for data drawn from the distribution in panel a. As you can see, even though the original population distribution is non-normal, the sampling distribution of the mean becomes pretty close to normal by the time you have a sample of even 4 observations. " width="672" />
<p class="caption">
Figure 4.15: A demonstration of the central limit theorem. In panel a, we have a non-normal population distribution; and panels b-d show the sampling distribution of the mean for samples of size 2,4 and 8, for data drawn from the distribution in panel a. As you can see, even though the original population distribution is non-normal, the sampling distribution of the mean becomes pretty close to normal by the time you have a sample of even 4 observations.
</p>
</div>
<div class="figure"><span style="display:block;" id="fig:cltdemo-4"></span>
<img src="schuster-statistics-remix_files/figure-html/cltdemo-4.png" alt="A demonstration of the central limit theorem. In panel a, we have a non-normal population distribution; and panels b-d show the sampling distribution of the mean for samples of size 2,4 and 8, for data drawn from the distribution in panel a. As you can see, even though the original population distribution is non-normal, the sampling distribution of the mean becomes pretty close to normal by the time you have a sample of even 4 observations. " width="672" />
<p class="caption">
Figure 4.16: A demonstration of the central limit theorem. In panel a, we have a non-normal population distribution; and panels b-d show the sampling distribution of the mean for samples of size 2,4 and 8, for data drawn from the distribution in panel a. As you can see, even though the original population distribution is non-normal, the sampling distribution of the mean becomes pretty close to normal by the time you have a sample of even 4 observations.
</p>
</div>
<p>On the basis of these figures, it seems like we have evidence for all of the following claims about the sampling distribution of the mean:</p>
<ul>
<li>The mean of the sampling distribution is the same as the mean of the population</li>
<li>The standard deviation of the sampling distribution (i.e., the standard error) gets smaller as the sample size increases</li>
<li>The shape of the sampling distribution becomes normal as the sample size increases</li>
</ul>
<p>As it happens, not only are all of these statements true, there is a very famous theorem in statistics that proves all three of them, known as the <strong><em>central limit theorem</em></strong>. Among other things, the central limit theorem tells us that if the population distribution has mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>, then the sampling distribution of the mean also has mean <span class="math inline">\(\mu\)</span>, and the standard error of the mean is
<span class="math display">\[
\mbox{SEM} = \frac{\sigma}{ \sqrt{N} }
\]</span>
Because we divide the population standard devation <span class="math inline">\(\sigma\)</span> by the square root of the sample size <span class="math inline">\(N\)</span>, the SEM gets smaller as the sample size increases. It also tells us that the shape of the sampling distribution becomes normal.<a href="#fn84" class="footnote-ref" id="fnref84"><sup>84</sup></a></p>
<p>This result is useful for all sorts of things. It tells us why large experiments are more reliable than small ones, and because it gives us an explicit formula for the standard error it tells us <em>how much</em> more reliable a large experiment is. It tells us why the normal distribution is, well, <em>normal</em>. In real experiments, many of the things that we want to measure are actually averages of lots of different quantities (e.g., arguably, “general” intelligence as measured by IQ is an average of a large number of “specific” skills and abilities), and when that happens, the averaged quantity should follow a normal distribution. Because of this mathematical law, the normal distribution pops up over and over again in real data.</p>
</div>
</div>
<div id="pointestimates" class="section level2" number="4.8">
<h2><span class="header-section-number">4.8</span> Estimating population parameters</h2>
<p>Text by <span class="citation"><a href="#ref-Navarro2018" role="doc-biblioref">Navarro</a> (<a href="#ref-Navarro2018" role="doc-biblioref">2018</a>)</span></p>
<p>In all the IQ examples in the previous sections, we actually knew the population parameters ahead of time. As every undergraduate gets taught in their very first lecture on the measurement of intelligence, IQ scores are <em>defined</em> to have mean 100 and standard deviation 15. However, this is a bit of a lie. How do we know that IQ scores have a true population mean of 100? Well, we know this because the people who designed the tests have administered them to very large samples, and have then “rigged” the scoring rules so that their sample has mean 100. That’s not a bad thing of course: it’s an important part of designing a psychological measurement. However, it’s important to keep in mind that this theoretical mean of 100 only attaches to the population that the test designers used to design the tests. Good test designers will actually go to some lengths to provide “test norms” that can apply to lots of different populations (e.g., different age groups, nationalities etc).</p>
<p>This is very handy, but of course almost every research project of interest involves looking at a different population of people to those used in the test norms. For instance, suppose you wanted to measure the effect of low level lead poisoning on cognitive functioning in Port Pirie, a South Australian industrial town with a lead smelter. Perhaps you decide that you want to compare IQ scores among people in Port Pirie to a comparable sample in Whyalla, a South Australian industrial town with a steel refinery.<a href="#fn85" class="footnote-ref" id="fnref85"><sup>85</sup></a> Regardless of which town you’re thinking about, it doesn’t make a lot of sense simply to <em>assume</em> that the true population mean IQ is 100. No-one has, to my knowledge, produced sensible norming data that can automatically be applied to South Australian industrial towns. We’re going to have to <strong><em>estimate</em></strong> the population parameters from a sample of data. So how do we do this?</p>
<div id="estimating-the-population-mean" class="section level3" number="4.8.1">
<h3><span class="header-section-number">4.8.1</span> Estimating the population mean</h3>
<p>Suppose we go to Port Pirie and 100 of the locals are kind enough to sit through an IQ test. The average IQ score among these people turns out to be <span class="math inline">\(\bar{X}=98.5\)</span>. So what is the true mean IQ for the entire population of Port Pirie? Obviously, we don’t know the answer to that question. It could be <span class="math inline">\(97.2\)</span>, but if could also be <span class="math inline">\(103.5\)</span>. Our sampling isn’t exhaustive so we cannot give a definitive answer. Nevertheless if I was forced at gunpoint to give a “best guess” I’d have to say <span class="math inline">\(98.5\)</span>. That’s the essence of statistical estimation: giving a best guess.</p>
<p>In this example, estimating the unknown poulation parameter is straightforward. I calculate the sample mean, and I use that as my <strong><em>estimate of the population mean</em></strong>. It’s pretty simple, and in the next section I’ll explain the statistical justification for this intuitive answer. However, for the moment what I want to do is make sure you recognise that the sample statistic and the estimate of the population parameter are conceptually different things. A sample statistic is a description of your data, whereas the estimate is a guess about the population. With that in mind, statisticians often different notation to refer to them. For instance, if true population mean is denoted <span class="math inline">\(\mu\)</span>, then we would use <span class="math inline">\(\hat\mu\)</span> to refer to our estimate of the population mean. In contrast, the sample mean is denoted <span class="math inline">\(\bar{X}\)</span> or sometimes <span class="math inline">\(m\)</span>. However, in simple random samples, the estimate of the population mean is identical to the sample mean: if I observe a sample mean of <span class="math inline">\(\bar{X} = 98.5\)</span>, then my estimate of the population mean is also <span class="math inline">\(\hat\mu = 98.5\)</span>. To help keep the notation clear, here’s a handy table:</p>
<div class="sourceCode" id="cb507"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb507-1"><a href="inferential-statistics-the-central-limit-theorem.html#cb507-1" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(<span class="fu">data.frame</span>(<span class="at">stringsAsFactors=</span><span class="cn">FALSE</span>,</span>
<span id="cb507-2"><a href="inferential-statistics-the-central-limit-theorem.html#cb507-2" aria-hidden="true" tabindex="-1"></a>                   <span class="at">Symbol =</span> <span class="fu">c</span>(<span class="st">&quot;$</span><span class="sc">\\</span><span class="st">bar{X}$&quot;</span>, <span class="st">&quot;$</span><span class="sc">\\</span><span class="st">mu$&quot;</span>, <span class="st">&quot;$</span><span class="sc">\\</span><span class="st">hat{</span><span class="sc">\\</span><span class="st">mu}$&quot;</span>),</span>
<span id="cb507-3"><a href="inferential-statistics-the-central-limit-theorem.html#cb507-3" aria-hidden="true" tabindex="-1"></a>              <span class="at">What.is.it =</span> <span class="fu">c</span>(<span class="st">&quot;Sample mean&quot;</span>, <span class="st">&quot;True population mean&quot;</span>,</span>
<span id="cb507-4"><a href="inferential-statistics-the-central-limit-theorem.html#cb507-4" aria-hidden="true" tabindex="-1"></a>                              <span class="st">&quot;Estimate of the population mean&quot;</span>),</span>
<span id="cb507-5"><a href="inferential-statistics-the-central-limit-theorem.html#cb507-5" aria-hidden="true" tabindex="-1"></a>   <span class="at">Do.we.know.what.it.is =</span> <span class="fu">c</span>(<span class="st">&quot;Yes  calculated from the raw data&quot;</span>,</span>
<span id="cb507-6"><a href="inferential-statistics-the-central-limit-theorem.html#cb507-6" aria-hidden="true" tabindex="-1"></a>                              <span class="st">&quot;Almost never known for sure&quot;</span>,</span>
<span id="cb507-7"><a href="inferential-statistics-the-central-limit-theorem.html#cb507-7" aria-hidden="true" tabindex="-1"></a>                              <span class="st">&quot;Yes  identical to the sample mean&quot;</span>)))</span></code></pre></div>
<table>
<thead>
<tr class="header">
<th align="left">Symbol</th>
<th align="left">What.is.it</th>
<th align="left">Do.we.know.what.it.is</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(\bar{X}\)</span></td>
<td align="left">Sample mean</td>
<td align="left">Yes calculated from the raw data</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\mu\)</span></td>
<td align="left">True population mean</td>
<td align="left">Almost never known for sure</td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(\hat{\mu}\)</span></td>
<td align="left">Estimate of the population mean</td>
<td align="left">Yes identical to the sample mean</td>
</tr>
</tbody>
</table>
</div>
<div id="estimating-the-population-standard-deviation" class="section level3" number="4.8.2">
<h3><span class="header-section-number">4.8.2</span> Estimating the population standard deviation</h3>
<p>So far, estimation seems pretty simple, and you might be wondering why I forced you to read through all that stuff about sampling theory. In the case of the mean, our estimate of the population parameter (i.e. <span class="math inline">\(\hat\mu\)</span>) turned out to identical to the corresponding sample statistic (i.e. <span class="math inline">\(\bar{X}\)</span>). However, that’s not always true. To see this, let’s have a think about how to construct an <strong><em>estimate of the population standard deviation</em></strong>, which we’ll denote <span class="math inline">\(\hat\sigma\)</span>. What shall we use as our estimate in this case? Your first thought might be that we could do the same thing we did when estimating the mean, and just use the sample statistic as our estimate. That’s almost the right thing to do, but not quite.</p>
<p>Here’s why. Suppose I have a sample that contains a single observation. For this example, it helps to consider a sample where you have no intutions at all about what the true population values might be, so let’s use something completely fictitious. Suppose the observation in question measures the <em>cromulence</em> of my shoes. It turns out that my shoes have a cromulence of 20. So here’s my sample:</p>
<pre><code>20</code></pre>
<p>This is a perfectly legitimate sample, even if it does have a sample size of <span class="math inline">\(N=1\)</span>. It has a sample mean of 20, and because every observation in this sample is equal to the sample mean (obviously!) it has a sample standard deviation of 0. As a description of the <em>sample</em> this seems quite right: the sample contains a single observation and therefore there is no variation observed within the sample. A sample standard deviation of <span class="math inline">\(s = 0\)</span> is the right answer here. But as an estimate of the <em>population</em> standard deviation, it feels completely insane, right? Admittedly, you and I don’t know anything at all about what “cromulence” is, but we know something about data: the only reason that we don’t see any variability in the <em>sample</em> is that the sample is too small to display any variation! So, if you have a sample size of <span class="math inline">\(N=1\)</span>, it <em>feels</em> like the right answer is just to say “no idea at all.”</p>
<p>Notice that you <em>don’t</em> have the same intuition when it comes to the sample mean and the population mean. If forced to make a best guess about the population mean, it doesn’t feel completely insane to guess that the population mean is 20. Sure, you probably wouldn’t feel very confident in that guess, because you have only the one observation to work with, but it’s still the best guess you can make.</p>
<p>Let’s extend this example a little. Suppose I now make a second observation. My data set now has <span class="math inline">\(N=2\)</span> observations of the cromulence of shoes, and the complete sample now looks like this:</p>
<pre><code>20, 22</code></pre>
<p>This time around, our sample is <em>just</em> large enough for us to be able to observe some variability: two observations is the bare minimum number needed for any variability to be observed! For our new data set, the sample mean is <span class="math inline">\(\bar{X}=21\)</span>, and the sample standard deviation is <span class="math inline">\(s=1\)</span>. What intuitions do we have about the population? Again, as far as the population mean goes, the best guess we can possibly make is the sample mean: if forced to guess, we’d probably guess that the population mean cromulence is 21. What about the standard deviation? This is a little more complicated. The sample standard deviation is only based on two observations, and if you’re at all like me you probably have the intuition that, with only two observations, we haven’t given the population “enough of a chance” to reveal its true variability to us. It’s not just that we suspect that the estimate is <em>wrong</em>: after all, with only two observations we expect it to be wrong to some degree. The worry is that the error is <em>systematic</em>. Specifically, we suspect that the sample standard deviation is likely to be smaller than the population standard deviation.</p>
<div class="figure"><span style="display:block;" id="fig:sampdistsd"></span>
<img src="img/estimation/samplingDistSampleSD.png" alt="The sampling distribution of the sample standard deviation for a &quot;two IQ scores&quot; experiment. The true population standard deviation is 15 (dashed line), but as you can see from the histogram, the vast majority of experiments will produce a much smaller sample standard deviation than this. On average, this experiment would produce a sample standard deviation of only 8.5, well below the true value! In other words, the sample standard deviation is a *biased* estimate of the population standard deviation. " width="338" />
<p class="caption">
Figure 4.17: The sampling distribution of the sample standard deviation for a “two IQ scores” experiment. The true population standard deviation is 15 (dashed line), but as you can see from the histogram, the vast majority of experiments will produce a much smaller sample standard deviation than this. On average, this experiment would produce a sample standard deviation of only 8.5, well below the true value! In other words, the sample standard deviation is a <em>biased</em> estimate of the population standard deviation.
</p>
</div>
<p>This intuition feels right, but it would be nice to demonstrate this somehow. There are in fact mathematical proofs that confirm this intuition, but unless you have the right mathematical background they don’t help very much. Instead, what I’ll do is use R to simulate the results of some experiments. With that in mind, let’s return to our IQ studies. Suppose the true population mean IQ is 100 and the standard deviation is 15. I can use the <code>rnorm()</code> function to generate the the results of an experiment in which I measure <span class="math inline">\(N=2\)</span> IQ scores, and calculate the sample standard deviation. If I do this over and over again, and plot a histogram of these sample standard deviations, what I have is the <em>sampling distribution of the standard deviation</em>. I’ve plotted this distribution in Figure <a href="inferential-statistics-the-central-limit-theorem.html#fig:sampdistsd">4.17</a>. Even though the true population standard deviation is 15, the average of the <em>sample</em> standard deviations is only 8.5. Notice that this is a very different result to what we found in Figure <a href="inferential-statistics-the-central-limit-theorem.html#fig:IQsampb">4.11</a> when we plotted the sampling distribution of the mean. If you look at that sampling distribution, what you see is that the population mean is 100, and the average of the sample means is also 100.</p>
<p>Now let’s extend the simulation. Instead of restricting ourselves to the situation where we have a sample size of <span class="math inline">\(N=2\)</span>, let’s repeat the exercise for sample sizes from 1 to 10. If we plot the average sample mean and average sample standard deviation as a function of sample size, you get the results shown in Figure <a href="inferential-statistics-the-central-limit-theorem.html#fig:estimatorbias">4.18</a>. On the left hand side (panel a), I’ve plotted the average sample mean and on the right hand side (panel b), I’ve plotted the average standard deviation. The two plots are quite different: <em>on average</em>, the average sample mean is equal to the population mean. It is an <strong><em>unbiased estimator</em></strong>, which is essentially the reason why your best estimate for the population mean is the sample mean.<a href="#fn86" class="footnote-ref" id="fnref86"><sup>86</sup></a> The plot on the right is quite different: on average, the sample standard deviation <span class="math inline">\(s\)</span> is <em>smaller</em> than the population standard deviation <span class="math inline">\(\sigma\)</span>. It is a <strong><em>biased estimator</em></strong>. In other words, if we want to make a “best guess” <span class="math inline">\(\hat\sigma\)</span> about the value of the population standard deviation <span class="math inline">\(\sigma\)</span>, we should make sure our guess is a little bit larger than the sample standard deviation <span class="math inline">\(s\)</span>.</p>
<div class="figure"><span style="display:block;" id="fig:estimatorbias"></span>
<img src="img/estimation/biasMean.png" alt="An illustration of the fact that the sample mean is an unbiased estimator of the population mean (panel a), but the sample standard deviation is a biased estimator of the population standard deviation (panel b). To generate the figure, I generated 10,000 simulated data sets with 1 observation each, 10,000 more with 2 observations, and so on up to a sample size of 10. Each data set consisted of fake IQ data: that is, the data were normally distributed with a true population mean of 100 and standard deviation 15. *On average*, the sample means turn out to be 100, regardless of sample size (panel a). However, the sample standard deviations turn out to be systematically too small (panel b), especially for small sample sizes." width="300" />
<p class="caption">
Figure 4.18: An illustration of the fact that the sample mean is an unbiased estimator of the population mean (panel a), but the sample standard deviation is a biased estimator of the population standard deviation (panel b). To generate the figure, I generated 10,000 simulated data sets with 1 observation each, 10,000 more with 2 observations, and so on up to a sample size of 10. Each data set consisted of fake IQ data: that is, the data were normally distributed with a true population mean of 100 and standard deviation 15. <em>On average</em>, the sample means turn out to be 100, regardless of sample size (panel a). However, the sample standard deviations turn out to be systematically too small (panel b), especially for small sample sizes.
</p>
</div>
<p>The fix to this systematic bias turns out to be very simple. Here’s how it works. Before tackling the standard deviation, let’s look at the variance. If you recall from Section <a href="descriptives.html#var">3.4</a>, the sample variance is defined to be the average of the squared deviations from the sample mean. That is:
<span class="math display">\[
s^2 = \frac{1}{N} \sum_{i=1}^N (X_i - \bar{X})^2
\]</span>
The sample variance <span class="math inline">\(s^2\)</span> is a biased estimator of the population variance <span class="math inline">\(\sigma^2\)</span>. But as it turns out, we only need to make a tiny tweak to transform this into an unbiased estimator. All we have to do is divide by <span class="math inline">\(N-1\)</span> rather than by <span class="math inline">\(N\)</span>. If we do that, we obtain the following formula:
<span class="math display">\[
\hat\sigma^2 = \frac{1}{N-1} \sum_{i=1}^N (X_i - \bar{X})^2 
\]</span>
This is an unbiased estimator of the population variance <span class="math inline">\(\sigma\)</span>. Moreover, this finally answers the question we raised in Section <a href="descriptives.html#var">3.4</a>. Why did R give us slightly different answers when we used the <code>var()</code> function? Because the <code>var()</code> function calculates <span class="math inline">\(\hat\sigma^2\)</span> not <span class="math inline">\(s^2\)</span>, that’s why. A similar story applies for the standard deviation. If we divide by <span class="math inline">\(N-1\)</span> rather than <span class="math inline">\(N\)</span>, our estimate of the population standard deviation becomes:
<span class="math display">\[
\hat\sigma = \sqrt{\frac{1}{N-1} \sum_{i=1}^N (X_i - \bar{X})^2} 
\]</span>
and when we use R’s built in standard deviation function <code>sd()</code>, what it’s doing is calculating <span class="math inline">\(\hat\sigma\)</span>, not <span class="math inline">\(s\)</span>.<a href="#fn87" class="footnote-ref" id="fnref87"><sup>87</sup></a></p>
<p>One final point: in practice, a lot of people tend to refer to <span class="math inline">\(\hat{\sigma}\)</span> (i.e., the formula where we divide by <span class="math inline">\(N-1\)</span>) as the <em>sample</em> standard deviation. Technically, this is incorrect: the <em>sample</em> standard deviation should be equal to <span class="math inline">\(s\)</span> (i.e., the formula where we divide by <span class="math inline">\(N\)</span>). These aren’t the same thing, either conceptually or numerically. One is a property of the sample, the other is an estimated characteristic of the population. However, in almost every real life application, what we actually care about is the estimate of the population parameter, and so people always report <span class="math inline">\(\hat\sigma\)</span> rather than <span class="math inline">\(s\)</span>. This is the right number to report, of course, it’s that people tend to get a little bit imprecise about terminology when they write it up, because “sample standard deviation” is shorter than “estimated population standard deviation.” It’s no big deal, and in practice I do the same thing everyone else does. Nevertheless, I think it’s important to keep the two <em>concepts</em> separate: it’s never a good idea to confuse “known properties of your sample” with “guesses about the population from which it came.” The moment you start thinking that <span class="math inline">\(s\)</span> and <span class="math inline">\(\hat\sigma\)</span> are the same thing, you start doing exactly that.</p>
<p>To finish this section off, here’s another couple of tables to help keep things clear:</p>
<div class="sourceCode" id="cb510"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb510-1"><a href="inferential-statistics-the-central-limit-theorem.html#cb510-1" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(<span class="fu">data.frame</span>(<span class="at">stringsAsFactors=</span><span class="cn">FALSE</span>,</span>
<span id="cb510-2"><a href="inferential-statistics-the-central-limit-theorem.html#cb510-2" aria-hidden="true" tabindex="-1"></a>                   <span class="at">Symbol =</span> <span class="fu">c</span>(<span class="st">&quot;$s$&quot;</span>, <span class="st">&quot;$</span><span class="sc">\\</span><span class="st">sigma$&quot;</span>, <span class="st">&quot;$</span><span class="sc">\\</span><span class="st">hat{</span><span class="sc">\\</span><span class="st">sigma}$&quot;</span>, <span class="st">&quot;$s^2$&quot;</span>,</span>
<span id="cb510-3"><a href="inferential-statistics-the-central-limit-theorem.html#cb510-3" aria-hidden="true" tabindex="-1"></a>                              <span class="st">&quot;$</span><span class="sc">\\</span><span class="st">sigma^2$&quot;</span>, <span class="st">&quot;$</span><span class="sc">\\</span><span class="st">hat{</span><span class="sc">\\</span><span class="st">sigma}^2$&quot;</span>),</span>
<span id="cb510-4"><a href="inferential-statistics-the-central-limit-theorem.html#cb510-4" aria-hidden="true" tabindex="-1"></a>              <span class="at">What.is.it =</span> <span class="fu">c</span>(<span class="st">&quot;Sample standard deviation&quot;</span>,</span>
<span id="cb510-5"><a href="inferential-statistics-the-central-limit-theorem.html#cb510-5" aria-hidden="true" tabindex="-1"></a>                              <span class="st">&quot;Population standard deviation&quot;</span>,</span>
<span id="cb510-6"><a href="inferential-statistics-the-central-limit-theorem.html#cb510-6" aria-hidden="true" tabindex="-1"></a>                              <span class="st">&quot;Estimate of the population standard deviation&quot;</span>, <span class="st">&quot;Sample variance&quot;</span>,</span>
<span id="cb510-7"><a href="inferential-statistics-the-central-limit-theorem.html#cb510-7" aria-hidden="true" tabindex="-1"></a>                              <span class="st">&quot;Population variance&quot;</span>,</span>
<span id="cb510-8"><a href="inferential-statistics-the-central-limit-theorem.html#cb510-8" aria-hidden="true" tabindex="-1"></a>                              <span class="st">&quot;Estimate of the population variance&quot;</span>),</span>
<span id="cb510-9"><a href="inferential-statistics-the-central-limit-theorem.html#cb510-9" aria-hidden="true" tabindex="-1"></a>   <span class="at">Do.we.know.what.it.is =</span> <span class="fu">c</span>(<span class="st">&quot;Yes - calculated from the raw data&quot;</span>,</span>
<span id="cb510-10"><a href="inferential-statistics-the-central-limit-theorem.html#cb510-10" aria-hidden="true" tabindex="-1"></a>                              <span class="st">&quot;Almost never known for sure&quot;</span>,</span>
<span id="cb510-11"><a href="inferential-statistics-the-central-limit-theorem.html#cb510-11" aria-hidden="true" tabindex="-1"></a>                              <span class="st">&quot;Yes - but not the same as the sample standard deviation&quot;</span>,</span>
<span id="cb510-12"><a href="inferential-statistics-the-central-limit-theorem.html#cb510-12" aria-hidden="true" tabindex="-1"></a>                              <span class="st">&quot;Yes - calculated from the raw data&quot;</span>,</span>
<span id="cb510-13"><a href="inferential-statistics-the-central-limit-theorem.html#cb510-13" aria-hidden="true" tabindex="-1"></a>                              <span class="st">&quot;Almost never known for sure&quot;</span>,</span>
<span id="cb510-14"><a href="inferential-statistics-the-central-limit-theorem.html#cb510-14" aria-hidden="true" tabindex="-1"></a>                              <span class="st">&quot;Yes -  but not the same as the sample variance&quot;</span>)</span>
<span id="cb510-15"><a href="inferential-statistics-the-central-limit-theorem.html#cb510-15" aria-hidden="true" tabindex="-1"></a>))</span></code></pre></div>
<table>
<colgroup>
<col width="14%" />
<col width="38%" />
<col width="47%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Symbol</th>
<th align="left">What.is.it</th>
<th align="left">Do.we.know.what.it.is</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(s\)</span></td>
<td align="left">Sample standard deviation</td>
<td align="left">Yes - calculated from the raw data</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\sigma\)</span></td>
<td align="left">Population standard deviation</td>
<td align="left">Almost never known for sure</td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(\hat{\sigma}\)</span></td>
<td align="left">Estimate of the population standard deviation</td>
<td align="left">Yes - but not the same as the sample standard deviation</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(s^2\)</span></td>
<td align="left">Sample variance</td>
<td align="left">Yes - calculated from the raw data</td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(\sigma^2\)</span></td>
<td align="left">Population variance</td>
<td align="left">Almost never known for sure</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\hat{\sigma}^2\)</span></td>
<td align="left">Estimate of the population variance</td>
<td align="left">Yes - but not the same as the sample variance</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="ci" class="section level2" number="4.9">
<h2><span class="header-section-number">4.9</span> Estimating a confidence interval</h2>
<p>Text by <span class="citation"><a href="#ref-Navarro2018" role="doc-biblioref">Navarro</a> (<a href="#ref-Navarro2018" role="doc-biblioref">2018</a>)</span></p>
<blockquote>
<p><em>Statistics means never having to say you’re certain</em> – Unknown origin<a href="#fn88" class="footnote-ref" id="fnref88"><sup>88</sup></a> but I’ve never found the original source.</p>
</blockquote>
<p>Up to this point in this chapter, I’ve outlined the basics of sampling theory which statisticians rely on to make guesses about population parameters on the basis of a sample of data. As this discussion illustrates, one of the reasons we need all this sampling theory is that every data set leaves us with a some of uncertainty, so our estimates are never going to be perfectly accurate. The thing that has been missing from this discussion is an attempt to <em>quantify</em> the amount of uncertainty that attaches to our estimate. It’s not enough to be able guess that, say, the mean IQ of undergraduate psychology students is 115 (yes, I just made that number up). We also want to be able to say something that expresses the degree of certainty that we have in our guess. For example, it would be nice to be able to say that there is a 95% chance that the true mean lies between 109 and 121. The name for this is a <strong><em>confidence interval</em></strong> for the mean.</p>
<p>Armed with an understanding of sampling distributions, constructing a confidence interval for the mean is actually pretty easy. Here’s how it works. Suppose the true population mean is <span class="math inline">\(\mu\)</span> and the standard deviation is <span class="math inline">\(\sigma\)</span>. I’ve just finished running my study that has <span class="math inline">\(N\)</span> participants, and the mean IQ among those participants is <span class="math inline">\(\bar{X}\)</span>. We know from our discussion of the central limit theorem (Section <a href="inferential-statistics-the-central-limit-theorem.html#clt">4.7.9</a> that the sampling distribution of the mean is approximately normal. We also know from our discussion of the normal distribution Section <a href="#normal"><strong>??</strong></a> that there is a 95% chance that a normally-distributed quantity will fall within two standard deviations of the true mean. To be more precise, we can use the <code>qnorm()</code> function to compute the 2.5th and 97.5th percentiles of the normal distribution</p>
<div class="sourceCode" id="cb511"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb511-1"><a href="inferential-statistics-the-central-limit-theorem.html#cb511-1" aria-hidden="true" tabindex="-1"></a><span class="fu">qnorm</span>( <span class="at">p =</span> <span class="fu">c</span>(.<span class="dv">025</span>, .<span class="dv">975</span>) )</span></code></pre></div>
<pre><code>## [1] -1.959964  1.959964</code></pre>
<p>Okay, so I lied earlier on. The more correct answer is that 95% chance that a normally-distributed quantity will fall within 1.96 standard deviations of the true mean. Next, recall that the standard deviation of the sampling distribution is referred to as the standard error, and the standard error of the mean is written as SEM. When we put all these pieces together, we learn that there is a 95% probability that the sample mean <span class="math inline">\(\bar{X}\)</span> that we have actually observed lies within 1.96 standard errors of the population mean. Mathematically, we write this as:
<span class="math display">\[
\mu - \left( 1.96 \times \mbox{SEM} \right) \ \leq \  \bar{X}\  \leq \  \mu + \left( 1.96 \times \mbox{SEM} \right) 
\]</span>
where the SEM is equal to <span class="math inline">\(\sigma / \sqrt{N}\)</span>, and we can be 95% confident that this is true. However, that’s not answering the question that we’re actually interested in. The equation above tells us what we should expect about the sample mean, given that we know what the population parameters are. What we <em>want</em> is to have this work the other way around: we want to know what we should believe about the population parameters, given that we have observed a particular sample. However, it’s not too difficult to do this. Using a little high school algebra, a sneaky way to rewrite our equation is like this:
<span class="math display">\[
\bar{X} -  \left( 1.96 \times \mbox{SEM} \right) \ \leq \ \mu  \ \leq  \ \bar{X} +  \left( 1.96 \times \mbox{SEM}\right)
\]</span>
What this is telling is is that the range of values has a 95% probability of containing the population mean <span class="math inline">\(\mu\)</span>. We refer to this range as a <strong><em>95% confidence interval</em></strong>, denoted <span class="math inline">\(\mbox{CI}_{95}\)</span>. In short, as long as <span class="math inline">\(N\)</span> is sufficiently large – large enough for us to believe that the sampling distribution of the mean is normal – then we can write this as our formula for the 95% confidence interval:
<span class="math display">\[
\mbox{CI}_{95} = \bar{X} \pm \left( 1.96 \times \frac{\sigma}{\sqrt{N}} \right)
\]</span>
Of course, there’s nothing special about the number 1.96: it just happens to be the multiplier you need to use if you want a 95% confidence interval. If I’d wanted a 70% confidence interval, I could have used the <code>qnorm()</code> function to calculate the 15th and 85th quantiles:</p>
<div class="sourceCode" id="cb513"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb513-1"><a href="inferential-statistics-the-central-limit-theorem.html#cb513-1" aria-hidden="true" tabindex="-1"></a><span class="fu">qnorm</span>( <span class="at">p =</span> <span class="fu">c</span>(.<span class="dv">15</span>, .<span class="dv">85</span>) )</span></code></pre></div>
<pre><code>## [1] -1.036433  1.036433</code></pre>
<p>and so the formula for <span class="math inline">\(\mbox{CI}_{70}\)</span> would be the same as the formula for <span class="math inline">\(\mbox{CI}_{95}\)</span> except that we’d use 1.04 as our magic number rather than 1.96.</p>
<div id="a-slight-mistake-in-the-formula" class="section level3" number="4.9.1">
<h3><span class="header-section-number">4.9.1</span> A slight mistake in the formula</h3>
<p>As usual, I lied. The formula that I’ve given above for the 95% confidence interval is approximately correct, but I glossed over an important detail in the discussion. Notice my formula requires you to use the standard error of the mean, SEM, which in turn requires you to use the true population standard deviation <span class="math inline">\(\sigma\)</span>. Yet, in Section <a href="inferential-statistics-the-central-limit-theorem.html#pointestimates">4.8</a> I stressed the fact that we don’t actually <em>know</em> the true population parameters. Because we don’t know the true value of <span class="math inline">\(\sigma\)</span>, we have to use an estimate of the population standard deviation <span class="math inline">\(\hat{\sigma}\)</span> instead. This is pretty straightforward to do, but this has the consequence that we need to use the quantiles of the <span class="math inline">\(t\)</span>-distribution rather than the normal distribution to calculate our magic number; and the answer depends on the sample size. When <span class="math inline">\(N\)</span> is very large, we get pretty much the same value using <code>qt()</code> that we would if we used <code>qnorm()</code>…</p>
<div class="sourceCode" id="cb515"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb515-1"><a href="inferential-statistics-the-central-limit-theorem.html#cb515-1" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">10000</span>   <span class="co"># suppose our sample size is 10,000</span></span>
<span id="cb515-2"><a href="inferential-statistics-the-central-limit-theorem.html#cb515-2" aria-hidden="true" tabindex="-1"></a><span class="fu">qt</span>( <span class="at">p =</span> .<span class="dv">975</span>, <span class="at">df =</span> N<span class="dv">-1</span>)   <span class="co"># calculate the 97.5th quantile of the t-dist</span></span></code></pre></div>
<pre><code>## [1] 1.960201</code></pre>
<p>But when <span class="math inline">\(N\)</span> is small, we get a much bigger number when we use the <span class="math inline">\(t\)</span> distribution:</p>
<div class="sourceCode" id="cb517"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb517-1"><a href="inferential-statistics-the-central-limit-theorem.html#cb517-1" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">10</span>   <span class="co"># suppose our sample size is 10</span></span>
<span id="cb517-2"><a href="inferential-statistics-the-central-limit-theorem.html#cb517-2" aria-hidden="true" tabindex="-1"></a><span class="fu">qt</span>( <span class="at">p =</span> .<span class="dv">975</span>, <span class="at">df =</span> N<span class="dv">-1</span>)   <span class="co"># calculate the 97.5th quantile of the t-dist</span></span></code></pre></div>
<pre><code>## [1] 2.262157</code></pre>
<p>There’s nothing too mysterious about what’s happening here. Bigger values mean that the confidence interval is wider, indicating that we’re more uncertain about what the true value of <span class="math inline">\(\mu\)</span> actually is. When we use the <span class="math inline">\(t\)</span> distribution instead of the normal distribution, we get bigger numbers, indicating that we have more uncertainty. And why do we have that extra uncertainty? Well, because our estimate of the population standard deviation <span class="math inline">\(\hat\sigma\)</span> might be wrong! If it’s wrong, it implies that we’re a bit less sure about what our sampling distribution of the mean actually looks like… and this uncertainty ends up getting reflected in a wider confidence interval.</p>
</div>
<div id="interpreting-a-confidence-interval" class="section level3" number="4.9.2">
<h3><span class="header-section-number">4.9.2</span> Interpreting a confidence interval</h3>
<p>The hardest thing about confidence intervals is understanding what they <em>mean</em>. Whenever people first encounter confidence intervals, the first instinct is almost always to say that “there is a 95% probabaility that the true mean lies inside the confidence interval.” It’s simple, and it seems to capture the common sense idea of what it means to say that I am “95% confident.” Unfortunately, it’s not quite right. The intuitive definition relies very heavily on your own personal <em>beliefs</em> about the value of the population mean. I say that I am 95% confident because those are my beliefs. In everyday life that’s perfectly okay, but if you remember back to Section <a href="inferential-statistics-the-central-limit-theorem.html#probmeaning">4.4</a>, you’ll notice that talking about personal belief and confidence is a Bayesian idea. Personally (speaking as a Bayesian) I have no problem with the idea that the phrase “95% probability” is allowed to refer to a personal belief. However, confidence intervals are <em>not</em> Bayesian tools. Like everything else in this chapter, confidence intervals are <em>frequentist</em> tools, and if you are going to use frequentist methods then it’s not appropriate to attach a Bayesian interpretation to them. If you use frequentist methods, you must adopt frequentist interpretations!</p>
<p>Okay, so if that’s not the right answer, what is? Remember what we said about frequentist probability: the only way we are allowed to make “probability statements” is to talk about a sequence of events, and to count up the frequencies of different kinds of events. From that perspective, the interpretation of a 95% confidence interval must have something to do with replication. Specifically: if we replicated the experiment over and over again and computed a 95% confidence interval for each replication, then 95% of those <em>intervals</em> would contain the true mean. More generally, 95% of all confidence intervals constructed using this procedure should contain the true population mean. This idea is illustrated in Figure <a href="inferential-statistics-the-central-limit-theorem.html#fig:cirep">4.19</a>, which shows 50 confidence intervals constructed for a “measure 10 IQ scores” experiment (top panel) and another 50 confidence intervals for a “measure 25 IQ scores” experiment (bottom panel). A bit fortuitously, across the 100 replications that I simulated, it turned out that exactly 95 of them contained the true mean.</p>
<div class="figure"><span style="display:block;" id="fig:cirep"></span>
<img src="img/estimation/confIntReplicated.png" alt="95% confidence intervals. The top (panel a) shows 50 simulated replications of an experiment in which we measure the IQs of 10 people. The dot marks the location of the sample mean, and the line shows the 95% confidence interval. In total 47 of the 50 confidence intervals do contain the true mean (i.e., 100), but the three intervals marked with asterisks do not. The lower graph (panel b) shows a similar simulation, but this time we simulate replications of an experiment that measures the IQs of 25 people." width="600" />
<p class="caption">
Figure 4.19: 95% confidence intervals. The top (panel a) shows 50 simulated replications of an experiment in which we measure the IQs of 10 people. The dot marks the location of the sample mean, and the line shows the 95% confidence interval. In total 47 of the 50 confidence intervals do contain the true mean (i.e., 100), but the three intervals marked with asterisks do not. The lower graph (panel b) shows a similar simulation, but this time we simulate replications of an experiment that measures the IQs of 25 people.
</p>
</div>
<p>The critical difference here is that the Bayesian claim makes a probability statement about the population mean (i.e., it refers to our uncertainty about the population mean), which is not allowed under the frequentist interpretation of probability because you can’t “replicate” a population! In the frequentist claim, the population mean is fixed and no probabilistic claims can be made about it. Confidence intervals, however, are repeatable so we can replicate experiments. Therefore a frequentist is allowed to talk about the probability that the <em>confidence interval</em> (a random variable) contains the true mean; but is not allowed to talk about the probability that the <em>true population mean</em> (not a repeatable event) falls within the confidence interval.</p>
<p>I know that this seems a little pedantic, but it does matter. It matters because the difference in interpretation leads to a difference in the mathematics. There is a Bayesian alternative to confidence intervals, known as <em>credible intervals</em>. In most situations credible intervals are quite similar to confidence intervals, but in other cases they are drastically different. As promised, though, I’ll talk more about the Bayesian perspective in Chapter <a href="#bayes"><strong>??</strong></a>.</p>
</div>
<div id="calculating-confidence-intervals-in-r" class="section level3" number="4.9.3">
<h3><span class="header-section-number">4.9.3</span> Calculating confidence intervals in R</h3>
<p>As far as I can tell, the core packages in R don’t include a simple function for calculating confidence intervals for the mean. They <em>do</em> include a lot of complicated, extremely powerful functions that can be used to calculate confidence intervals associated with lots of different things, such as the <code>confint()</code> function that we’ll use in Chapter <a href="regression.html#regression">8</a>. But I figure that when you’re first learning statistics, it might be useful to start with something simpler. As a consequence, the <code>lsr</code> package includes a function called <code>ciMean()</code> which you can use to calculate your confidence intervals. There are two arguments that you might want to specify:<a href="#fn89" class="footnote-ref" id="fnref89"><sup>89</sup></a></p>
<ul>
<li><code>x</code>. This should be a numeric vector containing the data.</li>
<li><code>conf</code>. This should be a number, specifying the confidence level. By default, <code>conf = .95</code>, since 95% confidence intervals are the de facto standard in psychology.</li>
</ul>
<p>So, for example, if I load the <code>afl24.Rdata</code> file, calculate the confidence interval associated with the mean attendance:</p>
<pre><code>&gt; ciMean( x = afl$attendance )
    2.5%    97.5% 
31597.32 32593.12 </code></pre>
<p>Hopefully that’s fairly clear.</p>
</div>
<div id="ciplots" class="section level3" number="4.9.4">
<h3><span class="header-section-number">4.9.4</span> Plotting confidence intervals in R</h3>
<p>There’s several different ways you can draw graphs that show confidence intervals as error bars. I’ll show three versions here, but this certainly doesn’t exhaust the possibilities. In doing so, what I’m assuming is that you want to draw is a plot showing the means and confidence intervals for one variable, broken down by different levels of a second variable. For instance, in our <code>afl</code> data that we discussed earlier, we might be interested in plotting the average <code>attendance</code> by <code>year</code>. I’ll do this using two different functions, <code>bargraph.CI()</code> and <code>lineplot.CI()</code> (both of which are in the <code>sciplot</code> package). Assuming that you’ve installed these packages on your system (see Section <a href="introR.html#packageinstall">2.17</a> if you’ve forgotten how to do this), you’ll need to load them. You’ll also need to load the <code>lsr</code> package, because we’ll make use of the <code>ciMean()</code> function to actually calculate the confidence intervals</p>
<div class="sourceCode" id="cb520"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb520-1"><a href="inferential-statistics-the-central-limit-theorem.html#cb520-1" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>( <span class="fu">file.path</span>(projecthome, <span class="st">&quot;data/afl24.Rdata&quot;</span> ))  <span class="co"># contains the &quot;afl&quot; data frame</span></span>
<span id="cb520-2"><a href="inferential-statistics-the-central-limit-theorem.html#cb520-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>( sciplot )     <span class="co"># bargraph.CI() and lineplot.CI() functions</span></span>
<span id="cb520-3"><a href="inferential-statistics-the-central-limit-theorem.html#cb520-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>( lsr )         <span class="co"># ciMean() function</span></span></code></pre></div>
<p>Here’s how to plot the means and confidence intervals drawn using <code>bargraph.CI()</code>.</p>
<div class="sourceCode" id="cb521"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb521-1"><a href="inferential-statistics-the-central-limit-theorem.html#cb521-1" aria-hidden="true" tabindex="-1"></a><span class="fu">bargraph.CI</span>( <span class="at">x.factor =</span> year,            <span class="co"># grouping variable </span></span>
<span id="cb521-2"><a href="inferential-statistics-the-central-limit-theorem.html#cb521-2" aria-hidden="true" tabindex="-1"></a>              <span class="at">response =</span> attendance,      <span class="co"># outcome variable</span></span>
<span id="cb521-3"><a href="inferential-statistics-the-central-limit-theorem.html#cb521-3" aria-hidden="true" tabindex="-1"></a>              <span class="at">data =</span> afl,                 <span class="co"># data frame with the variables</span></span>
<span id="cb521-4"><a href="inferential-statistics-the-central-limit-theorem.html#cb521-4" aria-hidden="true" tabindex="-1"></a>              <span class="at">ci.fun=</span> ciMean,             <span class="co"># name of the function to calculate CIs</span></span>
<span id="cb521-5"><a href="inferential-statistics-the-central-limit-theorem.html#cb521-5" aria-hidden="true" tabindex="-1"></a>              <span class="at">xlab =</span> <span class="st">&quot;Year&quot;</span>,              <span class="co"># x-axis label</span></span>
<span id="cb521-6"><a href="inferential-statistics-the-central-limit-theorem.html#cb521-6" aria-hidden="true" tabindex="-1"></a>              <span class="at">ylab =</span> <span class="st">&quot;Average Attendance&quot;</span> <span class="co"># y-axis label</span></span>
<span id="cb521-7"><a href="inferential-statistics-the-central-limit-theorem.html#cb521-7" aria-hidden="true" tabindex="-1"></a> )</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:bargraphCI"></span>
<img src="schuster-statistics-remix_files/figure-html/bargraphCI-1.png" alt="Means and 95% confidence intervals for AFL `attendance`, plotted separately for each `year` from 1987 to 2010. This graph was drawn using the `bargraph.CI()` function." width="672" />
<p class="caption">
Figure 4.20: Means and 95% confidence intervals for AFL <code>attendance</code>, plotted separately for each <code>year</code> from 1987 to 2010. This graph was drawn using the <code>bargraph.CI()</code> function.
</p>
</div>
<p>We can use the same arguments when calling the <code>lineplot.CI()</code> function:</p>
<div class="sourceCode" id="cb522"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb522-1"><a href="inferential-statistics-the-central-limit-theorem.html#cb522-1" aria-hidden="true" tabindex="-1"></a><span class="fu">lineplot.CI</span>( <span class="at">x.factor =</span> year,            <span class="co"># grouping variable </span></span>
<span id="cb522-2"><a href="inferential-statistics-the-central-limit-theorem.html#cb522-2" aria-hidden="true" tabindex="-1"></a>             <span class="at">response =</span> attendance,      <span class="co"># outcome variable</span></span>
<span id="cb522-3"><a href="inferential-statistics-the-central-limit-theorem.html#cb522-3" aria-hidden="true" tabindex="-1"></a>             <span class="at">data =</span> afl,                 <span class="co"># data frame with the variables</span></span>
<span id="cb522-4"><a href="inferential-statistics-the-central-limit-theorem.html#cb522-4" aria-hidden="true" tabindex="-1"></a>             <span class="at">ci.fun=</span> ciMean,             <span class="co"># name of the function to calculate CIs</span></span>
<span id="cb522-5"><a href="inferential-statistics-the-central-limit-theorem.html#cb522-5" aria-hidden="true" tabindex="-1"></a>             <span class="at">xlab =</span> <span class="st">&quot;Year&quot;</span>,              <span class="co"># x-axis label</span></span>
<span id="cb522-6"><a href="inferential-statistics-the-central-limit-theorem.html#cb522-6" aria-hidden="true" tabindex="-1"></a>             <span class="at">ylab =</span> <span class="st">&quot;Average Attendance&quot;</span> <span class="co"># y-axis label</span></span>
<span id="cb522-7"><a href="inferential-statistics-the-central-limit-theorem.html#cb522-7" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:lineplotCI"></span>
<img src="schuster-statistics-remix_files/figure-html/lineplotCI-1.png" alt="Means and 95% confidence intervals for AFL `attendance`, plotted separately for each `year` from 1987 to 2010. This graph was drawn using the `lineplot.CI()` function." width="672" />
<p class="caption">
Figure 4.21: Means and 95% confidence intervals for AFL <code>attendance</code>, plotted separately for each <code>year</code> from 1987 to 2010. This graph was drawn using the <code>lineplot.CI()</code> function.
</p>
</div>
</div>
</div>
<div id="summary-3" class="section level2" number="4.10">
<h2><span class="header-section-number">4.10</span> Summary</h2>
<p>Text by <span class="citation"><a href="#ref-Navarro2018" role="doc-biblioref">Navarro</a> (<a href="#ref-Navarro2018" role="doc-biblioref">2018</a>)</span></p>
<p>In this chapter I’ve covered two main topics. The first half of the chapter talks about sampling theory, and the second half talks about how we can use sampling theory to construct estimates of the population parameters. The section breakdown looks like this:</p>
<ul>
<li>Basic ideas about samples, sampling and populations (Section <a href="inferential-statistics-the-central-limit-theorem.html#srs">4.5</a>)</li>
<li>Statistical theory of sampling: the law of large numbers (Section <a href="inferential-statistics-the-central-limit-theorem.html#lawlargenumbers">4.6</a>), sampling distributions and the central limit theorem (Section <a href="inferential-statistics-the-central-limit-theorem.html#samplesandclt">4.7</a>).</li>
<li>Estimating means and standard deviations (Section <a href="inferential-statistics-the-central-limit-theorem.html#pointestimates">4.8</a>)</li>
<li>Estimating a confidence interval (Section <a href="inferential-statistics-the-central-limit-theorem.html#ci">4.9</a>)</li>
</ul>
<p>As always, there’s a lot of topics related to sampling and estimation that aren’t covered in this chapter, but for an introductory psychology class this is fairly comprehensive I think. For most applied researchers you won’t need much more theory than this. One big question that I haven’t touched on in this chapter is what you do when you don’t have a simple random sample. There is a lot of statistical theory you can draw on to handle this situation, but it’s well beyond the scope of this book.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Fisher1922b" class="csl-entry">
Fisher, R. A. 1922. <span>“On the Mathematical Foundation of Theoretical Statistics.”</span> <em>Philosophical Transactions of the Royal Society A</em> 222: 309–68.
</div>
<div id="ref-Keynes1923" class="csl-entry">
Keynes, John Maynard. 1923. <em>A Tract on Monetary Reform</em>. London: Macmillan; Company.
</div>
<div id="ref-Meehl1967" class="csl-entry">
Meehl, P. H. 1967. <span>“Theory Testing in Psychology and Physics: A Methodological Paradox.”</span> <em>Philosophy of Science</em> 34: 103–15.
</div>
<div id="ref-Navarro2018" class="csl-entry">
Navarro, D. 2018. <em>Learning Statistics with r: A Tutorial for Psychology Students and Other Beginners (Version 0.6)</em>. <a href="https://learningstatisticswithr.com">https://learningstatisticswithr.com</a>.
</div>
<div id="ref-Stigler1986" class="csl-entry">
Stigler, S. M. 1986. <em>The History of Statistics</em>. Cambridge, MA: Harvard University Press.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="80">
<li id="fn80"><p>This doesn’t mean that frequentists can’t make hypothetical statements, of course; it’s just that if you want to make a statement about probability, then it must be possible to redescribe that statement in terms of a sequence of potentially observable events, and the relative frequencies of different outcomes that appear within that sequence.<a href="inferential-statistics-the-central-limit-theorem.html#fnref80" class="footnote-back">↩︎</a></p></li>
<li id="fn81"><p>The proper mathematical definition of randomness is extraordinarily technical, and way beyond the scope of this book. We’ll be non-technical here and say that a process has an element of randomness to it whenever it is possible to repeat the process and get different answers each time.<a href="inferential-statistics-the-central-limit-theorem.html#fnref81" class="footnote-back">↩︎</a></p></li>
<li id="fn82"><p>Nothing in life is that simple: there’s not an obvious division of people into binary categories like “schizophrenic” and “not schizophrenic.” But this isn’t a clinical psychology text, so please forgive me a few simplifications here and there.<a href="inferential-statistics-the-central-limit-theorem.html#fnref82" class="footnote-back">↩︎</a></p></li>
<li id="fn83"><p>Technically, the law of large numbers pertains to any sample statistic that can be described as an average of independent quantities. That’s certainly true for the sample mean. However, it’s also possible to write many other sample statistics as averages of one form or another. The variance of a sample, for instance, can be rewritten as a kind of average and so is subject to the law of large numbers. The minimum value of a sample, however, cannot be written as an average of anything and is therefore not governed by the law of large numbers.<a href="inferential-statistics-the-central-limit-theorem.html#fnref83" class="footnote-back">↩︎</a></p></li>
<li id="fn84"><p>As usual, I’m being a bit sloppy here. The central limit theorem is a bit more general than this section implies. Like most introductory stats texts, I’ve discussed one situation where the central limit theorem holds: when you’re taking an average across lots of independent events drawn from the same distribution. However, the central limit theorem is much broader than this. There’s a whole class of things called “<span class="math inline">\(U\)</span>-statistics” for instance, all of which satisfy the central limit theorem and therefore become normally distributed for large sample sizes. The mean is one such statistic, but it’s not the only one.<a href="inferential-statistics-the-central-limit-theorem.html#fnref84" class="footnote-back">↩︎</a></p></li>
<li id="fn85"><p>Please note that if you were <em>actually</em> interested in this question, you would need to be a <em>lot</em> more careful than I’m being here. You <em>can’t</em> just compare IQ scores in Whyalla to Port Pirie and assume that any differences are due to lead poisoning. Even if it were true that the only differences between the two towns corresponded to the different refineries (and it isn’t, not by a long shot), you need to account for the fact that people already <em>believe</em> that lead pollution causes cognitive deficits: if you recall back to Chapter <a href="statistics-for-research.html#studydesign">1.6</a>, this means that there are different demand effects for the Port Pirie sample than for the Whyalla sample. In other words, you might end up with an illusory group difference in your data, caused by the fact that people <em>think</em> that there is a real difference. I find it pretty implausible to think that the locals wouldn’t be well aware of what you were trying to do if a bunch of researchers turned up in Port Pirie with lab coats and IQ tests, and even less plausible to think that a lot of people would be pretty resentful of you for doing it. Those people won’t be as co-operative in the tests. Other people in Port Pirie might be <em>more</em> motivated to do well because they don’t want their home town to look bad. The motivational effects that would apply in Whyalla are likely to be weaker, because people don’t have any concept of “iron ore poisoning” in the same way that they have a concept for “lead poisoning.” Psychology is <em>hard</em>.<a href="inferential-statistics-the-central-limit-theorem.html#fnref85" class="footnote-back">↩︎</a></p></li>
<li id="fn86"><p>I should note that I’m hiding something here. Unbiasedness is a desirable characteristic for an estimator, but there are other things that matter besides bias. However, it’s beyond the scope of this book to discuss this in any detail. I just want to draw your attention to the fact that there’s some hidden complexity here.<a href="inferential-statistics-the-central-limit-theorem.html#fnref86" class="footnote-back">↩︎</a></p></li>
<li id="fn87"><p>Okay, I’m hiding something else here. In a bizarre and counterintuitive twist, since <span class="math inline">\(\hat\sigma^2\)</span> is an unbiased estimator of <span class="math inline">\(\sigma^2\)</span>, you’d assume that taking the square root would be fine, and <span class="math inline">\(\hat\sigma\)</span> would be an unbiased estimator of <span class="math inline">\(\sigma\)</span>. Right? Weirdly, it’s not. There’s actually a subtle, tiny bias in <span class="math inline">\(\hat\sigma\)</span>. This is just bizarre: <span class="math inline">\(\hat\sigma^2\)</span> is and unbiased estimate of the population variance <span class="math inline">\(\sigma^2\)</span>, but when you take the square root, it turns out that <span class="math inline">\(\hat\sigma\)</span> is a biased estimator of the population standard deviation <span class="math inline">\(\sigma\)</span>. Weird, weird, weird, right? So, why is <span class="math inline">\(\hat\sigma\)</span> biased? The technical answer is “because non-linear transformations (e.g., the square root) don’t commute with expectation,” but that just sounds like gibberish to everyone who hasn’t taken a course in mathematical statistics. Fortunately, it doesn’t matter for practical purposes. The bias is small, and in real life everyone uses <span class="math inline">\(\hat\sigma\)</span> and it works just fine. Sometimes mathematics is just annoying.<a href="inferential-statistics-the-central-limit-theorem.html#fnref87" class="footnote-back">↩︎</a></p></li>
<li id="fn88"><p>This quote appears on a great many t-shirts and websites, and even gets a mention in a few academic papers (e.g., \url{<a href="http://www.amstat.org/publications/jse/v10n3/friedman.html" class="uri">http://www.amstat.org/publications/jse/v10n3/friedman.html</a><a href="inferential-statistics-the-central-limit-theorem.html#fnref88" class="footnote-back">↩︎</a></p></li>
<li id="fn89"><p>As of the current writing, these are the only arguments to the function. However, I am planning to add a bit more functionality to <code>ciMean()</code>. However, regardless of what those future changes might look like, the <code>x</code> and <code>conf</code> arguments will remain the same, and the commands used in this book will still work.<a href="inferential-statistics-the-central-limit-theorem.html#fnref89" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="descriptives.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="hypothesistesting.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/04-inferential.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["schuster-statistics-remix.pdf", "schuster-statistics-remix.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
