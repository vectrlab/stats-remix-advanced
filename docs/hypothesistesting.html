<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Hypothesis testing | Advanced Statistics Remix</title>
  <meta name="description" content="A textbook for advanced statistics" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Hypothesis testing | Advanced Statistics Remix" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="A textbook for advanced statistics" />
  <meta name="github-repo" content="vectrlab/stat-course-pack" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Hypothesis testing | Advanced Statistics Remix" />
  
  <meta name="twitter:description" content="A textbook for advanced statistics" />
  

<meta name="author" content="David Schuster" />


<meta name="date" content="2021-10-11" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="inferential-statistics-the-central-limit-theorem.html"/>
<link rel="next" href="issues-in-hypothesis-testing.html"/>
<script src="libs/header-attrs-2.10/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Course Pack for Statistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About this Book</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#attribution"><i class="fa fa-check"></i>Attribution</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i>License</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="statistics-for-research.html"><a href="statistics-for-research.html"><i class="fa fa-check"></i><b>1</b> Statistics for Research</a>
<ul>
<li class="chapter" data-level="1.1" data-path="statistics-for-research.html"><a href="statistics-for-research.html#introduction"><i class="fa fa-check"></i><b>1.1</b> Introduction</a></li>
<li class="chapter" data-level="1.2" data-path="statistics-for-research.html"><a href="statistics-for-research.html#measurement"><i class="fa fa-check"></i><b>1.2</b> Measurement</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="statistics-for-research.html"><a href="statistics-for-research.html#level-of-measurement"><i class="fa fa-check"></i><b>1.2.1</b> Level of Measurement</a></li>
<li class="chapter" data-level="1.2.2" data-path="statistics-for-research.html"><a href="statistics-for-research.html#continuous-or-discrete"><i class="fa fa-check"></i><b>1.2.2</b> Continuous or Discrete</a></li>
<li class="chapter" data-level="1.2.3" data-path="statistics-for-research.html"><a href="statistics-for-research.html#qualitative-or-quantitative"><i class="fa fa-check"></i><b>1.2.3</b> Qualitative or Quantitative</a></li>
<li class="chapter" data-level="1.2.4" data-path="statistics-for-research.html"><a href="statistics-for-research.html#distribution-a-collection-of-our-observations"><i class="fa fa-check"></i><b>1.2.4</b> Distribution: A collection of our observations</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="statistics-for-research.html"><a href="statistics-for-research.html#descriptive-statistics-summarizing-our-observations"><i class="fa fa-check"></i><b>1.3</b> Descriptive Statistics: Summarizing our observations</a></li>
<li class="chapter" data-level="1.4" data-path="statistics-for-research.html"><a href="statistics-for-research.html#inferential-statistics-generalizing-from-our-observations"><i class="fa fa-check"></i><b>1.4</b> Inferential Statistics: Generalizing from our observations</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="statistics-for-research.html"><a href="statistics-for-research.html#populations-and-samples-who-or-what-the-research-is-about"><i class="fa fa-check"></i><b>1.4.1</b> Populations and Samples: Who (or what) the research is about</a></li>
<li class="chapter" data-level="1.4.2" data-path="statistics-for-research.html"><a href="statistics-for-research.html#constructs-provide-the-context"><i class="fa fa-check"></i><b>1.4.2</b> Constructs provide the context</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="statistics-for-research.html"><a href="statistics-for-research.html#the-cautionary-tale-of-simpsons-paradox"><i class="fa fa-check"></i><b>1.5</b> The cautionary tale of Simpson’s paradox</a></li>
<li class="chapter" data-level="1.6" data-path="statistics-for-research.html"><a href="statistics-for-research.html#studydesign"><i class="fa fa-check"></i><b>1.6</b> A brief introduction to research design</a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="statistics-for-research.html"><a href="statistics-for-research.html#some-thoughts-about-psychological-measurement"><i class="fa fa-check"></i><b>1.6.1</b> Some thoughts about psychological measurement</a></li>
<li class="chapter" data-level="1.6.2" data-path="statistics-for-research.html"><a href="statistics-for-research.html#operationalisation-defining-your-measurement"><i class="fa fa-check"></i><b>1.6.2</b> Operationalisation: defining your measurement</a></li>
<li class="chapter" data-level="1.6.3" data-path="statistics-for-research.html"><a href="statistics-for-research.html#ivdv"><i class="fa fa-check"></i><b>1.6.3</b> The “role” of variables: predictors and outcomes</a></li>
<li class="chapter" data-level="1.6.4" data-path="statistics-for-research.html"><a href="statistics-for-research.html#researchdesigns"><i class="fa fa-check"></i><b>1.6.4</b> Experimental and non-experimental research</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="statistics-for-research.html"><a href="statistics-for-research.html#causality-research-and-statistics"><i class="fa fa-check"></i><b>1.7</b> Causality, Research, and Statistics</a>
<ul>
<li class="chapter" data-level="1.7.1" data-path="statistics-for-research.html"><a href="statistics-for-research.html#experimental-quasi-experimental-and-non-experimental-studies"><i class="fa fa-check"></i><b>1.7.1</b> Experimental, Quasi-Experimental, and Non-Experimental Studies</a></li>
<li class="chapter" data-level="1.7.2" data-path="statistics-for-research.html"><a href="statistics-for-research.html#demonstrating-causality"><i class="fa fa-check"></i><b>1.7.2</b> Demonstrating Causality</a></li>
<li class="chapter" data-level="1.7.3" data-path="statistics-for-research.html"><a href="statistics-for-research.html#statistics-and-causality"><i class="fa fa-check"></i><b>1.7.3</b> Statistics and Causality</a></li>
<li class="chapter" data-level="1.7.4" data-path="statistics-for-research.html"><a href="statistics-for-research.html#validity-and-reliability"><i class="fa fa-check"></i><b>1.7.4</b> Validity and Reliability</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introR.html"><a href="introR.html"><i class="fa fa-check"></i><b>2</b> Getting started with R</a>
<ul>
<li class="chapter" data-level="2.1" data-path="introR.html"><a href="introR.html#videos"><i class="fa fa-check"></i><b>2.1</b> Videos</a></li>
<li class="chapter" data-level="2.2" data-path="introR.html"><a href="introR.html#introduction-1"><i class="fa fa-check"></i><b>2.2</b> Introduction</a></li>
<li class="chapter" data-level="2.3" data-path="introR.html"><a href="introR.html#gettingR"><i class="fa fa-check"></i><b>2.3</b> Installing R</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="introR.html"><a href="introR.html#installing-r-on-a-windows-computer"><i class="fa fa-check"></i><b>2.3.1</b> Installing R on a Windows computer</a></li>
<li class="chapter" data-level="2.3.2" data-path="introR.html"><a href="introR.html#installing-r-on-a-mac"><i class="fa fa-check"></i><b>2.3.2</b> Installing R on a Mac</a></li>
<li class="chapter" data-level="2.3.3" data-path="introR.html"><a href="introR.html#installing-r-on-a-linux-computer"><i class="fa fa-check"></i><b>2.3.3</b> Installing R on a Linux computer</a></li>
<li class="chapter" data-level="2.3.4" data-path="introR.html"><a href="introR.html#installingrstudio"><i class="fa fa-check"></i><b>2.3.4</b> Downloading and installing RStudio</a></li>
<li class="chapter" data-level="2.3.5" data-path="introR.html"><a href="introR.html#startingR"><i class="fa fa-check"></i><b>2.3.5</b> Starting up R</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="introR.html"><a href="introR.html#firstcommand"><i class="fa fa-check"></i><b>2.4</b> Typing commands at the R console</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="introR.html"><a href="introR.html#an-important-digression-about-formatting"><i class="fa fa-check"></i><b>2.4.1</b> An important digression about formatting</a></li>
<li class="chapter" data-level="2.4.2" data-path="introR.html"><a href="introR.html#be-very-careful-to-avoid-typos"><i class="fa fa-check"></i><b>2.4.2</b> Be very careful to avoid typos</a></li>
<li class="chapter" data-level="2.4.3" data-path="introR.html"><a href="introR.html#r-is-a-bit-flexible-with-spacing"><i class="fa fa-check"></i><b>2.4.3</b> R is (a bit) flexible with spacing</a></li>
<li class="chapter" data-level="2.4.4" data-path="introR.html"><a href="introR.html#r-can-sometimes-tell-that-youre-not-finished-yet-but-not-often"><i class="fa fa-check"></i><b>2.4.4</b> R can sometimes tell that you’re not finished yet (but not often)</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="introR.html"><a href="introR.html#arithmetic"><i class="fa fa-check"></i><b>2.5</b> Doing simple calculations with R</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="introR.html"><a href="introR.html#adding-subtracting-multiplying-and-dividing"><i class="fa fa-check"></i><b>2.5.1</b> Adding, subtracting, multiplying and dividing</a></li>
<li class="chapter" data-level="2.5.2" data-path="introR.html"><a href="introR.html#taking-powers"><i class="fa fa-check"></i><b>2.5.2</b> Taking powers</a></li>
<li class="chapter" data-level="2.5.3" data-path="introR.html"><a href="introR.html#bedmas"><i class="fa fa-check"></i><b>2.5.3</b> Doing calculations in the right order</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="introR.html"><a href="introR.html#assign"><i class="fa fa-check"></i><b>2.6</b> Storing a number as a variable</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="introR.html"><a href="introR.html#variable-assignment-using---and--"><i class="fa fa-check"></i><b>2.6.1</b> Variable assignment using <code>&lt;-</code> and <code>-&gt;</code></a></li>
<li class="chapter" data-level="2.6.2" data-path="introR.html"><a href="introR.html#doing-calculations-using-variables"><i class="fa fa-check"></i><b>2.6.2</b> Doing calculations using variables</a></li>
<li class="chapter" data-level="2.6.3" data-path="introR.html"><a href="introR.html#rules-and-conventions-for-naming-variables"><i class="fa fa-check"></i><b>2.6.3</b> Rules and conventions for naming variables</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="introR.html"><a href="introR.html#usingfunctions"><i class="fa fa-check"></i><b>2.7</b> Using functions to do calculations</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="introR.html"><a href="introR.html#functionarguments"><i class="fa fa-check"></i><b>2.7.1</b> Function arguments, their names and their defaults</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="introR.html"><a href="introR.html#RStudio1"><i class="fa fa-check"></i><b>2.8</b> Letting RStudio help you with your commands</a>
<ul>
<li class="chapter" data-level="2.8.1" data-path="introR.html"><a href="introR.html#autocomplete-using-tab"><i class="fa fa-check"></i><b>2.8.1</b> Autocomplete using “tab”</a></li>
<li class="chapter" data-level="2.8.2" data-path="introR.html"><a href="introR.html#browsing-your-command-history"><i class="fa fa-check"></i><b>2.8.2</b> Browsing your command history</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="introR.html"><a href="introR.html#vectors"><i class="fa fa-check"></i><b>2.9</b> Storing many numbers as a vector</a>
<ul>
<li class="chapter" data-level="2.9.1" data-path="introR.html"><a href="introR.html#creating-a-vector"><i class="fa fa-check"></i><b>2.9.1</b> Creating a vector</a></li>
<li class="chapter" data-level="2.9.2" data-path="introR.html"><a href="introR.html#a-handy-digression"><i class="fa fa-check"></i><b>2.9.2</b> A handy digression</a></li>
<li class="chapter" data-level="2.9.3" data-path="introR.html"><a href="introR.html#vectorsubset"><i class="fa fa-check"></i><b>2.9.3</b> Getting information out of vectors</a></li>
<li class="chapter" data-level="2.9.4" data-path="introR.html"><a href="introR.html#altering-the-elements-of-a-vector"><i class="fa fa-check"></i><b>2.9.4</b> Altering the elements of a vector</a></li>
<li class="chapter" data-level="2.9.5" data-path="introR.html"><a href="introR.html#veclength"><i class="fa fa-check"></i><b>2.9.5</b> Useful things to know about vectors</a></li>
</ul></li>
<li class="chapter" data-level="2.10" data-path="introR.html"><a href="introR.html#text"><i class="fa fa-check"></i><b>2.10</b> Storing text data</a>
<ul>
<li class="chapter" data-level="2.10.1" data-path="introR.html"><a href="introR.html#simpletext"><i class="fa fa-check"></i><b>2.10.1</b> Working with text</a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="introR.html"><a href="introR.html#logicals"><i class="fa fa-check"></i><b>2.11</b> Storing “true or false” data</a>
<ul>
<li class="chapter" data-level="2.11.1" data-path="introR.html"><a href="introR.html#assessing-mathematical-truths"><i class="fa fa-check"></i><b>2.11.1</b> Assessing mathematical truths</a></li>
<li class="chapter" data-level="2.11.2" data-path="introR.html"><a href="introR.html#logical-operations"><i class="fa fa-check"></i><b>2.11.2</b> Logical operations</a></li>
<li class="chapter" data-level="2.11.3" data-path="introR.html"><a href="introR.html#storing-and-using-logical-data"><i class="fa fa-check"></i><b>2.11.3</b> Storing and using logical data</a></li>
<li class="chapter" data-level="2.11.4" data-path="introR.html"><a href="introR.html#vectors-of-logicals"><i class="fa fa-check"></i><b>2.11.4</b> Vectors of logicals</a></li>
<li class="chapter" data-level="2.11.5" data-path="introR.html"><a href="introR.html#logictext"><i class="fa fa-check"></i><b>2.11.5</b> Applying logical operation to text</a></li>
</ul></li>
<li class="chapter" data-level="2.12" data-path="introR.html"><a href="introR.html#indexing"><i class="fa fa-check"></i><b>2.12</b> Indexing vectors</a>
<ul>
<li class="chapter" data-level="2.12.1" data-path="introR.html"><a href="introR.html#extracting-multiple-elements"><i class="fa fa-check"></i><b>2.12.1</b> Extracting multiple elements</a></li>
<li class="chapter" data-level="2.12.2" data-path="introR.html"><a href="introR.html#logical-indexing"><i class="fa fa-check"></i><b>2.12.2</b> Logical indexing</a></li>
</ul></li>
<li class="chapter" data-level="2.13" data-path="introR.html"><a href="introR.html#quitting-r"><i class="fa fa-check"></i><b>2.13</b> Quitting R</a></li>
<li class="chapter" data-level="2.14" data-path="introR.html"><a href="introR.html#summary"><i class="fa fa-check"></i><b>2.14</b> Summary</a></li>
<li class="chapter" data-level="2.15" data-path="introR.html"><a href="introR.html#mechanics"><i class="fa fa-check"></i><b>2.15</b> Additional R concepts</a></li>
<li class="chapter" data-level="2.16" data-path="introR.html"><a href="introR.html#comments"><i class="fa fa-check"></i><b>2.16</b> Using comments</a></li>
<li class="chapter" data-level="2.17" data-path="introR.html"><a href="introR.html#packageinstall"><i class="fa fa-check"></i><b>2.17</b> Installing and loading packages</a>
<ul>
<li class="chapter" data-level="2.17.1" data-path="introR.html"><a href="introR.html#the-package-panel-in-rstudio"><i class="fa fa-check"></i><b>2.17.1</b> The package panel in RStudio</a></li>
<li class="chapter" data-level="2.17.2" data-path="introR.html"><a href="introR.html#packageload"><i class="fa fa-check"></i><b>2.17.2</b> Loading a package</a></li>
<li class="chapter" data-level="2.17.3" data-path="introR.html"><a href="introR.html#packageunload"><i class="fa fa-check"></i><b>2.17.3</b> Unloading a package</a></li>
<li class="chapter" data-level="2.17.4" data-path="introR.html"><a href="introR.html#a-few-extra-comments"><i class="fa fa-check"></i><b>2.17.4</b> A few extra comments</a></li>
<li class="chapter" data-level="2.17.5" data-path="introR.html"><a href="introR.html#downloading-new-packages"><i class="fa fa-check"></i><b>2.17.5</b> Downloading new packages</a></li>
<li class="chapter" data-level="2.17.6" data-path="introR.html"><a href="introR.html#updating-r-and-r-packages"><i class="fa fa-check"></i><b>2.17.6</b> Updating R and R packages</a></li>
<li class="chapter" data-level="2.17.7" data-path="introR.html"><a href="introR.html#what-packages-does-this-book-use"><i class="fa fa-check"></i><b>2.17.7</b> What packages does this book use?</a></li>
</ul></li>
<li class="chapter" data-level="2.18" data-path="introR.html"><a href="introR.html#workspace"><i class="fa fa-check"></i><b>2.18</b> Managing the workspace</a>
<ul>
<li class="chapter" data-level="2.18.1" data-path="introR.html"><a href="introR.html#listing-the-contents-of-the-workspace"><i class="fa fa-check"></i><b>2.18.1</b> Listing the contents of the workspace</a></li>
<li class="chapter" data-level="2.18.2" data-path="introR.html"><a href="introR.html#removing-variables-from-the-workspace"><i class="fa fa-check"></i><b>2.18.2</b> Removing variables from the workspace</a></li>
</ul></li>
<li class="chapter" data-level="2.19" data-path="introR.html"><a href="introR.html#navigation"><i class="fa fa-check"></i><b>2.19</b> Navigating the file system</a>
<ul>
<li class="chapter" data-level="2.19.1" data-path="introR.html"><a href="introR.html#filesystem"><i class="fa fa-check"></i><b>2.19.1</b> The file system itself</a></li>
<li class="chapter" data-level="2.19.2" data-path="introR.html"><a href="introR.html#navigationR"><i class="fa fa-check"></i><b>2.19.2</b> Navigating the file system using the R console</a></li>
<li class="chapter" data-level="2.19.3" data-path="introR.html"><a href="introR.html#why-do-the-windows-paths-use-the-wrong-slash"><i class="fa fa-check"></i><b>2.19.3</b> Why do the Windows paths use the wrong slash?</a></li>
<li class="chapter" data-level="2.19.4" data-path="introR.html"><a href="introR.html#nav3"><i class="fa fa-check"></i><b>2.19.4</b> Navigating the file system using the RStudio file panel</a></li>
</ul></li>
<li class="chapter" data-level="2.20" data-path="introR.html"><a href="introR.html#load"><i class="fa fa-check"></i><b>2.20</b> Loading and saving data</a>
<ul>
<li class="chapter" data-level="2.20.1" data-path="introR.html"><a href="introR.html#loading-workspace-files-using-r"><i class="fa fa-check"></i><b>2.20.1</b> Loading workspace files using R</a></li>
<li class="chapter" data-level="2.20.2" data-path="introR.html"><a href="introR.html#loading-workspace-files-using-rstudio"><i class="fa fa-check"></i><b>2.20.2</b> Loading workspace files using RStudio</a></li>
<li class="chapter" data-level="2.20.3" data-path="introR.html"><a href="introR.html#loadingcsv"><i class="fa fa-check"></i><b>2.20.3</b> Importing data from CSV files using loadingcsv</a></li>
<li class="chapter" data-level="2.20.4" data-path="introR.html"><a href="introR.html#importing-data-from-csv-files-using-rstudio"><i class="fa fa-check"></i><b>2.20.4</b> Importing data from CSV files using RStudio</a></li>
<li class="chapter" data-level="2.20.5" data-path="introR.html"><a href="introR.html#saving-a-workspace-file-using-save"><i class="fa fa-check"></i><b>2.20.5</b> Saving a workspace file using <code>save</code></a></li>
<li class="chapter" data-level="2.20.6" data-path="introR.html"><a href="introR.html#save1"><i class="fa fa-check"></i><b>2.20.6</b> Saving a workspace file using RStudio</a></li>
<li class="chapter" data-level="2.20.7" data-path="introR.html"><a href="introR.html#other-things-you-might-want-to-save"><i class="fa fa-check"></i><b>2.20.7</b> Other things you might want to save</a></li>
</ul></li>
<li class="chapter" data-level="2.21" data-path="introR.html"><a href="introR.html#useful"><i class="fa fa-check"></i><b>2.21</b> Useful things to know about variables</a>
<ul>
<li class="chapter" data-level="2.21.1" data-path="introR.html"><a href="introR.html#specials"><i class="fa fa-check"></i><b>2.21.1</b> Special values</a></li>
<li class="chapter" data-level="2.21.2" data-path="introR.html"><a href="introR.html#names"><i class="fa fa-check"></i><b>2.21.2</b> Assigning names to vector elements</a></li>
<li class="chapter" data-level="2.21.3" data-path="introR.html"><a href="introR.html#variable-classes"><i class="fa fa-check"></i><b>2.21.3</b> Variable classes</a></li>
</ul></li>
<li class="chapter" data-level="2.22" data-path="introR.html"><a href="introR.html#factors"><i class="fa fa-check"></i><b>2.22</b> Factors</a>
<ul>
<li class="chapter" data-level="2.22.1" data-path="introR.html"><a href="introR.html#introducing-factors"><i class="fa fa-check"></i><b>2.22.1</b> Introducing factors</a></li>
<li class="chapter" data-level="2.22.2" data-path="introR.html"><a href="introR.html#labelling-the-factor-levels"><i class="fa fa-check"></i><b>2.22.2</b> Labelling the factor levels</a></li>
<li class="chapter" data-level="2.22.3" data-path="introR.html"><a href="introR.html#moving-on"><i class="fa fa-check"></i><b>2.22.3</b> Moving on…</a></li>
</ul></li>
<li class="chapter" data-level="2.23" data-path="introR.html"><a href="introR.html#dataframes"><i class="fa fa-check"></i><b>2.23</b> Data frames</a>
<ul>
<li class="chapter" data-level="2.23.1" data-path="introR.html"><a href="introR.html#introducing-data-frames"><i class="fa fa-check"></i><b>2.23.1</b> Introducing data frames</a></li>
<li class="chapter" data-level="2.23.2" data-path="introR.html"><a href="introR.html#pulling-out-the-contents-of-the-data-frame-using"><i class="fa fa-check"></i><b>2.23.2</b> Pulling out the contents of the data frame using <code>$</code></a></li>
<li class="chapter" data-level="2.23.3" data-path="introR.html"><a href="introR.html#getting-information-about-a-data-frame"><i class="fa fa-check"></i><b>2.23.3</b> Getting information about a data frame</a></li>
<li class="chapter" data-level="2.23.4" data-path="introR.html"><a href="introR.html#looking-for-more-on-data-frames"><i class="fa fa-check"></i><b>2.23.4</b> Looking for more on data frames?</a></li>
</ul></li>
<li class="chapter" data-level="2.24" data-path="introR.html"><a href="introR.html#lists"><i class="fa fa-check"></i><b>2.24</b> Lists</a></li>
<li class="chapter" data-level="2.25" data-path="introR.html"><a href="introR.html#formulas"><i class="fa fa-check"></i><b>2.25</b> Formulas</a></li>
<li class="chapter" data-level="2.26" data-path="introR.html"><a href="introR.html#generics"><i class="fa fa-check"></i><b>2.26</b> Generic functions</a></li>
<li class="chapter" data-level="2.27" data-path="introR.html"><a href="introR.html#help"><i class="fa fa-check"></i><b>2.27</b> Getting help</a>
<ul>
<li class="chapter" data-level="2.27.1" data-path="introR.html"><a href="introR.html#how-to-read-the-help-documentation"><i class="fa fa-check"></i><b>2.27.1</b> How to read the help documentation</a></li>
<li class="chapter" data-level="2.27.2" data-path="introR.html"><a href="introR.html#other-resources"><i class="fa fa-check"></i><b>2.27.2</b> Other resources</a></li>
</ul></li>
<li class="chapter" data-level="2.28" data-path="introR.html"><a href="introR.html#summary-1"><i class="fa fa-check"></i><b>2.28</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="descriptives.html"><a href="descriptives.html"><i class="fa fa-check"></i><b>3</b> Descriptive statistics</a>
<ul>
<li class="chapter" data-level="3.1" data-path="descriptives.html"><a href="descriptives.html#videos-1"><i class="fa fa-check"></i><b>3.1</b> Videos</a></li>
<li class="chapter" data-level="3.2" data-path="descriptives.html"><a href="descriptives.html#introduction-2"><i class="fa fa-check"></i><b>3.2</b> Introduction</a></li>
<li class="chapter" data-level="3.3" data-path="descriptives.html"><a href="descriptives.html#centraltendency"><i class="fa fa-check"></i><b>3.3</b> Measures of central tendency</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="descriptives.html"><a href="descriptives.html#mean"><i class="fa fa-check"></i><b>3.3.1</b> The mean</a></li>
<li class="chapter" data-level="3.3.2" data-path="descriptives.html"><a href="descriptives.html#calculating-the-mean-in-r"><i class="fa fa-check"></i><b>3.3.2</b> Calculating the mean in R</a></li>
<li class="chapter" data-level="3.3.3" data-path="descriptives.html"><a href="descriptives.html#median"><i class="fa fa-check"></i><b>3.3.3</b> The median</a></li>
<li class="chapter" data-level="3.3.4" data-path="descriptives.html"><a href="descriptives.html#mean-or-median-whats-the-difference"><i class="fa fa-check"></i><b>3.3.4</b> Mean or median? What’s the difference?</a></li>
<li class="chapter" data-level="3.3.5" data-path="descriptives.html"><a href="descriptives.html#housingpriceexample"><i class="fa fa-check"></i><b>3.3.5</b> A real life example</a></li>
<li class="chapter" data-level="3.3.6" data-path="descriptives.html"><a href="descriptives.html#trimmedmean"><i class="fa fa-check"></i><b>3.3.6</b> Trimmed mean</a></li>
<li class="chapter" data-level="3.3.7" data-path="descriptives.html"><a href="descriptives.html#mode"><i class="fa fa-check"></i><b>3.3.7</b> Mode</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="descriptives.html"><a href="descriptives.html#var"><i class="fa fa-check"></i><b>3.4</b> Measures of variability</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="descriptives.html"><a href="descriptives.html#range"><i class="fa fa-check"></i><b>3.4.1</b> Range</a></li>
<li class="chapter" data-level="3.4.2" data-path="descriptives.html"><a href="descriptives.html#interquartile-range"><i class="fa fa-check"></i><b>3.4.2</b> Interquartile range</a></li>
<li class="chapter" data-level="3.4.3" data-path="descriptives.html"><a href="descriptives.html#aad"><i class="fa fa-check"></i><b>3.4.3</b> Mean absolute deviation</a></li>
<li class="chapter" data-level="3.4.4" data-path="descriptives.html"><a href="descriptives.html#variance"><i class="fa fa-check"></i><b>3.4.4</b> Variance</a></li>
<li class="chapter" data-level="3.4.5" data-path="descriptives.html"><a href="descriptives.html#sd"><i class="fa fa-check"></i><b>3.4.5</b> Standard deviation</a></li>
<li class="chapter" data-level="3.4.6" data-path="descriptives.html"><a href="descriptives.html#mad"><i class="fa fa-check"></i><b>3.4.6</b> Median absolute deviation</a></li>
<li class="chapter" data-level="3.4.7" data-path="descriptives.html"><a href="descriptives.html#which-measure-to-use"><i class="fa fa-check"></i><b>3.4.7</b> Which measure to use?</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="descriptives.html"><a href="descriptives.html#skewandkurtosis"><i class="fa fa-check"></i><b>3.5</b> Skew and kurtosis</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="descriptives.html"><a href="descriptives.html#more-detail-on-skewness-measures"><i class="fa fa-check"></i><b>3.5.1</b> More detail on skewness measures</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="descriptives.html"><a href="descriptives.html#descriptive-summary"><i class="fa fa-check"></i><b>3.6</b> Getting an overall summary of a variable</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="descriptives.html"><a href="descriptives.html#summarising-a-variable"><i class="fa fa-check"></i><b>3.6.1</b> “Summarising” a variable</a></li>
<li class="chapter" data-level="3.6.2" data-path="descriptives.html"><a href="descriptives.html#summarising-a-data-frame"><i class="fa fa-check"></i><b>3.6.2</b> “Summarising” a data frame</a></li>
<li class="chapter" data-level="3.6.3" data-path="descriptives.html"><a href="descriptives.html#describing-a-data-frame"><i class="fa fa-check"></i><b>3.6.3</b> “Describing” a data frame</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="descriptives.html"><a href="descriptives.html#groupdescriptives"><i class="fa fa-check"></i><b>3.7</b> Descriptive statistics separately for each group</a></li>
<li class="chapter" data-level="3.8" data-path="descriptives.html"><a href="descriptives.html#good-descriptive-statistics-are-descriptive"><i class="fa fa-check"></i><b>3.8</b> Good descriptive statistics are descriptive!</a></li>
<li class="chapter" data-level="3.9" data-path="descriptives.html"><a href="descriptives.html#graphics"><i class="fa fa-check"></i><b>3.9</b> Drawing graphs</a>
<ul>
<li class="chapter" data-level="3.9.1" data-path="descriptives.html"><a href="descriptives.html#introplotting"><i class="fa fa-check"></i><b>3.9.1</b> An introduction to plotting</a></li>
<li class="chapter" data-level="3.9.2" data-path="descriptives.html"><a href="descriptives.html#hist"><i class="fa fa-check"></i><b>3.9.2</b> Histograms</a></li>
<li class="chapter" data-level="3.9.3" data-path="descriptives.html"><a href="descriptives.html#boxplots"><i class="fa fa-check"></i><b>3.9.3</b> Boxplots</a></li>
<li class="chapter" data-level="3.9.4" data-path="descriptives.html"><a href="descriptives.html#bargraph"><i class="fa fa-check"></i><b>3.9.4</b> Bar graphs</a></li>
<li class="chapter" data-level="3.9.5" data-path="descriptives.html"><a href="descriptives.html#saveimage"><i class="fa fa-check"></i><b>3.9.5</b> Saving image files using R and Rstudio</a></li>
<li class="chapter" data-level="3.9.6" data-path="descriptives.html"><a href="descriptives.html#summary-2"><i class="fa fa-check"></i><b>3.9.6</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html"><i class="fa fa-check"></i><b>4</b> Inferential statistics: The Central Limit Theorem</a>
<ul>
<li class="chapter" data-level="4.1" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#videos-2"><i class="fa fa-check"></i><b>4.1</b> Videos</a></li>
<li class="chapter" data-level="4.2" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#estimation"><i class="fa fa-check"></i><b>4.2</b> Introduction</a></li>
<li class="chapter" data-level="4.3" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#probstats"><i class="fa fa-check"></i><b>4.3</b> How are probability and statistics different?</a></li>
<li class="chapter" data-level="4.4" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#probmeaning"><i class="fa fa-check"></i><b>4.4</b> What does probability mean?</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#the-frequentist-view"><i class="fa fa-check"></i><b>4.4.1</b> The frequentist view</a></li>
<li class="chapter" data-level="4.4.2" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#the-bayesian-view"><i class="fa fa-check"></i><b>4.4.2</b> The Bayesian view</a></li>
<li class="chapter" data-level="4.4.3" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#whats-the-difference-and-who-is-right"><i class="fa fa-check"></i><b>4.4.3</b> What’s the difference? And who is right?</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#srs"><i class="fa fa-check"></i><b>4.5</b> Samples, populations and sampling</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#pop"><i class="fa fa-check"></i><b>4.5.1</b> Defining a population</a></li>
<li class="chapter" data-level="4.5.2" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#simple-random-samples"><i class="fa fa-check"></i><b>4.5.2</b> Simple random samples</a></li>
<li class="chapter" data-level="4.5.3" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#most-samples-are-not-simple-random-samples"><i class="fa fa-check"></i><b>4.5.3</b> Most samples are not simple random samples</a></li>
<li class="chapter" data-level="4.5.4" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#how-much-does-it-matter-if-you-dont-have-a-simple-random-sample"><i class="fa fa-check"></i><b>4.5.4</b> How much does it matter if you don’t have a simple random sample?</a></li>
<li class="chapter" data-level="4.5.5" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#population-parameters-and-sample-statistics"><i class="fa fa-check"></i><b>4.5.5</b> Population parameters and sample statistics</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#lawlargenumbers"><i class="fa fa-check"></i><b>4.6</b> The law of large numbers</a></li>
<li class="chapter" data-level="4.7" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#samplesandclt"><i class="fa fa-check"></i><b>4.7</b> Sampling distributions and the central limit theorem</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#samplingdists"><i class="fa fa-check"></i><b>4.7.1</b> Sampling distribution of the mean</a></li>
<li class="chapter" data-level="4.7.2" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#sample-size-and-population-size"><i class="fa fa-check"></i><b>4.7.2</b> Sample size and population size</a></li>
<li class="chapter" data-level="4.7.3" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#sampling-error"><i class="fa fa-check"></i><b>4.7.3</b> Sampling error</a></li>
<li class="chapter" data-level="4.7.4" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#another-sampling-distribution"><i class="fa fa-check"></i><b>4.7.4</b> Another sampling distribution</a></li>
<li class="chapter" data-level="4.7.5" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#defining-the-central-limit-theorem"><i class="fa fa-check"></i><b>4.7.5</b> Defining the central limit theorem</a></li>
<li class="chapter" data-level="4.7.6" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#more-on-standard-error"><i class="fa fa-check"></i><b>4.7.6</b> More on standard error</a></li>
<li class="chapter" data-level="4.7.7" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#the-sampling-distribution-tells-us-about-the-probability-of-sample-means"><i class="fa fa-check"></i><b>4.7.7</b> The sampling distribution tells us about the probability of sample means</a></li>
<li class="chapter" data-level="4.7.8" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#sampling-distributions-exist-for-any-sample-statistic"><i class="fa fa-check"></i><b>4.7.8</b> Sampling distributions exist for any sample statistic!</a></li>
<li class="chapter" data-level="4.7.9" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#clt"><i class="fa fa-check"></i><b>4.7.9</b> The central limit theorem</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#pointestimates"><i class="fa fa-check"></i><b>4.8</b> Estimating population parameters</a>
<ul>
<li class="chapter" data-level="4.8.1" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#estimating-the-population-mean"><i class="fa fa-check"></i><b>4.8.1</b> Estimating the population mean</a></li>
<li class="chapter" data-level="4.8.2" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#estimating-the-population-standard-deviation"><i class="fa fa-check"></i><b>4.8.2</b> Estimating the population standard deviation</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#ci"><i class="fa fa-check"></i><b>4.9</b> Estimating a confidence interval</a>
<ul>
<li class="chapter" data-level="4.9.1" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#a-slight-mistake-in-the-formula"><i class="fa fa-check"></i><b>4.9.1</b> A slight mistake in the formula</a></li>
<li class="chapter" data-level="4.9.2" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#interpreting-a-confidence-interval"><i class="fa fa-check"></i><b>4.9.2</b> Interpreting a confidence interval</a></li>
<li class="chapter" data-level="4.9.3" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#calculating-confidence-intervals-in-r"><i class="fa fa-check"></i><b>4.9.3</b> Calculating confidence intervals in R</a></li>
<li class="chapter" data-level="4.9.4" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#ciplots"><i class="fa fa-check"></i><b>4.9.4</b> Plotting confidence intervals in R</a></li>
</ul></li>
<li class="chapter" data-level="4.10" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#summary-3"><i class="fa fa-check"></i><b>4.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="hypothesistesting.html"><a href="hypothesistesting.html"><i class="fa fa-check"></i><b>5</b> Hypothesis testing</a>
<ul>
<li class="chapter" data-level="5.1" data-path="hypothesistesting.html"><a href="hypothesistesting.html#videos-3"><i class="fa fa-check"></i><b>5.1</b> Videos</a></li>
<li class="chapter" data-level="5.2" data-path="hypothesistesting.html"><a href="hypothesistesting.html#introduction-3"><i class="fa fa-check"></i><b>5.2</b> Introduction</a></li>
<li class="chapter" data-level="5.3" data-path="hypothesistesting.html"><a href="hypothesistesting.html#hypotheses"><i class="fa fa-check"></i><b>5.3</b> A menagerie of hypotheses</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="hypothesistesting.html"><a href="hypothesistesting.html#research-hypotheses-versus-statistical-hypotheses"><i class="fa fa-check"></i><b>5.3.1</b> Research hypotheses versus statistical hypotheses</a></li>
<li class="chapter" data-level="5.3.2" data-path="hypothesistesting.html"><a href="hypothesistesting.html#null-hypotheses-and-alternative-hypotheses"><i class="fa fa-check"></i><b>5.3.2</b> Null hypotheses and alternative hypotheses</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="hypothesistesting.html"><a href="hypothesistesting.html#errortypes"><i class="fa fa-check"></i><b>5.4</b> Two types of errors</a></li>
<li class="chapter" data-level="5.5" data-path="hypothesistesting.html"><a href="hypothesistesting.html#teststatistics"><i class="fa fa-check"></i><b>5.5</b> Test statistics and sampling distributions</a></li>
<li class="chapter" data-level="5.6" data-path="hypothesistesting.html"><a href="hypothesistesting.html#decisionmaking"><i class="fa fa-check"></i><b>5.6</b> Making decisions</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="hypothesistesting.html"><a href="hypothesistesting.html#critical-regions-and-critical-values"><i class="fa fa-check"></i><b>5.6.1</b> Critical regions and critical values</a></li>
<li class="chapter" data-level="5.6.2" data-path="hypothesistesting.html"><a href="hypothesistesting.html#a-note-on-statistical-significance"><i class="fa fa-check"></i><b>5.6.2</b> A note on statistical “significance”</a></li>
<li class="chapter" data-level="5.6.3" data-path="hypothesistesting.html"><a href="hypothesistesting.html#onesidedtests"><i class="fa fa-check"></i><b>5.6.3</b> The difference between one sided and two sided tests</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="hypothesistesting.html"><a href="hypothesistesting.html#pvalue"><i class="fa fa-check"></i><b>5.7</b> The <span class="math inline">\(p\)</span> value of a test</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="hypothesistesting.html"><a href="hypothesistesting.html#a-softer-view-of-decision-making"><i class="fa fa-check"></i><b>5.7.1</b> A softer view of decision making</a></li>
<li class="chapter" data-level="5.7.2" data-path="hypothesistesting.html"><a href="hypothesistesting.html#the-probability-of-extreme-data"><i class="fa fa-check"></i><b>5.7.2</b> The probability of extreme data</a></li>
<li class="chapter" data-level="5.7.3" data-path="hypothesistesting.html"><a href="hypothesistesting.html#a-common-mistake"><i class="fa fa-check"></i><b>5.7.3</b> A common mistake</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="hypothesistesting.html"><a href="hypothesistesting.html#writeup"><i class="fa fa-check"></i><b>5.8</b> Reporting the results of a hypothesis test</a>
<ul>
<li class="chapter" data-level="5.8.1" data-path="hypothesistesting.html"><a href="hypothesistesting.html#the-issue"><i class="fa fa-check"></i><b>5.8.1</b> The issue</a></li>
<li class="chapter" data-level="5.8.2" data-path="hypothesistesting.html"><a href="hypothesistesting.html#two-proposed-solutions"><i class="fa fa-check"></i><b>5.8.2</b> Two proposed solutions</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="hypothesistesting.html"><a href="hypothesistesting.html#running-the-hypothesis-test-in-practice"><i class="fa fa-check"></i><b>5.9</b> Running the hypothesis test in practice</a></li>
<li class="chapter" data-level="5.10" data-path="hypothesistesting.html"><a href="hypothesistesting.html#effectsize"><i class="fa fa-check"></i><b>5.10</b> Effect size, sample size and power</a>
<ul>
<li class="chapter" data-level="5.10.1" data-path="hypothesistesting.html"><a href="hypothesistesting.html#the-power-function"><i class="fa fa-check"></i><b>5.10.1</b> The power function</a></li>
<li class="chapter" data-level="5.10.2" data-path="hypothesistesting.html"><a href="hypothesistesting.html#effect-size"><i class="fa fa-check"></i><b>5.10.2</b> Effect size</a></li>
<li class="chapter" data-level="5.10.3" data-path="hypothesistesting.html"><a href="hypothesistesting.html#increasing-the-power-of-your-study"><i class="fa fa-check"></i><b>5.10.3</b> Increasing the power of your study</a></li>
</ul></li>
<li class="chapter" data-level="5.11" data-path="hypothesistesting.html"><a href="hypothesistesting.html#nhstmess"><i class="fa fa-check"></i><b>5.11</b> Some issues to consider</a>
<ul>
<li class="chapter" data-level="5.11.1" data-path="hypothesistesting.html"><a href="hypothesistesting.html#neyman-versus-fisher"><i class="fa fa-check"></i><b>5.11.1</b> Neyman versus Fisher</a></li>
<li class="chapter" data-level="5.11.2" data-path="hypothesistesting.html"><a href="hypothesistesting.html#bayesians-versus-frequentists"><i class="fa fa-check"></i><b>5.11.2</b> Bayesians versus frequentists</a></li>
<li class="chapter" data-level="5.11.3" data-path="hypothesistesting.html"><a href="hypothesistesting.html#traps"><i class="fa fa-check"></i><b>5.11.3</b> Traps</a></li>
</ul></li>
<li class="chapter" data-level="5.12" data-path="hypothesistesting.html"><a href="hypothesistesting.html#summary-4"><i class="fa fa-check"></i><b>5.12</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="issues-in-hypothesis-testing.html"><a href="issues-in-hypothesis-testing.html"><i class="fa fa-check"></i><b>6</b> Issues in Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="6.1" data-path="issues-in-hypothesis-testing.html"><a href="issues-in-hypothesis-testing.html#videos-4"><i class="fa fa-check"></i><b>6.1</b> Videos</a></li>
<li class="chapter" data-level="6.2" data-path="issues-in-hypothesis-testing.html"><a href="issues-in-hypothesis-testing.html#introduction-4"><i class="fa fa-check"></i><b>6.2</b> Introduction</a></li>
<li class="chapter" data-level="6.3" data-path="issues-in-hypothesis-testing.html"><a href="issues-in-hypothesis-testing.html#the-researcher-affects-nhst-outcomes"><i class="fa fa-check"></i><b>6.3</b> The researcher affects NHST outcomes</a></li>
<li class="chapter" data-level="6.4" data-path="issues-in-hypothesis-testing.html"><a href="issues-in-hypothesis-testing.html#nhst-misunderstandings"><i class="fa fa-check"></i><b>6.4</b> NHST Misunderstandings</a></li>
<li class="chapter" data-level="6.5" data-path="issues-in-hypothesis-testing.html"><a href="issues-in-hypothesis-testing.html#nhst-issues"><i class="fa fa-check"></i><b>6.5</b> NHST Issues</a></li>
<li class="chapter" data-level="6.6" data-path="issues-in-hypothesis-testing.html"><a href="issues-in-hypothesis-testing.html#conclusions"><i class="fa fa-check"></i><b>6.6</b> Conclusions</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="data-cleaning-and-missing-values-analysis.html"><a href="data-cleaning-and-missing-values-analysis.html"><i class="fa fa-check"></i><b>7</b> Data Cleaning and Missing Values Analysis</a>
<ul>
<li class="chapter" data-level="7.1" data-path="data-cleaning-and-missing-values-analysis.html"><a href="data-cleaning-and-missing-values-analysis.html#videos-5"><i class="fa fa-check"></i><b>7.1</b> Videos</a></li>
<li class="chapter" data-level="7.2" data-path="data-cleaning-and-missing-values-analysis.html"><a href="data-cleaning-and-missing-values-analysis.html#introduction-dealing-with-the-unexpected"><i class="fa fa-check"></i><b>7.2</b> Introduction: Dealing with the Unexpected</a></li>
<li class="chapter" data-level="7.3" data-path="data-cleaning-and-missing-values-analysis.html"><a href="data-cleaning-and-missing-values-analysis.html#data-cleaning"><i class="fa fa-check"></i><b>7.3</b> Data Cleaning</a></li>
<li class="chapter" data-level="7.4" data-path="data-cleaning-and-missing-values-analysis.html"><a href="data-cleaning-and-missing-values-analysis.html#a-general-plan-for-data-cleaning"><i class="fa fa-check"></i><b>7.4</b> A General Plan for Data Cleaning</a></li>
<li class="chapter" data-level="7.5" data-path="data-cleaning-and-missing-values-analysis.html"><a href="data-cleaning-and-missing-values-analysis.html#step-0.-design-your-research-to-mimimize-data-problems"><i class="fa fa-check"></i><b>7.5</b> Step 0. Design your Research to Mimimize Data Problems</a></li>
<li class="chapter" data-level="7.6" data-path="data-cleaning-and-missing-values-analysis.html"><a href="data-cleaning-and-missing-values-analysis.html#step-1.-examine-your-data"><i class="fa fa-check"></i><b>7.6</b> Step 1. Examine Your Data</a>
<ul>
<li class="chapter" data-level="7.6.1" data-path="data-cleaning-and-missing-values-analysis.html"><a href="data-cleaning-and-missing-values-analysis.html#sort"><i class="fa fa-check"></i><b>7.6.1</b> Sorting, flipping and merging data</a></li>
<li class="chapter" data-level="7.6.2" data-path="data-cleaning-and-missing-values-analysis.html"><a href="data-cleaning-and-missing-values-analysis.html#binding-vectors-together"><i class="fa fa-check"></i><b>7.6.2</b> Binding vectors together</a></li>
<li class="chapter" data-level="7.6.3" data-path="data-cleaning-and-missing-values-analysis.html"><a href="data-cleaning-and-missing-values-analysis.html#reshape"><i class="fa fa-check"></i><b>7.6.3</b> Reshaping a data frame</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="data-cleaning-and-missing-values-analysis.html"><a href="data-cleaning-and-missing-values-analysis.html#step-2.-outlier-analysis"><i class="fa fa-check"></i><b>7.7</b> Step 2. Outlier Analysis</a></li>
<li class="chapter" data-level="7.8" data-path="data-cleaning-and-missing-values-analysis.html"><a href="data-cleaning-and-missing-values-analysis.html#step-3.-missing-values-analysis"><i class="fa fa-check"></i><b>7.8</b> Step 3. Missing values analysis</a>
<ul>
<li class="chapter" data-level="7.8.1" data-path="introR.html"><a href="introR.html#specials"><i class="fa fa-check"></i><b>7.8.1</b> Special values in R</a></li>
<li class="chapter" data-level="7.8.2" data-path="data-cleaning-and-missing-values-analysis.html"><a href="data-cleaning-and-missing-values-analysis.html#missing"><i class="fa fa-check"></i><b>7.8.2</b> Handling missing values in R</a></li>
<li class="chapter" data-level="7.8.3" data-path="data-cleaning-and-missing-values-analysis.html"><a href="data-cleaning-and-missing-values-analysis.html#why-values-are-missing-mcar-mar-and-mnar"><i class="fa fa-check"></i><b>7.8.3</b> Why values are missing: MCAR, MAR, and MNAR</a></li>
</ul></li>
<li class="chapter" data-level="7.9" data-path="data-cleaning-and-missing-values-analysis.html"><a href="data-cleaning-and-missing-values-analysis.html#step-4.-test-specific-assumption-checking"><i class="fa fa-check"></i><b>7.9</b> Step 4. Test-specific assumption checking</a></li>
<li class="chapter" data-level="7.10" data-path="data-cleaning-and-missing-values-analysis.html"><a href="data-cleaning-and-missing-values-analysis.html#communicate-results-of-data-cleaning-in-apa-style"><i class="fa fa-check"></i><b>7.10</b> Communicate results of data cleaning in APA style</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>8</b> Regression</a>
<ul>
<li class="chapter" data-level="8.1" data-path="regression.html"><a href="regression.html#videos-6"><i class="fa fa-check"></i><b>8.1</b> Videos</a></li>
<li class="chapter" data-level="8.2" data-path="regression.html"><a href="regression.html#introduction-5"><i class="fa fa-check"></i><b>8.2</b> Introduction</a></li>
<li class="chapter" data-level="8.3" data-path="regression.html"><a href="regression.html#the-general-linear-model-glm"><i class="fa fa-check"></i><b>8.3</b> The General Linear Model (GLM)</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="regression.html"><a href="regression.html#the-traditional-approach-two-kinds-of-parametric-statistical-tests"><i class="fa fa-check"></i><b>8.3.1</b> The Traditional approach: Two kinds of parametric statistical tests</a></li>
<li class="chapter" data-level="8.3.2" data-path="regression.html"><a href="regression.html#the-glm-approach"><i class="fa fa-check"></i><b>8.3.2</b> The GLM approach</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="regression.html"><a href="regression.html#correl"><i class="fa fa-check"></i><b>8.4</b> Correlations</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="regression.html"><a href="regression.html#the-data"><i class="fa fa-check"></i><b>8.4.1</b> The data</a></li>
<li class="chapter" data-level="8.4.2" data-path="regression.html"><a href="regression.html#the-strength-and-direction-of-a-relationship"><i class="fa fa-check"></i><b>8.4.2</b> The strength and direction of a relationship</a></li>
<li class="chapter" data-level="8.4.3" data-path="regression.html"><a href="regression.html#the-correlation-coefficient"><i class="fa fa-check"></i><b>8.4.3</b> The correlation coefficient</a></li>
<li class="chapter" data-level="8.4.4" data-path="regression.html"><a href="regression.html#calculating-correlations-in-r"><i class="fa fa-check"></i><b>8.4.4</b> Calculating correlations in R</a></li>
<li class="chapter" data-level="8.4.5" data-path="regression.html"><a href="regression.html#interpretingcorrelations"><i class="fa fa-check"></i><b>8.4.5</b> Interpreting a correlation</a></li>
<li class="chapter" data-level="8.4.6" data-path="regression.html"><a href="regression.html#spearmans-rank-correlations"><i class="fa fa-check"></i><b>8.4.6</b> Spearman’s rank correlations</a></li>
<li class="chapter" data-level="8.4.7" data-path="regression.html"><a href="regression.html#the-correlate-function"><i class="fa fa-check"></i><b>8.4.7</b> The <code>correlate()</code> function</a></li>
<li class="chapter" data-level="8.4.8" data-path="regression.html"><a href="regression.html#missing-values-in-pairwise-calculations-1"><i class="fa fa-check"></i><b>8.4.8</b> Missing values in pairwise calculations</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="regression.html"><a href="regression.html#introregression"><i class="fa fa-check"></i><b>8.5</b> Linear regression</a></li>
<li class="chapter" data-level="8.6" data-path="regression.html"><a href="regression.html#regressionestimation"><i class="fa fa-check"></i><b>8.6</b> Estimating a linear regression model</a>
<ul>
<li class="chapter" data-level="8.6.1" data-path="regression.html"><a href="regression.html#lm"><i class="fa fa-check"></i><b>8.6.1</b> Using the <code>lm()</code> function</a></li>
<li class="chapter" data-level="8.6.2" data-path="regression.html"><a href="regression.html#interpreting-the-estimated-model"><i class="fa fa-check"></i><b>8.6.2</b> Interpreting the estimated model</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="regression.html"><a href="regression.html#multipleregression"><i class="fa fa-check"></i><b>8.7</b> Multiple linear regression</a>
<ul>
<li class="chapter" data-level="8.7.1" data-path="regression.html"><a href="regression.html#doing-it-in-r"><i class="fa fa-check"></i><b>8.7.1</b> Doing it in R</a></li>
<li class="chapter" data-level="8.7.2" data-path="regression.html"><a href="regression.html#formula-for-the-general-case"><i class="fa fa-check"></i><b>8.7.2</b> Formula for the general case</a></li>
</ul></li>
<li class="chapter" data-level="8.8" data-path="regression.html"><a href="regression.html#r2"><i class="fa fa-check"></i><b>8.8</b> Quantifying the fit of the regression model</a>
<ul>
<li class="chapter" data-level="8.8.1" data-path="regression.html"><a href="regression.html#the-r2-value"><i class="fa fa-check"></i><b>8.8.1</b> The <span class="math inline">\(R^2\)</span> value</a></li>
<li class="chapter" data-level="8.8.2" data-path="regression.html"><a href="regression.html#the-relationship-between-regression-and-correlation"><i class="fa fa-check"></i><b>8.8.2</b> The relationship between regression and correlation</a></li>
<li class="chapter" data-level="8.8.3" data-path="regression.html"><a href="regression.html#the-adjusted-r2-value"><i class="fa fa-check"></i><b>8.8.3</b> The adjusted <span class="math inline">\(R^2\)</span> value</a></li>
</ul></li>
<li class="chapter" data-level="8.9" data-path="regression.html"><a href="regression.html#regressiontests"><i class="fa fa-check"></i><b>8.9</b> Hypothesis tests for regression models</a>
<ul>
<li class="chapter" data-level="8.9.1" data-path="regression.html"><a href="regression.html#testing-the-model-as-a-whole-the-omnibus-test"><i class="fa fa-check"></i><b>8.9.1</b> Testing the model as a whole: The omnibus test</a></li>
<li class="chapter" data-level="8.9.2" data-path="regression.html"><a href="regression.html#tests-for-individual-coefficients"><i class="fa fa-check"></i><b>8.9.2</b> Tests for individual coefficients</a></li>
<li class="chapter" data-level="8.9.3" data-path="regression.html"><a href="regression.html#regressionsummary"><i class="fa fa-check"></i><b>8.9.3</b> Running the hypothesis tests in R</a></li>
</ul></li>
<li class="chapter" data-level="8.10" data-path="regression.html"><a href="regression.html#corrhyp"><i class="fa fa-check"></i><b>8.10</b> Testing the significance of a correlation</a>
<ul>
<li class="chapter" data-level="8.10.1" data-path="regression.html"><a href="regression.html#hypothesis-tests-for-a-single-correlation"><i class="fa fa-check"></i><b>8.10.1</b> Hypothesis tests for a single correlation</a></li>
<li class="chapter" data-level="8.10.2" data-path="regression.html"><a href="regression.html#corrhyp2"><i class="fa fa-check"></i><b>8.10.2</b> Hypothesis tests for all pairwise correlations</a></li>
</ul></li>
<li class="chapter" data-level="8.11" data-path="regression.html"><a href="regression.html#regressioncoefs"><i class="fa fa-check"></i><b>8.11</b> Regarding regression coefficients</a>
<ul>
<li class="chapter" data-level="8.11.1" data-path="regression.html"><a href="regression.html#confidence-intervals-for-the-coefficients"><i class="fa fa-check"></i><b>8.11.1</b> Confidence intervals for the coefficients</a></li>
<li class="chapter" data-level="8.11.2" data-path="regression.html"><a href="regression.html#calculating-standardised-regression-coefficients"><i class="fa fa-check"></i><b>8.11.2</b> Calculating standardised regression coefficients</a></li>
</ul></li>
<li class="chapter" data-level="8.12" data-path="regression.html"><a href="regression.html#regressionassumptions"><i class="fa fa-check"></i><b>8.12</b> Assumptions of regression</a></li>
<li class="chapter" data-level="8.13" data-path="regression.html"><a href="regression.html#regressiondiagnostics"><i class="fa fa-check"></i><b>8.13</b> Model checking</a>
<ul>
<li class="chapter" data-level="8.13.1" data-path="regression.html"><a href="regression.html#three-kinds-of-residuals"><i class="fa fa-check"></i><b>8.13.1</b> Three kinds of residuals</a></li>
<li class="chapter" data-level="8.13.2" data-path="regression.html"><a href="regression.html#regressionoutliers"><i class="fa fa-check"></i><b>8.13.2</b> Three kinds of anomalous data</a></li>
<li class="chapter" data-level="8.13.3" data-path="regression.html"><a href="regression.html#regressionnormality"><i class="fa fa-check"></i><b>8.13.3</b> Checking the normality of the residuals</a></li>
<li class="chapter" data-level="8.13.4" data-path="regression.html"><a href="regression.html#regressionlinearity"><i class="fa fa-check"></i><b>8.13.4</b> Checking the linearity of the relationship</a></li>
<li class="chapter" data-level="8.13.5" data-path="regression.html"><a href="regression.html#regressionhomogeneity"><i class="fa fa-check"></i><b>8.13.5</b> Checking the homogeneity of variance</a></li>
<li class="chapter" data-level="8.13.6" data-path="regression.html"><a href="regression.html#regressioncollinearity"><i class="fa fa-check"></i><b>8.13.6</b> Checking for collinearity</a></li>
</ul></li>
<li class="chapter" data-level="8.14" data-path="regression.html"><a href="regression.html#modelselreg"><i class="fa fa-check"></i><b>8.14</b> Model selection</a>
<ul>
<li class="chapter" data-level="8.14.1" data-path="regression.html"><a href="regression.html#backward-elimination"><i class="fa fa-check"></i><b>8.14.1</b> Backward elimination</a></li>
<li class="chapter" data-level="8.14.2" data-path="regression.html"><a href="regression.html#forward-selection"><i class="fa fa-check"></i><b>8.14.2</b> Forward selection</a></li>
<li class="chapter" data-level="8.14.3" data-path="regression.html"><a href="regression.html#a-caveat"><i class="fa fa-check"></i><b>8.14.3</b> A caveat</a></li>
<li class="chapter" data-level="8.14.4" data-path="regression.html"><a href="regression.html#comparing-two-regression-models"><i class="fa fa-check"></i><b>8.14.4</b> Comparing two regression models</a></li>
</ul></li>
<li class="chapter" data-level="8.15" data-path="regression.html"><a href="regression.html#practical-issues-in-correlation-and-regression"><i class="fa fa-check"></i><b>8.15</b> Practical Issues in Correlation and Regression</a>
<ul>
<li class="chapter" data-level="8.15.1" data-path="regression.html"><a href="regression.html#correlation-is-not-causation"><i class="fa fa-check"></i><b>8.15.1</b> Correlation is not causation</a></li>
<li class="chapter" data-level="8.15.2" data-path="regression.html"><a href="regression.html#interpreting-nhst-in-big-data"><i class="fa fa-check"></i><b>8.15.2</b> Interpreting NHST in Big Data</a></li>
<li class="chapter" data-level="8.15.3" data-path="regression.html"><a href="regression.html#outliers"><i class="fa fa-check"></i><b>8.15.3</b> Outliers</a></li>
<li class="chapter" data-level="8.15.4" data-path="regression.html"><a href="regression.html#restriction-of-range"><i class="fa fa-check"></i><b>8.15.4</b> Restriction of Range</a></li>
<li class="chapter" data-level="8.15.5" data-path="regression.html"><a href="regression.html#regression-toward-the-mean"><i class="fa fa-check"></i><b>8.15.5</b> Regression Toward the Mean</a></li>
<li class="chapter" data-level="8.15.6" data-path="regression.html"><a href="regression.html#report-effect-size"><i class="fa fa-check"></i><b>8.15.6</b> Report Effect Size</a></li>
<li class="chapter" data-level="8.15.7" data-path="regression.html"><a href="regression.html#what-are-degrees-of-freedom-again"><i class="fa fa-check"></i><b>8.15.7</b> What are Degrees of Freedom, again?</a></li>
</ul></li>
<li class="chapter" data-level="8.16" data-path="regression.html"><a href="regression.html#summary-5"><i class="fa fa-check"></i><b>8.16</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="statistics-reference.html"><a href="statistics-reference.html"><i class="fa fa-check"></i><b>9</b> Statistics Reference</a>
<ul>
<li class="chapter" data-level="9.1" data-path="statistics-reference.html"><a href="statistics-reference.html#one-sample-z-test"><i class="fa fa-check"></i><b>9.1</b> One-Sample <em>z</em>-Test</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="statistics-reference.html"><a href="statistics-reference.html#definition"><i class="fa fa-check"></i><b>9.1.1</b> Definition</a></li>
<li class="chapter" data-level="9.1.2" data-path="statistics-reference.html"><a href="statistics-reference.html#test-statistic"><i class="fa fa-check"></i><b>9.1.2</b> Test Statistic</a></li>
<li class="chapter" data-level="9.1.3" data-path="statistics-reference.html"><a href="statistics-reference.html#assumptions-required-data"><i class="fa fa-check"></i><b>9.1.3</b> Assumptions &amp; Required Data</a></li>
<li class="chapter" data-level="9.1.4" data-path="statistics-reference.html"><a href="statistics-reference.html#when-to-use-it"><i class="fa fa-check"></i><b>9.1.4</b> When to use it</a></li>
<li class="chapter" data-level="9.1.5" data-path="statistics-reference.html"><a href="statistics-reference.html#example"><i class="fa fa-check"></i><b>9.1.5</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="statistics-reference.html"><a href="statistics-reference.html#correlation"><i class="fa fa-check"></i><b>9.2</b> Correlation</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="statistics-reference.html"><a href="statistics-reference.html#definition-1"><i class="fa fa-check"></i><b>9.2.1</b> Definition</a></li>
<li class="chapter" data-level="9.2.2" data-path="statistics-reference.html"><a href="statistics-reference.html#test-statistic-1"><i class="fa fa-check"></i><b>9.2.2</b> Test Statistic</a></li>
<li class="chapter" data-level="9.2.3" data-path="statistics-reference.html"><a href="statistics-reference.html#assumptions-required-data-1"><i class="fa fa-check"></i><b>9.2.3</b> Assumptions &amp; Required Data</a></li>
<li class="chapter" data-level="9.2.4" data-path="statistics-reference.html"><a href="statistics-reference.html#when-to-use-it-1"><i class="fa fa-check"></i><b>9.2.4</b> When to use it</a></li>
<li class="chapter" data-level="9.2.5" data-path="statistics-reference.html"><a href="statistics-reference.html#example-1"><i class="fa fa-check"></i><b>9.2.5</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="statistics-reference.html"><a href="statistics-reference.html#linear-regression"><i class="fa fa-check"></i><b>9.3</b> Linear Regression</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="statistics-reference.html"><a href="statistics-reference.html#definition-2"><i class="fa fa-check"></i><b>9.3.1</b> Definition</a></li>
<li class="chapter" data-level="9.3.2" data-path="statistics-reference.html"><a href="statistics-reference.html#test-statistic-2"><i class="fa fa-check"></i><b>9.3.2</b> Test Statistic</a></li>
<li class="chapter" data-level="9.3.3" data-path="statistics-reference.html"><a href="statistics-reference.html#assumptions-required-data-2"><i class="fa fa-check"></i><b>9.3.3</b> Assumptions &amp; Required Data</a></li>
<li class="chapter" data-level="9.3.4" data-path="statistics-reference.html"><a href="statistics-reference.html#when-to-use-it-2"><i class="fa fa-check"></i><b>9.3.4</b> When to use it</a></li>
<li class="chapter" data-level="9.3.5" data-path="statistics-reference.html"><a href="statistics-reference.html#example-2"><i class="fa fa-check"></i><b>9.3.5</b> Example</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Advanced Statistics Remix</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="hypothesistesting" class="section level1" number="5">
<h1><span class="header-section-number">Chapter 5</span> Hypothesis testing</h1>
<p>Text by <span class="citation"><a href="#ref-Navarro2018" role="doc-biblioref">Navarro</a> (<a href="#ref-Navarro2018" role="doc-biblioref">2018</a>)</span></p>
<div id="videos-3" class="section level2" number="5.1">
<h2><span class="header-section-number">5.1</span> Videos</h2>
<ul>
<li><p><a href="https://youtu.be/-Cu0E2diTBE">Video: Statistical power and errors</a></p></li>
<li><p><a href="https://youtu.be/TE61rB6ajTY">Video: The one-sample Z-test</a></p></li>
</ul>
</div>
<div id="introduction-3" class="section level2" number="5.2">
<h2><span class="header-section-number">5.2</span> Introduction</h2>
<blockquote>
<p><em>The process of induction is the process of assuming the simplest law that can be made to harmonize with our experience. This process, however, has no logical foundation but only a psychological one. It is clear that there are no grounds for believing that the simplest course of events will really happen. It is an hypothesis that the sun will rise tomorrow: and this means that we do not know whether it will rise.</em></p>
<p>– Ludwig Wittgenstein<a href="#fn90" class="footnote-ref" id="fnref90"><sup>90</sup></a></p>
</blockquote>
<p>In the last chapter, I discussed the ideas behind estimation, which is one of the two “big ideas” in inferential statistics. It’s now time to turn out attention to the other big idea, which is <em>hypothesis testing</em>. In its most abstract form, hypothesis testing really a very simple idea: the researcher has some theory about the world, and wants to determine whether or not the data actually support that theory. However, the details are messy, and most people find the theory of hypothesis testing to be the most frustrating part of statistics. The structure of the chapter is as follows. Firstly, I’ll describe how hypothesis testing works, in a fair amount of detail, using a simple running example to show you how a hypothesis test is “built.” I’ll try to avoid being too dogmatic while doing so, and focus instead on the underlying logic of the testing procedure.<a href="#fn91" class="footnote-ref" id="fnref91"><sup>91</sup></a> Afterwards, I’ll spend a bit of time talking about the various dogmas, rules and heresies that surround the theory of hypothesis testing.</p>
</div>
<div id="hypotheses" class="section level2" number="5.3">
<h2><span class="header-section-number">5.3</span> A menagerie of hypotheses</h2>
<p>Eventually we all succumb. For me, that day will arrive once I’m finally promoted to full professor. Safely ensconced in my ivory tower, happily protected by tenure, I will finally be able to take leave of my senses (so to speak), and indulge in that most thoroughly unproductive line of psychological research: the search for extrasensory perception (ESP).<a href="#fn92" class="footnote-ref" id="fnref92"><sup>92</sup></a></p>
<p>Let’s suppose that this glorious day has come. My first study is a simple one, in which I seek to test whether clairvoyance exists. Each participant sits down at a table, and is shown a card by an experimenter. The card is black on one side and white on the other. The experimenter takes the card away, and places it on a table in an adjacent room. The card is placed black side up or white side up completely at random, with the randomisation occurring only after the experimenter has left the room with the participant. A second experimenter comes in and asks the participant which side of the card is now facing upwards. It’s purely a one-shot experiment. Each person sees only one card, and gives only one answer; and at no stage is the participant actually in contact with someone who knows the right answer. My data set, therefore, is very simple. I have asked the question of <span class="math inline">\(N\)</span> people, and some number <span class="math inline">\(X\)</span> of these people have given the correct response. To make things concrete, let’s suppose that I have tested <span class="math inline">\(N = 100\)</span> people, and <span class="math inline">\(X = 62\)</span> of these got the answer right… a surprisingly large number, sure, but is it large enough for me to feel safe in claiming I’ve found evidence for ESP? This is the situation where hypothesis testing comes in useful. However, before we talk about how to <em>test</em> hypotheses, we need to be clear about what we mean by hypotheses.</p>
<div id="research-hypotheses-versus-statistical-hypotheses" class="section level3" number="5.3.1">
<h3><span class="header-section-number">5.3.1</span> Research hypotheses versus statistical hypotheses</h3>
<p>The first distinction that you need to keep clear in your mind is between research hypotheses and statistical hypotheses. In my ESP study, my overall scientific goal is to demonstrate that clairvoyance exists. In this situation, I have a clear research goal: I am hoping to discover evidence for ESP. In other situations I might actually be a lot more neutral than that, so I might say that my research goal is to determine whether or not clairvoyance exists. Regardless of how I want to portray myself, the basic point that I’m trying to convey here is that a research hypothesis involves making a substantive, testable scientific claim… if you are a psychologist, then your research hypotheses are fundamentally <em>about</em> psychological constructs. Any of the following would count as <strong><em>research hypotheses</em></strong>:</p>
<ul>
<li><em>Listening to music reduces your ability to pay attention to other things.</em> This is a claim about the causal relationship between two psychologically meaningful concepts (listening to music and paying attention to things), so it’s a perfectly reasonable research hypothesis.</li>
<li><em>Intelligence is related to personality</em>. Like the last one, this is a relational claim about two psychological constructs (intelligence and personality), but the claim is weaker: it is not causal.</li>
<li><em>Intelligence </em>is* speed of information processing<em>. This hypothesis has a quite different character: it’s not actually a relational claim at all. It’s an ontological claim about the fundamental character of intelligence (and I’m pretty sure it’s wrong). It’s worth expanding on this one actually: It’s usually easier to think about how to construct experiments to test research hypotheses of the form “does X affect Y?” than it is to address claims like “what is X?” And in practice, what usually happens is that you find ways of testing relational claims that follow from your ontological ones. For instance, if I believe that intelligence </em>is* speed of information processing in the brain, my experiments will often involve looking for relationships between measures of intelligence and measures of speed. As a consequence, most everyday research questions do tend to be relational in nature, but they’re almost always motivated by deeper ontological questions about the state of nature.</li>
</ul>
<p>Notice that in practice, my research hypotheses could overlap a lot. My ultimate goal in the ESP experiment might be to test an ontological claim like “ESP exists,” but I might operationally restrict myself to a narrower hypothesis like “Some people can `see’ objects in a clairvoyant fashion.” That said, there are some things that really don’t count as proper research hypotheses in any meaningful sense:</p>
<ul>
<li><em>Love is a battlefield</em>. This is too vague to be testable. While it’s okay for a research hypothesis to have a degree of vagueness to it, it has to be possible to operationalise your constructs. Maybe I’m just not creative enough to see it, but I can’t see how this can be converted into any concrete research design. If that’s true, then this isn’t a scientific research hypothesis, it’s a pop song. That doesn’t mean it’s not interesting – a lot of deep questions that humans have fall into this category. Maybe one day science will be able to construct testable theories of love, or to test to see if God exists, and so on; but right now we can’t, and I wouldn’t bet on ever seeing a satisfying scientific approach to either.</li>
<li><em>The first rule of tautology club is the first rule of tautology club</em>. This is not a substantive claim of any kind. It’s true by definition. No conceivable state of nature could possibly be inconsistent with this claim. As such, we say that this is an unfalsifiable hypothesis, and as such it is outside the domain of science. Whatever else you do in science, your claims must have the possibility of being wrong.</li>
<li><em>More people in my experiment will say “yes” than “no”</em>. This one fails as a research hypothesis because it’s a claim about the data set, not about the psychology (unless of course your actual research question is whether people have some kind of “yes” bias!). As we’ll see shortly, this hypothesis is starting to sound more like a statistical hypothesis than a research hypothesis.</li>
</ul>
<p>As you can see, research hypotheses can be somewhat messy at times; and ultimately they are <em>scientific</em> claims. <strong><em>Statistical hypotheses</em></strong> are neither of these two things. Statistical hypotheses must be mathematically precise, and they must correspond to specific claims about the characteristics of the data generating mechanism (i.e., the “population”). Even so, the intent is that statistical hypotheses bear a clear relationship to the substantive research hypotheses that you care about! For instance, in my ESP study my research hypothesis is that some people are able to see through walls or whatever. What I want to do is to “map” this onto a statement about how the data were generated. So let’s think about what that statement would be. The quantity that I’m interested in within the experiment is <span class="math inline">\(P(\mbox{&quot;correct&quot;})\)</span>, the true-but-unknown probability with which the participants in my experiment answer the question correctly. Let’s use the Greek letter <span class="math inline">\(\theta\)</span> (theta) to refer to this probability. Here are four different statistical hypotheses:</p>
<ul>
<li>If ESP doesn’t exist and if my experiment is well designed, then my participants are just guessing. So I should expect them to get it right half of the time and so my statistical hypothesis is that the true probability of choosing correctly is <span class="math inline">\(\theta = 0.5\)</span>.</li>
<li>Alternatively, suppose ESP does exist and participants can see the card. If that’s true, people will perform better than chance. The statistical hypotheis would be that <span class="math inline">\(\theta &gt; 0.5\)</span>.</li>
<li>A third possibility is that ESP does exist, but the colours are all reversed and people don’t realise it (okay, that’s wacky, but you never know…). If that’s how it works then you’d expect people’s performance to be <em>below</em> chance. This would correspond to a statistical hypothesis that <span class="math inline">\(\theta &lt; 0.5\)</span>.</li>
<li>Finally, suppose ESP exists, but I have no idea whether people are seeing the right colour or the wrong one. In that case, the only claim I could make about the data would be that the probability of making the correct answer is <em>not</em> equal to 50. This corresponds to the statistical hypothesis that <span class="math inline">\(\theta \neq 0.5\)</span>.</li>
</ul>
<p>All of these are legitimate examples of a statistical hypothesis because they are statements about a population parameter and are meaningfully related to my experiment.</p>
<p>What this discussion makes clear, I hope, is that when attempting to construct a statistical hypothesis test the researcher actually has two quite distinct hypotheses to consider. First, he or she has a research hypothesis (a claim about psychology), and this corresponds to a statistical hypothesis (a claim about the data generating population). In my ESP example, these might be</p>
<table>
<thead>
<tr class="header">
<th align="left">Dan.s.research.hypothesis</th>
<th align="left">Dan.s.statistical.hypothesis</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">ESP.exists</td>
<td align="left"><span class="math inline">\(\theta \neq 0.5\)</span></td>
</tr>
</tbody>
</table>
<p>And the key thing to recognise is this: <em>a statistical hypothesis test is a test of the statistical hypothesis, not the research hypothesis</em>. If your study is badly designed, then the link between your research hypothesis and your statistical hypothesis is broken. To give a silly example, suppose that my ESP study was conducted in a situation where the participant can actually see the card reflected in a window; if that happens, I would be able to find very strong evidence that <span class="math inline">\(\theta \neq 0.5\)</span>, but this would tell us nothing about whether “ESP exists.”</p>
</div>
<div id="null-hypotheses-and-alternative-hypotheses" class="section level3" number="5.3.2">
<h3><span class="header-section-number">5.3.2</span> Null hypotheses and alternative hypotheses</h3>
<p>So far, so good. I have a research hypothesis that corresponds to what I want to believe about the world, and I can map it onto a statistical hypothesis that corresponds to what I want to believe about how the data were generated. It’s at this point that things get somewhat counterintuitive for a lot of people. Because what I’m about to do is invent a new statistical hypothesis (the “null” hypothesis, <span class="math inline">\(H_0\)</span>) that corresponds to the exact opposite of what I want to believe, and then focus exclusively on that, almost to the neglect of the thing I’m actually interested in (which is now called the “alternative” hypothesis, <span class="math inline">\(H_1\)</span>). In our ESP example, the null hypothesis is that <span class="math inline">\(\theta = 0.5\)</span>, since that’s what we’d expect if ESP <em>didn’t</em> exist. My hope, of course, is that ESP is totally real, and so the <em>alternative</em> to this null hypothesis is <span class="math inline">\(\theta \neq 0.5\)</span>. In essence, what we’re doing here is dividing up the possible values of <span class="math inline">\(\theta\)</span> into two groups: those values that I really hope aren’t true (the null), and those values that I’d be happy with if they turn out to be right (the alternative). Having done so, the important thing to recognise is that the goal of a hypothesis test is <em>not</em> to show that the alternative hypothesis is (probably) true; the goal is to show that the null hypothesis is (probably) false. Most people find this pretty weird.</p>
<p>The best way to think about it, in my experience, is to imagine that a hypothesis test is a criminal trial<a href="#fn93" class="footnote-ref" id="fnref93"><sup>93</sup></a>… <em>the trial of the null hypothesis</em>. The null hypothesis is the defendant, the researcher is the prosecutor, and the statistical test itself is the judge. Just like a criminal trial, there is a presumption of innocence: the null hypothesis is <em>deemed</em> to be true unless you, the researcher, can prove beyond a reasonable doubt that it is false. You are free to design your experiment however you like (within reason, obviously!), and your goal when doing so is to maximise the chance that the data will yield a conviction… for the crime of being false. The catch is that the statistical test sets the rules of the trial, and those rules are designed to protect the null hypothesis – specifically to ensure that if the null hypothesis is actually true, the chances of a false conviction are guaranteed to be low. This is pretty important: after all, the null hypothesis doesn’t get a lawyer. And given that the researcher is trying desperately to prove it to be false, <em>someone</em> has to protect it.</p>
</div>
</div>
<div id="errortypes" class="section level2" number="5.4">
<h2><span class="header-section-number">5.4</span> Two types of errors</h2>
<p>Before going into details about how a statistical test is constructed, it’s useful to understand the philosophy behind it. I hinted at it when pointing out the similarity between a null hypothesis test and a criminal trial, but I should now be explicit. Ideally, we would like to construct our test so that we never make any errors. Unfortunately, since the world is messy, this is never possible. Sometimes you’re just really unlucky: for instance, suppose you flip a coin 10 times in a row and it comes up heads all 10 times. That feels like very strong evidence that the coin is biased (and it is!), but of course there’s a 1 in 1024 chance that this would happen even if the coin was totally fair. In other words, in real life we <em>always</em> have to accept that there’s a chance that we did the wrong thing. As a consequence, the goal behind statistical hypothesis testing is not to <em>eliminate</em> errors, but to <em>minimise</em> them.</p>
<p>At this point, we need to be a bit more precise about what we mean by “errors.” Firstly, let’s state the obvious: it is either the case that the null hypothesis is true, or it is false; and our test will either reject the null hypothesis or retain it.<a href="#fn94" class="footnote-ref" id="fnref94"><sup>94</sup></a> So, as the table below illustrates, after we run the test and make our choice, one of four things might have happened:</p>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="left">retain <span class="math inline">\(H_0\)</span></th>
<th align="left">reject <span class="math inline">\(H_0\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(H_0\)</span> is true</td>
<td align="left">correct decision</td>
<td align="left">error (type I)</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(H_0\)</span> is false</td>
<td align="left">error (type II)</td>
<td align="left">correct decision</td>
</tr>
</tbody>
</table>
<p>As a consequence there are actually <em>two</em> different types of error here. If we reject a null hypothesis that is actually true, then we have made a <strong><em>type I error</em></strong>. On the other hand, if we retain the null hypothesis when it is in fact false, then we have made a <strong><em>type II error</em></strong>.</p>
<p>Remember how I said that statistical testing was kind of like a criminal trial? Well, I meant it. A criminal trial requires that you establish “beyond a reasonable doubt” that the defendant did it. All of the evidentiary rules are (in theory, at least) designed to ensure that there’s (almost) no chance of wrongfully convicting an innocent defendant. The trial is designed to protect the rights of a defendant: as the English jurist William Blackstone famously said, it is “better that ten guilty persons escape than that one innocent suffer.” In other words, a criminal trial doesn’t treat the two types of error in the same way~… punishing the innocent is deemed to be much worse than letting the guilty go free. A statistical test is pretty much the same: the single most important design principle of the test is to <em>control</em> the probability of a type I error, to keep it below some fixed probability. This probability, which is denoted <span class="math inline">\(\alpha\)</span>, is called the <strong><em>significance level</em></strong> of the test (or sometimes, the <em>size</em> of the test). And I’ll say it again, because it is so central to the whole set-up~… a hypothesis test is said to have significance level <span class="math inline">\(\alpha\)</span> if the type I error rate is no larger than <span class="math inline">\(\alpha\)</span>.</p>
<p>So, what about the type II error rate? Well, we’d also like to keep those under control too, and we denote this probability by <span class="math inline">\(\beta\)</span>. However, it’s much more common to refer to the <strong><em>power</em></strong> of the test, which is the probability with which we reject a null hypothesis when it really is false, which is <span class="math inline">\(1-\beta\)</span>. To help keep this straight, here’s the same table again, but with the relevant numbers added:</p>
<table>
<colgroup>
<col width="16%" />
<col width="50%" />
<col width="33%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"></th>
<th align="left">retain <span class="math inline">\(H_0\)</span></th>
<th align="left">reject <span class="math inline">\(H_0\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(H_0\)</span> is true</td>
<td align="left"><span class="math inline">\(1-\alpha\)</span> (probability of correct retention)</td>
<td align="left"><span class="math inline">\(\alpha\)</span> (type I error rate)</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(H_0\)</span> is false</td>
<td align="left"><span class="math inline">\(\beta\)</span> (type II error rate)</td>
<td align="left"><span class="math inline">\(1-\beta\)</span> (power of the test)</td>
</tr>
</tbody>
</table>
<p>A “powerful” hypothesis test is one that has a small value of <span class="math inline">\(\beta\)</span>, while still keeping <span class="math inline">\(\alpha\)</span> fixed at some (small) desired level. By convention, scientists make use of three different <span class="math inline">\(\alpha\)</span> levels: <span class="math inline">\(.05\)</span>, <span class="math inline">\(.01\)</span> and <span class="math inline">\(.001\)</span>. Notice the asymmetry here~… the tests are designed to <em>ensure</em> that the <span class="math inline">\(\alpha\)</span> level is kept small, but there’s no corresponding guarantee regarding <span class="math inline">\(\beta\)</span>. We’d certainly <em>like</em> the type II error rate to be small, and we try to design tests that keep it small, but this is very much secondary to the overwhelming need to control the type I error rate. As Blackstone might have said if he were a statistician, it is “better to retain 10 false null hypotheses than to reject a single true one.” To be honest, I don’t know that I agree with this philosophy – there are situations where I think it makes sense, and situations where I think it doesn’t – but that’s neither here nor there. It’s how the tests are built.</p>
</div>
<div id="teststatistics" class="section level2" number="5.5">
<h2><span class="header-section-number">5.5</span> Test statistics and sampling distributions</h2>
<p>At this point we need to start talking specifics about how a hypothesis test is constructed. To that end, let’s return to the ESP example. Let’s ignore the actual data that we obtained, for the moment, and think about the structure of the experiment. Regardless of what the actual numbers are, the <em>form</em> of the data is that <span class="math inline">\(X\)</span> out of <span class="math inline">\(N\)</span> people correctly identified the colour of the hidden card. Moreover, let’s suppose for the moment that the null hypothesis really is true: ESP doesn’t exist, and the true probability that anyone picks the correct colour is exactly <span class="math inline">\(\theta = 0.5\)</span>. What would we <em>expect</em> the data to look like? Well, obviously, we’d expect the proportion of people who make the correct response to be pretty close to 50%. Or, to phrase this in more mathematical terms, we’d say that <span class="math inline">\(X/N\)</span> is approximately <span class="math inline">\(0.5\)</span>. Of course, we wouldn’t expect this fraction to be <em>exactly</em> 0.5: if, for example we tested <span class="math inline">\(N=100\)</span> people, and <span class="math inline">\(X = 53\)</span> of them got the question right, we’d probably be forced to concede that the data are quite consistent with the null hypothesis. On the other hand, if <span class="math inline">\(X = 99\)</span> of our participants got the question right, then we’d feel pretty confident that the null hypothesis is wrong. Similarly, if only <span class="math inline">\(X=3\)</span> people got the answer right, we’d be similarly confident that the null was wrong. Let’s be a little more technical about this: we have a quantity <span class="math inline">\(X\)</span> that we can calculate by looking at our data; after looking at the value of <span class="math inline">\(X\)</span>, we make a decision about whether to believe that the null hypothesis is correct, or to reject the null hypothesis in favour of the alternative. The name for this thing that we calculate to guide our choices is a <strong><em>test statistic</em></strong>.</p>
<p>Having chosen a test statistic, the next step is to state precisely which values of the test statistic would cause is to reject the null hypothesis, and which values would cause us to keep it. In order to do so, we need to determine what the <strong><em>sampling distribution of the test statistic</em></strong> would be if the null hypothesis were actually true (we talked about sampling distributions earlier in Section <a href="inferential-statistics-the-central-limit-theorem.html#samplingdists">4.7.1</a>). Why do we need this? Because this distribution tells us exactly what values of <span class="math inline">\(X\)</span> our null hypothesis would lead us to expect. And therefore, we can use this distribution as a tool for assessing how closely the null hypothesis agrees with our data.</p>
<div class="figure"><span style="display:block;" id="fig:samplingdist"></span>
<img src="schuster-statistics-remix_files/figure-html/samplingdist-1.png" alt="The sampling distribution for our test statistic $X$ when the null hypothesis is true. For our ESP scenario, this is a binomial distribution. Not surprisingly, since the null hypothesis says that the probability of a correct response is $\theta = .5$, the sampling distribution says that the most likely value is 50 (our of 100) correct responses. Most of the probability mass lies between 40 and 60." width="672" />
<p class="caption">
Figure 5.1: The sampling distribution for our test statistic <span class="math inline">\(X\)</span> when the null hypothesis is true. For our ESP scenario, this is a binomial distribution. Not surprisingly, since the null hypothesis says that the probability of a correct response is <span class="math inline">\(\theta = .5\)</span>, the sampling distribution says that the most likely value is 50 (our of 100) correct responses. Most of the probability mass lies between 40 and 60.
</p>
</div>
<p>How do we actually determine the sampling distribution of the test statistic? For a lot of hypothesis tests this step is actually quite complicated, and later on in the book you’ll see me being slightly evasive about it for some of the tests (some of them I don’t even understand myself). However, sometimes it’s very easy. And, fortunately for us, our ESP example provides us with one of the easiest cases. Our population parameter <span class="math inline">\(\theta\)</span> is just the overall probability that people respond correctly when asked the question, and our test statistic <span class="math inline">\(X\)</span> is the <em>count</em> of the number of people who did so, out of a sample size of <span class="math inline">\(N\)</span>. There is a distribution that describes exactly that, called the binomial distribution. So, to use the notation and terminology of the binomial distribution, we would say that the null hypothesis predicts that <span class="math inline">\(X\)</span> is binomially distributed, which is written
<span class="math display">\[
X \sim \mbox{Binomial}(\theta,N)
\]</span>
Since the null hypothesis states that <span class="math inline">\(\theta = 0.5\)</span> and our experiment has <span class="math inline">\(N=100\)</span> people, we have the sampling distribution we need. This sampling distribution is plotted in Figure <a href="hypothesistesting.html#fig:samplingdist">5.1</a>. No surprises really: the null hypothesis says that <span class="math inline">\(X=50\)</span> is the most likely outcome, and it says that we’re almost certain to see somewhere between 40 and 60 correct responses.</p>
</div>
<div id="decisionmaking" class="section level2" number="5.6">
<h2><span class="header-section-number">5.6</span> Making decisions</h2>
<p>Okay, we’re very close to being finished. We’ve constructed a test statistic (<span class="math inline">\(X\)</span>), and we chose this test statistic in such a way that we’re pretty confident that if <span class="math inline">\(X\)</span> is close to <span class="math inline">\(N/2\)</span> then we should retain the null, and if not we should reject it. The question that remains is this: exactly which values of the test statistic should we associate with the null hypothesis, and which exactly values go with the alternative hypothesis? In my ESP study, for example, I’ve observed a value of <span class="math inline">\(X=62\)</span>. What decision should I make? Should I choose to believe the null hypothesis, or the alternative hypothesis?</p>
<div id="critical-regions-and-critical-values" class="section level3" number="5.6.1">
<h3><span class="header-section-number">5.6.1</span> Critical regions and critical values</h3>
<p>To answer this question, we need to introduce the concept of a <strong><em>critical region</em></strong> for the test statistic <span class="math inline">\(X\)</span>. The critical region of the test corresponds to those values of <span class="math inline">\(X\)</span> that would lead us to reject null hypothesis (which is why the critical region is also sometimes called the rejection region). How do we find this critical region? Well, let’s consider what we know:</p>
<ul>
<li><span class="math inline">\(X\)</span> should be very big or very small in order to reject the null hypothesis.</li>
<li>If the null hypothesis is true, the sampling distribution of <span class="math inline">\(X\)</span> is Binomial<span class="math inline">\((0.5, N)\)</span>.</li>
<li>If <span class="math inline">\(\alpha =.05\)</span>, the critical region must cover 5% of this sampling distribution.</li>
</ul>
<p>It’s important to make sure you understand this last point: the critical region corresponds to those values of <span class="math inline">\(X\)</span> for which we would reject the null hypothesis, and the sampling distribution in question describes the probability that we would obtain a particular value of <span class="math inline">\(X\)</span> if the null hypothesis were actually true. Now, let’s suppose that we chose a critical region that covers 20% of the sampling distribution, and suppose that the null hypothesis is actually true. What would be the probability of incorrectly rejecting the null? The answer is of course 20%. And therefore, we would have built a test that had an <span class="math inline">\(\alpha\)</span> level of <span class="math inline">\(0.2\)</span>. If we want <span class="math inline">\(\alpha = .05\)</span>, the critical region is only <em>allowed</em> to cover 5% of the sampling distribution of our test statistic.</p>
<div class="figure"><span style="display:block;" id="fig:crit2"></span>
<img src="schuster-statistics-remix_files/figure-html/crit2-1.png" alt="The critical region associated with the hypothesis test for the ESP study, for a hypothesis test with a significance level of $\alpha = .05$. The plot itself shows the sampling distribution of $X$ under the null hypothesis: the grey bars correspond to those values of $X$ for which we would retain the null hypothesis. The black bars show the critical region: those values of $X$ for which we would reject the null. Because the alternative hypothesis is two sided (i.e., allows both $\theta &lt;.5$ and $\theta &gt;.5$), the critical region covers both tails of the distribution. To ensure an $\alpha$ level of $.05$, we need to ensure that each of the two regions encompasses 2.5% of the sampling distribution." width="672" />
<p class="caption">
Figure 5.2: The critical region associated with the hypothesis test for the ESP study, for a hypothesis test with a significance level of <span class="math inline">\(\alpha = .05\)</span>. The plot itself shows the sampling distribution of <span class="math inline">\(X\)</span> under the null hypothesis: the grey bars correspond to those values of <span class="math inline">\(X\)</span> for which we would retain the null hypothesis. The black bars show the critical region: those values of <span class="math inline">\(X\)</span> for which we would reject the null. Because the alternative hypothesis is two sided (i.e., allows both <span class="math inline">\(\theta &lt;.5\)</span> and <span class="math inline">\(\theta &gt;.5\)</span>), the critical region covers both tails of the distribution. To ensure an <span class="math inline">\(\alpha\)</span> level of <span class="math inline">\(.05\)</span>, we need to ensure that each of the two regions encompasses 2.5% of the sampling distribution.
</p>
</div>
<p>As it turns out, those three things uniquely solve the problem: our critical region consists of the most <em>extreme values</em>, known as the <strong><em>tails</em></strong> of the distribution. This is illustrated in Figure <a href="hypothesistesting.html#fig:crit2">5.2</a>. As it turns out, if we want <span class="math inline">\(\alpha = .05\)</span>, then our critical regions correspond to <span class="math inline">\(X \leq 40\)</span> and <span class="math inline">\(X \geq 60\)</span>.<a href="#fn95" class="footnote-ref" id="fnref95"><sup>95</sup></a> That is, if the number of people saying “true” is between 41 and 59, then we should retain the null hypothesis. If the number is between 0 to 40 or between 60 to 100, then we should reject the null hypothesis. The numbers 40 and 60 are often referred to as the <strong><em>critical values</em></strong>, since they define the edges of the critical region.</p>
<p>At this point, our hypothesis test is essentially complete: (1) we choose an <span class="math inline">\(\alpha\)</span> level (e.g., <span class="math inline">\(\alpha = .05\)</span>, (2) come up with some test statistic (e.g., <span class="math inline">\(X\)</span>) that does a good job (in some meaningful sense) of comparing <span class="math inline">\(H_0\)</span> to <span class="math inline">\(H_1\)</span>, (3) figure out the sampling distribution of the test statistic on the assumption that the null hypothesis is true (in this case, binomial) and then (4) calculate the critical region that produces an appropriate <span class="math inline">\(\alpha\)</span> level (0-40 and 60-100). All that we have to do now is calculate the value of the test statistic for the real data (e.g., <span class="math inline">\(X = 62\)</span>) and then compare it to the critical values to make our decision. Since 62 is greater than the critical value of 60, we would reject the null hypothesis. Or, to phrase it slightly differently, we say that the test has produced a <strong><em>significant</em></strong> result.</p>
</div>
<div id="a-note-on-statistical-significance" class="section level3" number="5.6.2">
<h3><span class="header-section-number">5.6.2</span> A note on statistical “significance”</h3>
<blockquote>
<p><em>Like other occult techniques of divination, the statistical method has a private jargon deliberately contrived to obscure its methods from non-practitioners.</em></p>
<p>– Attributed to G. O. Ashley<a href="#fn96" class="footnote-ref" id="fnref96"><sup>96</sup></a></p>
</blockquote>
<p>A very brief digression is in order at this point, regarding the word “significant.” The concept of statistical significance is actually a very simple one, but has a very unfortunate name. If the data allow us to reject the null hypothesis, we say that “the result is <em>statistically significant</em>,” which is often shortened to “the result is significant.” This terminology is rather old, and dates back to a time when “significant” just meant something like “indicated,” rather than its modern meaning, which is much closer to “important.” As a result, a lot of modern readers get very confused when they start learning statistics, because they think that a “significant result” must be an important one. It doesn’t mean that at all. All that “statistically significant” means is that the data allowed us to reject a null hypothesis. Whether or not the result is actually important in the real world is a very different question, and depends on all sorts of other things.</p>
</div>
<div id="onesidedtests" class="section level3" number="5.6.3">
<h3><span class="header-section-number">5.6.3</span> The difference between one sided and two sided tests</h3>
<p>There’s one more thing I want to point out about the hypothesis test that I’ve just constructed. If we take a moment to think about the statistical hypotheses I’ve been using,
<span class="math display">\[
\begin{array}{cc}
H_0 : &amp; \theta = .5 \\
H_1 : &amp; \theta \neq .5 
\end{array}
\]</span>
we notice that the alternative hypothesis covers <em>both</em> the possibility that <span class="math inline">\(\theta &lt; .5\)</span> and the possibility that <span class="math inline">\(\theta &gt; .5\)</span>. This makes sense if I really think that ESP could produce better-than-chance performance <em>or</em> worse-than-chance performance (and there are some people who think that). In statistical language, this is an example of a <strong><em>two-sided test</em></strong>. It’s called this because the alternative hypothesis covers the area on both “sides” of the null hypothesis, and as a consequence the critical region of the test covers both tails of the sampling distribution (2.5% on either side if <span class="math inline">\(\alpha =.05\)</span>), as illustrated earlier in Figure <a href="hypothesistesting.html#fig:crit2">5.2</a>.</p>
<p>However, that’s not the only possibility. It might be the case, for example, that I’m only willing to believe in ESP if it produces better than chance performance. If so, then my alternative hypothesis would only covers the possibility that <span class="math inline">\(\theta &gt; .5\)</span>, and as a consequence the null hypothesis now becomes <span class="math inline">\(\theta \leq .5\)</span>:
<span class="math display">\[
\begin{array}{cc}
H_0 : &amp; \theta \leq .5 \\
H_1 : &amp; \theta &gt; .5 
\end{array}
\]</span>
When this happens, we have what’s called a <strong><em>one-sided test</em></strong>, and when this happens the critical region only covers one tail of the sampling distribution. This is illustrated in Figure <a href="hypothesistesting.html#fig:crit1">5.3</a>.</p>
<div class="figure"><span style="display:block;" id="fig:crit1"></span>
<img src="schuster-statistics-remix_files/figure-html/crit1-1.png" alt="The critical region for a one sided test. In this case, the alternative hypothesis is that $\theta &gt; .05$, so we would only reject the null hypothesis for large values of $X$. As a consequence, the critical region only covers the upper tail of the sampling distribution; specifically the upper 5% of the distribution. Contrast this to the two-sided version earlier)" width="672" />
<p class="caption">
Figure 5.3: The critical region for a one sided test. In this case, the alternative hypothesis is that <span class="math inline">\(\theta &gt; .05\)</span>, so we would only reject the null hypothesis for large values of <span class="math inline">\(X\)</span>. As a consequence, the critical region only covers the upper tail of the sampling distribution; specifically the upper 5% of the distribution. Contrast this to the two-sided version earlier)
</p>
</div>
</div>
</div>
<div id="pvalue" class="section level2" number="5.7">
<h2><span class="header-section-number">5.7</span> The <span class="math inline">\(p\)</span> value of a test</h2>
<p>In one sense, our hypothesis test is complete; we’ve constructed a test statistic, figured out its sampling distribution if the null hypothesis is true, and then constructed the critical region for the test. Nevertheless, I’ve actually omitted the most important number of all: <strong><em>the <span class="math inline">\(p\)</span> value</em></strong>. It is to this topic that we now turn. There are two somewhat different ways of interpreting a <span class="math inline">\(p\)</span> value, one proposed by Sir Ronald Fisher and the other by Jerzy Neyman. Both versions are legitimate, though they reflect very different ways of thinking about hypothesis tests. Most introductory textbooks tend to give Fisher’s version only, but I think that’s a bit of a shame. To my mind, Neyman’s version is cleaner, and actually better reflects the logic of the null hypothesis test. You might disagree though, so I’ve included both. I’ll start with Neyman’s version…</p>
<div id="a-softer-view-of-decision-making" class="section level3" number="5.7.1">
<h3><span class="header-section-number">5.7.1</span> A softer view of decision making</h3>
<p>One problem with the hypothesis testing procedure that I’ve described is that it makes no distinction at all between a result this “barely significant” and those that are “highly significant.” For instance, in my ESP study the data I obtained only just fell inside the critical region - so I did get a significant effect, but was a pretty near thing. In contrast, suppose that I’d run a study in which <span class="math inline">\(X=97\)</span> out of my <span class="math inline">\(N=100\)</span> participants got the answer right. This would obviously be significant too, but my a much larger margin; there’s really no ambiguity about this at all. The procedure that I described makes no distinction between the two. If I adopt the standard convention of allowing <span class="math inline">\(\alpha = .05\)</span> as my acceptable Type I error rate, then both of these are significant results.</p>
<p>This is where the <span class="math inline">\(p\)</span> value comes in handy. To understand how it works, let’s suppose that we ran lots of hypothesis tests on the same data set: but with a different value of <span class="math inline">\(\alpha\)</span> in each case. When we do that for my original ESP data, what we’d get is something like this</p>
<table>
<thead>
<tr class="header">
<th align="right">Value of <span class="math inline">\(\alpha\)</span></th>
<th align="left">Reject the null?</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0.05</td>
<td align="left">Yes</td>
</tr>
<tr class="even">
<td align="right">0.04</td>
<td align="left">Yes</td>
</tr>
<tr class="odd">
<td align="right">0.03</td>
<td align="left">Yes</td>
</tr>
<tr class="even">
<td align="right">0.02</td>
<td align="left">No</td>
</tr>
<tr class="odd">
<td align="right">0.01</td>
<td align="left">No</td>
</tr>
</tbody>
</table>
<p>When we test ESP data (<span class="math inline">\(X=62\)</span> successes out of <span class="math inline">\(N=100\)</span> observations) using <span class="math inline">\(\alpha\)</span> levels of .03 and above, we’d always find ourselves rejecting the null hypothesis. For <span class="math inline">\(\alpha\)</span> levels of .02 and below, we always end up retaining the null hypothesis. Therefore, somewhere between .02 and .03 there must be a smallest value of <span class="math inline">\(\alpha\)</span> that would allow us to reject the null hypothesis for this data. This is the <span class="math inline">\(p\)</span> value; as it turns out the ESP data has <span class="math inline">\(p = .021\)</span>. In short:</p>
<blockquote>
<p><span class="math inline">\(p\)</span> is defined to be the smallest Type I error rate (<span class="math inline">\(\alpha\)</span>) that you have to be willing to tolerate if you want to reject the null hypothesis.</p>
</blockquote>
<p>If it turns out that <span class="math inline">\(p\)</span> describes an error rate that you find intolerable, then you must retain the null. If you’re comfortable with an error rate equal to <span class="math inline">\(p\)</span>, then it’s okay to reject the null hypothesis in favour of your preferred alternative.</p>
<p>In effect, <span class="math inline">\(p\)</span> is a summary of all the possible hypothesis tests that you could have run, taken across all possible <span class="math inline">\(\alpha\)</span> values. And as a consequence it has the effect of “softening” our decision process. For those tests in which <span class="math inline">\(p \leq \alpha\)</span> you would have rejected the null hypothesis, whereas for those tests in which <span class="math inline">\(p &gt; \alpha\)</span> you would have retained the null. In my ESP study I obtained <span class="math inline">\(X=62\)</span>, and as a consequence I’ve ended up with <span class="math inline">\(p = .021\)</span>. So the error rate I have to tolerate is 2.1%. In contrast, suppose my experiment had yielded <span class="math inline">\(X=97\)</span>. What happens to my <span class="math inline">\(p\)</span> value now? This time it’s shrunk to <span class="math inline">\(p = 1.36 \times 10^{-25}\)</span>, which is a tiny, tiny<a href="#fn97" class="footnote-ref" id="fnref97"><sup>97</sup></a> Type I error rate. For this second case I would be able to reject the null hypothesis with a lot more confidence, because I only have to be “willing” to tolerate a type I error rate of about 1 in 10 trillion trillion in order to justify my decision to reject.</p>
</div>
<div id="the-probability-of-extreme-data" class="section level3" number="5.7.2">
<h3><span class="header-section-number">5.7.2</span> The probability of extreme data</h3>
<p>The second definition of the <span class="math inline">\(p\)</span>-value comes from Sir Ronald Fisher, and it’s actually this one that you tend to see in most introductory statistics textbooks. Notice how, when I constructed the critical region, it corresponded to the <em>tails</em> (i.e., extreme values) of the sampling distribution? That’s not a coincidence: almost all “good” tests have this characteristic (good in the sense of minimising our type II error rate, <span class="math inline">\(\beta\)</span>). The reason for that is that a good critical region almost always corresponds to those values of the test statistic that are least likely to be observed if the null hypothesis is true. If this rule is true, then we can define the <span class="math inline">\(p\)</span>-value as the probability that we would have observed a test statistic that is at least as extreme as the one we actually did get. In other words, if the data are extremely implausible according to the null hypothesis, then the null hypothesis is probably wrong.</p>
</div>
<div id="a-common-mistake" class="section level3" number="5.7.3">
<h3><span class="header-section-number">5.7.3</span> A common mistake</h3>
<p>Okay, so you can see that there are two rather different but legitimate ways to interpret the <span class="math inline">\(p\)</span> value, one based on Neyman’s approach to hypothesis testing and the other based on Fisher’s. Unfortunately, there is a third explanation that people sometimes give, especially when they’re first learning statistics, and it is <em>absolutely and completely wrong</em>. This mistaken approach is to refer to the <span class="math inline">\(p\)</span> value as “the probability that the null hypothesis is true.” It’s an intuitively appealing way to think, but it’s wrong in two key respects: (1) null hypothesis testing is a frequentist tool, and the frequentist approach to probability does <em>not</em> allow you to assign probabilities to the null hypothesis… according to this view of probability, the null hypothesis is either true or it is not; it cannot have a “5% chance” of being true. (2) even within the Bayesian approach, which does let you assign probabilities to hypotheses, the <span class="math inline">\(p\)</span> value would not correspond to the probability that the null is true; this interpretation is entirely inconsistent with the mathematics of how the <span class="math inline">\(p\)</span> value is calculated. Put bluntly, despite the intuitive appeal of thinking this way, there is <em>no</em> justification for interpreting a <span class="math inline">\(p\)</span> value this way. Never do it.</p>
</div>
</div>
<div id="writeup" class="section level2" number="5.8">
<h2><span class="header-section-number">5.8</span> Reporting the results of a hypothesis test</h2>
<p>When writing up the results of a hypothesis test, there’s usually several pieces of information that you need to report, but it varies a fair bit from test to test. Throughout the rest of the book I’ll spend a little time talking about how to report the results of different tests, so that you can get a feel for how it’s usually done. However, regardless of what test you’re doing, the one thing that you always have to do is say something about the <span class="math inline">\(p\)</span> value, and whether or not the outcome was significant.</p>
<p>The fact that you have to do this is unsurprising; it’s the whole point of doing the test. What might be surprising is the fact that there is some contention over exactly how you’re supposed to do it. Leaving aside those people who completely disagree with the entire framework underpinning null hypothesis testing, there’s a certain amount of tension that exists regarding whether or not to report the exact <span class="math inline">\(p\)</span> value that you obtained, or if you should state only that <span class="math inline">\(p &lt; \alpha\)</span> for a significance level that you chose in advance (e.g., <span class="math inline">\(p&lt;.05\)</span>).</p>
<div id="the-issue" class="section level3" number="5.8.1">
<h3><span class="header-section-number">5.8.1</span> The issue</h3>
<p>To see why this is an issue, the key thing to recognise is that <span class="math inline">\(p\)</span> values are <em>terribly</em> convenient. In practice, the fact that we can compute a <span class="math inline">\(p\)</span> value means that we don’t actually have to specify any <span class="math inline">\(\alpha\)</span> level at all in order to run the test. Instead, what you can do is calculate your <span class="math inline">\(p\)</span> value and interpret it directly: if you get <span class="math inline">\(p = .062\)</span>, then it means that you’d have to be willing to tolerate a Type I error rate of 6.2% to justify rejecting the null. If you personally find 6.2% intolerable, then you retain the null. Therefore, the argument goes, why don’t we just report the actual <span class="math inline">\(p\)</span> value and let the reader make up their own minds about what an acceptable Type I error rate is? This approach has the big advantage of “softening” the decision making process – in fact, if you accept the Neyman definition of the <span class="math inline">\(p\)</span> value, that’s the whole point of the <span class="math inline">\(p\)</span> value. We no longer have a fixed significance level of <span class="math inline">\(\alpha = .05\)</span> as a bright line separating “accept” from “reject” decisions; and this removes the rather pathological problem of being forced to treat <span class="math inline">\(p = .051\)</span> in a fundamentally different way to <span class="math inline">\(p = .049\)</span>.</p>
<p>This flexibility is both the advantage and the disadvantage to the <span class="math inline">\(p\)</span> value. The reason why a lot of people don’t like the idea of reporting an exact <span class="math inline">\(p\)</span> value is that it gives the researcher a bit <em>too much</em> freedom. In particular, it lets you change your mind about what error tolerance you’re willing to put up with <em>after</em> you look at the data. For instance, consider my ESP experiment. Suppose I ran my test, and ended up with a <span class="math inline">\(p\)</span> value of .09. Should I accept or reject? Now, to be honest, I haven’t yet bothered to think about what level of Type I error I’m “really” willing to accept. I don’t have an opinion on that topic. But I <em>do</em> have an opinion about whether or not ESP exists, and I <em>definitely</em> have an opinion about whether my research should be published in a reputable scientific journal. And amazingly, now that I’ve looked at the data I’m starting to think that a 9% error rate isn’t so bad, especially when compared to how annoying it would be to have to admit to the world that my experiment has failed. So, to avoid looking like I just made it up after the fact, I now say that my <span class="math inline">\(\alpha\)</span> is .1: a 10% type I error rate isn’t too bad, and at that level my test is significant! I win.</p>
<p>In other words, the worry here is that I might have the best of intentions, and be the most honest of people, but the temptation to just “shade” things a little bit here and there is really, really strong. As anyone who has ever run an experiment can attest, it’s a long and difficult process, and you often get <em>very</em> attached to your hypotheses. It’s hard to let go and admit the experiment didn’t find what you wanted it to find. And that’s the danger here. If we use the “raw” <span class="math inline">\(p\)</span>-value, people will start interpreting the data in terms of what they <em>want</em> to believe, not what the data are actually saying… and if we allow that, well, why are we bothering to do science at all? Why not let everyone believe whatever they like about anything, regardless of what the facts are? Okay, that’s a bit extreme, but that’s where the worry comes from. According to this view, you really <em>must</em> specify your <span class="math inline">\(\alpha\)</span> value in advance, and then only report whether the test was significant or not. It’s the only way to keep ourselves honest.</p>
</div>
<div id="two-proposed-solutions" class="section level3" number="5.8.2">
<h3><span class="header-section-number">5.8.2</span> Two proposed solutions</h3>
<p>In practice, it’s pretty rare for a researcher to specify a single <span class="math inline">\(\alpha\)</span> level ahead of time. Instead, the convention is that scientists rely on three standard significance levels: .05, .01 and .001. When reporting your results, you indicate which (if any) of these significance levels allow you to reject the null hypothesis. This is summarised in Table <a href="hypothesistesting.html#tab:pvaltable">5.1</a>. This allows us to soften the decision rule a little bit, since <span class="math inline">\(p&lt;.01\)</span> implies that the data meet a stronger evidentiary standard than <span class="math inline">\(p&lt;.05\)</span> would. Nevertheless, since these levels are fixed in advance by convention, it does prevent people choosing their <span class="math inline">\(\alpha\)</span> level after looking at the data.</p>
<table>
<caption><span id="tab:pvaltable">Table 5.1: </span>A commonly adopted convention for reporting <span class="math inline">\(p\)</span> values: in many places it is conventional to report one of four different things (e.g., <span class="math inline">\(p&lt;.05\)</span>) as shown below. I’ve included the “significance stars” notation (i.e., a * indicates <span class="math inline">\(p&lt;.05\)</span>) because you sometimes see this notation produced by statistical software. It’s also worth noting that some people will write <em>n.s.</em> (not significant) rather than <span class="math inline">\(p&gt;.05\)</span>.</caption>
<colgroup>
<col width="11%" />
<col width="10%" />
<col width="67%" />
<col width="11%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Usual notation</th>
<th align="left">Signif. stars</th>
<th align="left">Signif. stars</th>
<th align="left">The null is…</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(p&gt;.05\)</span></td>
<td align="left">NA</td>
<td align="left">The test wasn’t significant</td>
<td align="left">Retained</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(p&lt;.05\)</span></td>
<td align="left">*</td>
<td align="left">The test was significant at <span class="math inline">\(\alpha = .05\)</span> but not at <span class="math inline">\(\alpha =.01\)</span> or <span class="math inline">\(\alpha = .001\)</span></td>
<td align="left">Rejected</td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(p&lt;.01\)</span></td>
<td align="left">**</td>
<td align="left">The test was significant at <span class="math inline">\(\alpha = .05\)</span> and <span class="math inline">\(\alpha = .01\)</span> but not at <span class="math inline">\(\alpha = .001\)</span></td>
<td align="left">Rejected</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(p&lt;.001\)</span></td>
<td align="left">***</td>
<td align="left">The test was significant at all levels</td>
<td align="left">Rejected</td>
</tr>
</tbody>
</table>
<p>Nevertheless, quite a lot of people still prefer to report exact <span class="math inline">\(p\)</span> values. To many people, the advantage of allowing the reader to make up their own mind about how to interpret <span class="math inline">\(p = .06\)</span> outweighs any disadvantages. In practice, however, even among those researchers who prefer exact <span class="math inline">\(p\)</span> values it is quite common to just write <span class="math inline">\(p&lt;.001\)</span> instead of reporting an exact value for small <span class="math inline">\(p\)</span>. This is in part because a lot of software doesn’t actually print out the <span class="math inline">\(p\)</span> value when it’s that small (e.g., SPSS just writes <span class="math inline">\(p = .000\)</span> whenever <span class="math inline">\(p&lt;.001\)</span>), and in part because a very small <span class="math inline">\(p\)</span> value can be kind of misleading. The human mind sees a number like .0000000001 and it’s hard to suppress the gut feeling that the evidence in favour of the alternative hypothesis is a near certainty. In practice however, this is usually wrong. Life is a big, messy, complicated thing: and every statistical test ever invented relies on simplifications, approximations and assumptions. As a consequence, it’s probably not reasonable to walk away from <em>any</em> statistical analysis with a feeling of confidence stronger than <span class="math inline">\(p&lt;.001\)</span> implies. In other words, <span class="math inline">\(p&lt;.001\)</span> is really code for “as far as <em>this test</em> is concerned, the evidence is overwhelming.”</p>
<p>In light of all this, you might be wondering exactly what you should do. There’s a fair bit of contradictory advice on the topic, with some people arguing that you should report the exact <span class="math inline">\(p\)</span> value, and other people arguing that you should use the tiered approach illustrated in Table <a href="hypothesistesting.html#tab:pvaltable">5.1</a>. As a result, the best advice I can give is to suggest that you look at papers/reports written in your field and see what the convention seems to be. If there doesn’t seem to be any consistent pattern, then use whichever method you prefer.</p>
</div>
</div>
<div id="running-the-hypothesis-test-in-practice" class="section level2" number="5.9">
<h2><span class="header-section-number">5.9</span> Running the hypothesis test in practice</h2>
<p>At this point some of you might be wondering if this is a “real” hypothesis test, or just a toy example that I made up. It’s real. In the previous discussion I built the test from first principles, thinking that it was the simplest possible problem that you might ever encounter in real life. However, this test already exists: it’s called the <em>binomial test</em>, and it’s implemented by an R function called <code>binom.test()</code>. To test the null hypothesis that the response probability is one-half <code>p = .5</code>,<a href="#fn98" class="footnote-ref" id="fnref98"><sup>98</sup></a> using data in which <code>x = 62</code> of <code>n = 100</code> people made the correct response, here’s how to do it in R:</p>
<div class="sourceCode" id="cb523"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb523-1"><a href="hypothesistesting.html#cb523-1" aria-hidden="true" tabindex="-1"></a><span class="fu">binom.test</span>( <span class="at">x=</span><span class="dv">62</span>, <span class="at">n=</span><span class="dv">100</span>, <span class="at">p=</span>.<span class="dv">5</span> )</span></code></pre></div>
<pre><code>## 
##  Exact binomial test
## 
## data:  62 and 100
## number of successes = 62, number of trials = 100, p-value =
## 0.02098
## alternative hypothesis: true probability of success is not equal to 0.5
## 95 percent confidence interval:
##  0.5174607 0.7152325
## sample estimates:
## probability of success 
##                   0.62</code></pre>
<p>Right now, this output looks pretty unfamiliar to you, but you can see that it’s telling you more or less the right things. Specifically, the <span class="math inline">\(p\)</span>-value of 0.02 is less than the usual choice of <span class="math inline">\(\alpha = .05\)</span>, so you can reject the null. We’ll talk a lot more about how to read this sort of output as we go along; and after a while you’ll hopefully find it quite easy to read and understand. For now, however, I just wanted to make the point that R contains a whole lot of functions corresponding to different kinds of hypothesis test. And while I’ll usually spend quite a lot of time explaining the logic behind how the tests are built, every time I discuss a hypothesis test the discussion will end with me showing you a fairly simple R command that you can use to run the test in practice.</p>
</div>
<div id="effectsize" class="section level2" number="5.10">
<h2><span class="header-section-number">5.10</span> Effect size, sample size and power</h2>
<p>In previous sections I’ve emphasised the fact that the major design principle behind statistical hypothesis testing is that we try to control our Type I error rate. When we fix <span class="math inline">\(\alpha = .05\)</span> we are attempting to ensure that only 5% of true null hypotheses are incorrectly rejected. However, this doesn’t mean that we don’t care about Type II errors. In fact, from the researcher’s perspective, the error of failing to reject the null when it is actually false is an extremely annoying one. With that in mind, a secondary goal of hypothesis testing is to try to minimise <span class="math inline">\(\beta\)</span>, the Type II error rate, although we don’t usually <em>talk</em> in terms of minimising Type II errors. Instead, we talk about maximising the <em>power</em> of the test. Since power is defined as <span class="math inline">\(1-\beta\)</span>, this is the same thing.</p>
<div id="the-power-function" class="section level3" number="5.10.1">
<h3><span class="header-section-number">5.10.1</span> The power function</h3>
<div class="figure"><span style="display:block;" id="fig:crit3"></span>
<img src="schuster-statistics-remix_files/figure-html/crit3-1.png" alt="Sampling distribution under the *alternative* hypothesis, for a population parameter value of $\theta = 0.55$. A reasonable proportion of the distribution lies in the rejection region." width="672" />
<p class="caption">
Figure 5.4: Sampling distribution under the <em>alternative</em> hypothesis, for a population parameter value of <span class="math inline">\(\theta = 0.55\)</span>. A reasonable proportion of the distribution lies in the rejection region.
</p>
</div>
<p>Let’s take a moment to think about what a Type II error actually is. A Type II error occurs when the alternative hypothesis is true, but we are nevertheless unable to reject the null hypothesis. Ideally, we’d be able to calculate a single number <span class="math inline">\(\beta\)</span> that tells us the Type II error rate, in the same way that we can set <span class="math inline">\(\alpha = .05\)</span> for the Type I error rate. Unfortunately, this is a lot trickier to do. To see this, notice that in my ESP study the alternative hypothesis actually corresponds to lots of possible values of <span class="math inline">\(\theta\)</span>. In fact, the alternative hypothesis corresponds to every value of <span class="math inline">\(\theta\)</span> <em>except</em> 0.5. Let’s suppose that the true probability of someone choosing the correct response is 55% (i.e., <span class="math inline">\(\theta = .55\)</span>). If so, then the <em>true</em> sampling distribution for <span class="math inline">\(X\)</span> is not the same one that the null hypothesis predicts: the most likely value for <span class="math inline">\(X\)</span> is now 55 out of 100. Not only that, the whole sampling distribution has now shifted, as shown in Figure <a href="hypothesistesting.html#fig:crit3">5.4</a>. The critical regions, of course, do not change: by definition, the critical regions are based on what the null hypothesis predicts. What we’re seeing in this figure is the fact that when the null hypothesis is wrong, a much larger proportion of the sampling distribution distribution falls in the critical region. And of course that’s what should happen: the probability of rejecting the null hypothesis is larger when the null hypothesis is actually false! However <span class="math inline">\(\theta = .55\)</span> is not the only possibility consistent with the alternative hypothesis. Let’s instead suppose that the true value of <span class="math inline">\(\theta\)</span> is actually 0.7. What happens to the sampling distribution when this occurs? The answer, shown in Figure <a href="hypothesistesting.html#fig:crit4">5.5</a>, is that almost the entirety of the sampling distribution has now moved into the critical region. Therefore, if <span class="math inline">\(\theta = 0.7\)</span> the probability of us correctly rejecting the null hypothesis (i.e., the power of the test) is much larger than if <span class="math inline">\(\theta = 0.55\)</span>. In short, while <span class="math inline">\(\theta = .55\)</span> and <span class="math inline">\(\theta = .70\)</span> are both part of the alternative hypothesis, the Type II error rate is different.</p>
<div class="figure"><span style="display:block;" id="fig:crit4"></span>
<img src="schuster-statistics-remix_files/figure-html/crit4-1.png" alt="Sampling distribution under the *alternative* hypothesis, for a population parameter value of $\theta = 0.70$. Almost all of the distribution lies in the rejection region." width="672" />
<p class="caption">
Figure 5.5: Sampling distribution under the <em>alternative</em> hypothesis, for a population parameter value of <span class="math inline">\(\theta = 0.70\)</span>. Almost all of the distribution lies in the rejection region.
</p>
</div>
<div class="figure"><span style="display:block;" id="fig:powerfunction"></span>
<img src="schuster-statistics-remix_files/figure-html/powerfunction-1.png" alt="The probability that we will reject the null hypothesis, plotted as a function of the true value of $\theta$. Obviously, the test is more powerful (greater chance of correct rejection) if the true value of $\theta$ is very different from the value that the null hypothesis specifies (i.e., $\theta=.5$). Notice that when $\theta$ actually is equal to .5 (plotted as a black dot), the null hypothesis is in fact true: rejecting the null hypothesis in this instance would be a Type I error." width="672" />
<p class="caption">
Figure 5.6: The probability that we will reject the null hypothesis, plotted as a function of the true value of <span class="math inline">\(\theta\)</span>. Obviously, the test is more powerful (greater chance of correct rejection) if the true value of <span class="math inline">\(\theta\)</span> is very different from the value that the null hypothesis specifies (i.e., <span class="math inline">\(\theta=.5\)</span>). Notice that when <span class="math inline">\(\theta\)</span> actually is equal to .5 (plotted as a black dot), the null hypothesis is in fact true: rejecting the null hypothesis in this instance would be a Type I error.
</p>
</div>
<p>What all this means is that the power of a test (i.e., <span class="math inline">\(1-\beta\)</span>) depends on the true value of <span class="math inline">\(\theta\)</span>. To illustrate this, I’ve calculated the expected probability of rejecting the null hypothesis for all values of <span class="math inline">\(\theta\)</span>, and plotted it in Figure <a href="hypothesistesting.html#fig:powerfunction">5.6</a>. This plot describes what is usually called the <strong><em>power function</em></strong> of the test. It’s a nice summary of how good the test is, because it actually tells you the power (<span class="math inline">\(1-\beta\)</span>) for all possible values of <span class="math inline">\(\theta\)</span>. As you can see, when the true value of <span class="math inline">\(\theta\)</span> is very close to 0.5, the power of the test drops very sharply, but when it is further away, the power is large.</p>
</div>
<div id="effect-size" class="section level3" number="5.10.2">
<h3><span class="header-section-number">5.10.2</span> Effect size</h3>
<blockquote>
<p><em>Since all models are wrong the scientist must be alert to what is importantly wrong. It is inappropriate to be concerned with mice when there are tigers abroad</em></p>
<p>– George Box 1976</p>
</blockquote>
<p>The plot shown in Figure <a href="hypothesistesting.html#fig:powerfunction">5.6</a> captures a fairly basic point about hypothesis testing. If the true state of the world is very different from what the null hypothesis predicts, then your power will be very high; but if the true state of the world is similar to the null (but not identical) then the power of the test is going to be very low. Therefore, it’s useful to be able to have some way of quantifying how “similar” the true state of the world is to the null hypothesis. A statistic that does this is called a measure of <strong><em>effect size</em></strong> <span class="citation">(e.g. <a href="#ref-Cohen1988" role="doc-biblioref">J. Cohen 1988</a>; <a href="#ref-Ellis2010" role="doc-biblioref">Ellis 2010</a>)</span>. Effect size is defined slightly differently in different contexts,<a href="#fn99" class="footnote-ref" id="fnref99"><sup>99</sup></a> (and so this section just talks in general terms) but the qualitative idea that it tries to capture is always the same: how big is the difference between the <em>true</em> population parameters, and the parameter values that are assumed by the null hypothesis? In our ESP example, if we let <span class="math inline">\(\theta_0 = 0.5\)</span> denote the value assumed by the null hypothesis, and let <span class="math inline">\(\theta\)</span> denote the true value, then a simple measure of effect size could be something like the difference between the true value and null (i.e., <span class="math inline">\(\theta - \theta_0\)</span>), or possibly just the magnitude of this difference, <span class="math inline">\(\mbox{abs}(\theta - \theta_0)\)</span>.</p>
<table>
<colgroup>
<col width="19%" />
<col width="40%" />
<col width="40%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"></th>
<th align="left">big effect size</th>
<th align="left">small effect size</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">significant result</td>
<td align="left">difference is real, and of practical importance</td>
<td align="left">difference is real, but might not be interesting</td>
</tr>
<tr class="even">
<td align="left">non-significant result</td>
<td align="left">no effect observed</td>
<td align="left">no effect observed</td>
</tr>
</tbody>
</table>
<p>Why calculate effect size? Let’s assume that you’ve run your experiment, collected the data, and gotten a significant effect when you ran your hypothesis test. Isn’t it enough just to say that you’ve gotten a significant effect? Surely that’s the <em>point</em> of hypothesis testing? Well, sort of. Yes, the point of doing a hypothesis test is to try to demonstrate that the null hypothesis is wrong, but that’s hardly the only thing we’re interested in. If the null hypothesis claimed that <span class="math inline">\(\theta = .5\)</span>, and we show that it’s wrong, we’ve only really told half of the story. Rejecting the null hypothesis implies that we believe that <span class="math inline">\(\theta \neq .5\)</span>, but there’s a big difference between <span class="math inline">\(\theta = .51\)</span> and <span class="math inline">\(\theta = .8\)</span>. If we find that <span class="math inline">\(\theta = .8\)</span>, then not only have we found that the null hypothesis is wrong, it appears to be <em>very</em> wrong. On the other hand, suppose we’ve successfully rejected the null hypothesis, but it looks like the true value of <span class="math inline">\(\theta\)</span> is only .51 (this would only be possible with a large study). Sure, the null hypothesis is wrong, but it’s not at all clear that we actually <em>care</em>, because the effect size is so small. In the context of my ESP study we might still care, since any demonstration of real psychic powers would actually be pretty cool<a href="#fn100" class="footnote-ref" id="fnref100"><sup>100</sup></a>, but in other contexts a 1% difference isn’t very interesting, even if it is a real difference. For instance, suppose we’re looking at differences in high school exam scores between males and females, and it turns out that the female scores are 1% higher on average than the males. If I’ve got data from thousands of students, then this difference will almost certainly be <em>statistically significant</em>, but regardless of how small the <span class="math inline">\(p\)</span> value is it’s just not very interesting. You’d hardly want to go around proclaiming a crisis in boys education on the basis of such a tiny difference would you? It’s for this reason that it is becoming more standard (slowly, but surely) to report some kind of standard measure of effect size along with the the results of the hypothesis test. The hypothesis test itself tells you whether you should believe that the effect you have observed is real (i.e., not just due to chance); the effect size tells you whether or not you should care.</p>
</div>
<div id="increasing-the-power-of-your-study" class="section level3" number="5.10.3">
<h3><span class="header-section-number">5.10.3</span> Increasing the power of your study</h3>
<p>Not surprisingly, scientists are fairly obsessed with maximising the power of their experiments. We want our experiments to work, and so we want to maximise the chance of rejecting the null hypothesis if it is false (and of course we usually want to believe that it is false!) As we’ve seen, one factor that influences power is the effect size. So the first thing you can do to increase your power is to increase the effect size. In practice, what this means is that you want to design your study in such a way that the effect size gets magnified. For instance, in my ESP study I might believe that psychic powers work best in a quiet, darkened room; with fewer distractions to cloud the mind. Therefore I would try to conduct my experiments in just such an environment: if I can strengthen people’s ESP abilities somehow, then the true value of <span class="math inline">\(\theta\)</span> will go up<a href="#fn101" class="footnote-ref" id="fnref101"><sup>101</sup></a> and therefore my effect size will be larger. In short, clever experimental design is one way to boost power; because it can alter the effect size.</p>
<p>Unfortunately, it’s often the case that even with the best of experimental designs you may have only a small effect. Perhaps, for example, ESP really does exist, but even under the best of conditions it’s very very weak. Under those circumstances, your best bet for increasing power is to increase the sample size. In general, the more observations that you have available, the more likely it is that you can discriminate between two hypotheses. If I ran my ESP experiment with 10 participants, and 7 of them correctly guessed the colour of the hidden card, you wouldn’t be terribly impressed. But if I ran it with 10,000 participants and 7,000 of them got the answer right, you would be much more likely to think I had discovered something. In other words, power increases with the sample size. This is illustrated in Figure <a href="hypothesistesting.html#fig:powerfunctionsample">5.7</a>, which shows the power of the test for a true parameter of <span class="math inline">\(\theta = 0.7\)</span>, for all sample sizes <span class="math inline">\(N\)</span> from 1 to 100, where I’m assuming that the null hypothesis predicts that <span class="math inline">\(\theta_0 = 0.5\)</span>.</p>
<pre><code>##   [1] 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000 0.11837800
##   [7] 0.08257300 0.05771362 0.19643626 0.14945203 0.11303734 0.25302172
##  [13] 0.20255096 0.16086106 0.29695959 0.24588947 0.38879291 0.33269435
##  [19] 0.28223844 0.41641377 0.36272868 0.31341925 0.43996501 0.38859619
##  [25] 0.51186665 0.46049782 0.41129777 0.52752694 0.47870819 0.58881596
##  [31] 0.54162450 0.49507894 0.59933871 0.55446069 0.65155826 0.60907715
##  [37] 0.69828554 0.65867614 0.61815357 0.70325017 0.66542910 0.74296156
##  [43] 0.70807163 0.77808343 0.74621569 0.71275488 0.78009449 0.74946571
##  [49] 0.81000236 0.78219322 0.83626633 0.81119597 0.78435605 0.83676444
##  [55] 0.81250680 0.85920268 0.83741123 0.87881491 0.85934395 0.83818214
##  [61] 0.87858194 0.85962510 0.89539581 0.87849413 0.91004390 0.89503851
##  [67] 0.92276845 0.90949768 0.89480727 0.92209753 0.90907263 0.93304809
##  [73] 0.92153987 0.94254237 0.93240638 0.92108426 0.94185449 0.93185881
##  [79] 0.95005094 0.94125189 0.95714694 0.94942195 0.96327866 0.95651332
##  [85] 0.94886329 0.96265653 0.95594208 0.96796884 0.96208909 0.97255504
##  [91] 0.96741721 0.97650832 0.97202770 0.97991117 0.97601093 0.97153910
##  [97] 0.97944717 0.97554675 0.98240749 0.97901142</code></pre>
<div class="figure"><span style="display:block;" id="fig:powerfunctionsample"></span>
<img src="schuster-statistics-remix_files/figure-html/powerfunctionsample-1.png" alt="The power of our test, plotted as a function of the sample size $N$. In this case, the true value of $\theta$ is 0.7, but the null hypothesis is that $\theta = 0.5$. Overall, larger $N$ means greater power. (The small zig-zags in this function occur because of some odd interactions between $\theta$, $\alpha$ and the fact that the binomial distribution is discrete; it doesn't matter for any serious purpose) " width="672" />
<p class="caption">
Figure 5.7: The power of our test, plotted as a function of the sample size <span class="math inline">\(N\)</span>. In this case, the true value of <span class="math inline">\(\theta\)</span> is 0.7, but the null hypothesis is that <span class="math inline">\(\theta = 0.5\)</span>. Overall, larger <span class="math inline">\(N\)</span> means greater power. (The small zig-zags in this function occur because of some odd interactions between <span class="math inline">\(\theta\)</span>, <span class="math inline">\(\alpha\)</span> and the fact that the binomial distribution is discrete; it doesn’t matter for any serious purpose)
</p>
</div>
<p>Because power is important, whenever you’re contemplating running an experiment it would be pretty useful to know how much power you’re likely to have. It’s never possible to know for sure, since you can’t possibly know what your effect size is. However, it’s often (well, sometimes) possible to guess how big it should be. If so, you can guess what sample size you need! This idea is called <strong><em>power analysis</em></strong>, and if it’s feasible to do it, then it’s very helpful, since it can tell you something about whether you have enough time or money to be able to run the experiment successfully. It’s increasingly common to see people arguing that power analysis should be a required part of experimental design, so it’s worth knowing about.</p>
</div>
</div>
<div id="nhstmess" class="section level2" number="5.11">
<h2><span class="header-section-number">5.11</span> Some issues to consider</h2>
<p>What I’ve described to you in this chapter is the orthodox framework for null hypothesis significance testing (NHST). Understanding how NHST works is an absolute necessity, since it has been the dominant approach to inferential statistics ever since it came to prominence in the early 20th century. It’s what the vast majority of working scientists rely on for their data analysis, so even if you hate it you need to know it. However, the approach is not without problems. There are a number of quirks in the framework, historical oddities in how it came to be, theoretical disputes over whether or not the framework is right, and a lot of practical traps for the unwary. I’m not going to go into a lot of detail on this topic, but I think it’s worth briefly discussing a few of these issues.</p>
<div id="neyman-versus-fisher" class="section level3" number="5.11.1">
<h3><span class="header-section-number">5.11.1</span> Neyman versus Fisher</h3>
<p>The first thing you should be aware of is that orthodox NHST is actually a mash-up of two rather different approaches to hypothesis testing, one proposed by Sir Ronald Fisher and the other proposed by Jerzy Neyman <span class="citation">(for a historical summary see <a href="#ref-Lehmann2011" role="doc-biblioref">Lehmann 2011</a>)</span>. The history is messy because Fisher and Neyman were real people whose opinions changed over time, and at no point did either of them offer “the definitive statement” of how we should interpret their work many decades later. That said, here’s a quick summary of what I take these two approaches to be.</p>
<p>First, let’s talk about Fisher’s approach. As far as I can tell, Fisher assumed that you only had the one hypothesis (the null), and what you want to do is find out if the null hypothesis is inconsistent with the data. From his perspective, what you should do is check to see if the data are “sufficiently unlikely” according to the null. In fact, if you remember back to our earlier discussion, that’s how Fisher defines the <span class="math inline">\(p\)</span>-value. According to Fisher, if the null hypothesis provided a very poor account of the data, you could safely reject it. But, since you don’t have any other hypotheses to compare it to, there’s no way of “accepting the alternative” because you don’t necessarily have an explicitly stated alternative. That’s more or less all that there was to it.</p>
<p>In contrast, Neyman thought that the point of hypothesis testing was as a guide to action, and his approach was somewhat more formal than Fisher’s. His view was that there are multiple things that you could <em>do</em> (accept the null or accept the alternative) and the point of the test was to tell you which one the data support. From this perspective, it is critical to specify your alternative hypothesis properly. If you don’t know what the alternative hypothesis is, then you don’t know how powerful the test is, or even which action makes sense. His framework genuinely requires a competition between different hypotheses. For Neyman, the <span class="math inline">\(p\)</span> value didn’t directly measure the probability of the data (or data more extreme) under the null, it was more of an abstract description about which “possible tests” were telling you to accept the null, and which “possible tests” were telling you to accept the alternative.</p>
<p>As you can see, what we have today is an odd mishmash of the two. We talk about having both a null hypothesis and an alternative (Neyman), but usually<a href="#fn102" class="footnote-ref" id="fnref102"><sup>102</sup></a> define the <span class="math inline">\(p\)</span> value in terms of exreme data (Fisher), but we still have <span class="math inline">\(\alpha\)</span> values (Neyman). Some of the statistical tests have explicitly specified alternatives (Neyman) but others are quite vague about it (Fisher). And, according to some people at least, we’re not allowed to talk about accepting the alternative (Fisher). It’s a mess: but I hope this at least explains why it’s a mess.</p>
</div>
<div id="bayesians-versus-frequentists" class="section level3" number="5.11.2">
<h3><span class="header-section-number">5.11.2</span> Bayesians versus frequentists</h3>
<p>Earlier on in this chapter I was quite emphatic about the fact that you <em>cannot</em> interpret the <span class="math inline">\(p\)</span> value as the probability that the null hypothesis is true. NHST is fundamentally a frequentist tool (see Chapter <a href="inferential-statistics-the-central-limit-theorem.html#estimation">4.2</a>) and as such it does not allow you to assign probabilities to hypotheses: the null hypothesis is either true or it is not. The Bayesian approach to statistics interprets probability as a degree of belief, so it’s totally okay to say that there is a 10% chance that the null hypothesis is true: that’s just a reflection of the degree of confidence that you have in this hypothesis. <strong>You aren’t allowed to do this within the frequentist approach</strong>. Remember, if you’re a frequentist, a probability can only be defined in terms of what happens after a large number of independent replications (i.e., a long run frequency). If this is your interpretation of probability, talking about the “probability” that the null hypothesis is true is complete gibberish: a null hypothesis is either true or it is false. There’s no way you can talk about a long run frequency for this statement. To talk about “the probability of the null hypothesis” is as meaningless as “the colour of freedom.” It doesn’t have one!</p>
<p>Most importantly, this <em>isn’t</em> a purely ideological matter. If you decide that you are a Bayesian and that you’re okay with making probability statements about hypotheses, you have to follow the Bayesian rules for calculating those probabilities. For now what I want to point out to you is the <span class="math inline">\(p\)</span> value is a <em>terrible</em> approximation to the probability that <span class="math inline">\(H_0\)</span> is true. If what you want to know is the probability of the null, then the <span class="math inline">\(p\)</span> value is not what you’re looking for!</p>
</div>
<div id="traps" class="section level3" number="5.11.3">
<h3><span class="header-section-number">5.11.3</span> Traps</h3>
<p>As you can see, the theory behind hypothesis testing is a mess, and even now there are arguments in statistics about how it “should” work. However, disagreements among statisticians are not our real concern here. Our real concern is practical data analysis. And while the “orthodox” approach to null hypothesis significance testing has many drawbacks, even an unrepentant Bayesian like myself would agree that they can be useful if used responsibly. Most of the time they give sensible answers, and you can use them to learn interesting things. Setting aside the various ideologies and historical confusions that we’ve discussed, the fact remains that the biggest danger in all of statistics is <em>thoughtlessness</em>. I don’t mean stupidity, here: I literally mean thoughtlessness. The rush to interpret a result without spending time thinking through what each test actually says about the data, and checking whether that’s consistent with how you’ve interpreted it. That’s where the biggest trap lies.</p>
<p>To give an example of this, consider the following example <span class="citation">(see <a href="#ref-Gelman2006" role="doc-biblioref">Gelman and Stern 2006</a>)</span>. Suppose I’m running my ESP study, and I’ve decided to analyse the data separately for the male participants and the female participants. Of the male participants, 33 out of 50 guessed the colour of the card correctly. This is a significant effect (<span class="math inline">\(p = .03\)</span>). Of the female participants, 29 out of 50 guessed correctly. This is not a significant effect (<span class="math inline">\(p = .32\)</span>). Upon observing this, it is extremely tempting for people to start wondering why there is a difference between males and females in terms of their psychic abilities. However, this is wrong. If you think about it, we haven’t <em>actually</em> run a test that explicitly compares males to females. All we have done is compare males to chance (binomial test was significant) and compared females to chance (binomial test was non significant). If we want to argue that there is a real difference between the males and the females, we should probably run a test of the null hypothesis that there is no difference! We can do that using a different hypothesis test,<a href="#fn103" class="footnote-ref" id="fnref103"><sup>103</sup></a> but when we do that it turns out that we have no evidence that males and females are significantly different (<span class="math inline">\(p = .54\)</span>). <em>Now</em> do you think that there’s anything fundamentally different between the two groups? Of course not. What’s happened here is that the data from both groups (male and female) are pretty borderline: by pure chance, one of them happened to end up on the magic side of the <span class="math inline">\(p = .05\)</span> line, and the other one didn’t. That doesn’t actually imply that males and females are different. This mistake is so common that you should always be wary of it: the difference between significant and not-significant is <em>not</em> evidence of a real difference – if you want to say that there’s a difference between two groups, then you have to test for that difference!</p>
<p>The example above is just that: an example. I’ve singled it out because it’s such a common one, but the bigger picture is that data analysis can be tricky to get right. Think about <em>what</em> it is you want to test, <em>why</em> you want to test it, and whether or not the answers that your test gives could possibly make any sense in the real world.</p>
</div>
</div>
<div id="summary-4" class="section level2" number="5.12">
<h2><span class="header-section-number">5.12</span> Summary</h2>
<p>Null hypothesis testing is one of the most ubiquitous elements to statistical theory. The vast majority of scientific papers report the results of some hypothesis test or another. As a consequence it is almost impossible to get by in science without having at least a cursory understanding of what a <span class="math inline">\(p\)</span>-value means, making this one of the most important chapters in the book. As usual, I’ll end the chapter with a quick recap of the key ideas that we’ve talked about:</p>
<ul>
<li>Research hypotheses and statistical hypotheses. Null and alternative hypotheses. (Section <a href="hypothesistesting.html#hypotheses">5.3</a>).</li>
<li>Type 1 and Type 2 errors (Section <a href="hypothesistesting.html#errortypes">5.4</a>)</li>
<li>Test statistics and sampling distributions (Section <a href="hypothesistesting.html#teststatistics">5.5</a>)</li>
<li>Hypothesis testing as a decision making process (Section <a href="hypothesistesting.html#decisionmaking">5.6</a>)</li>
<li><span class="math inline">\(p\)</span>-values as “soft” decisions (Section <a href="hypothesistesting.html#pvalue">5.7</a>)</li>
<li>Writing up the results of a hypothesis test (Section <a href="hypothesistesting.html#writeup">5.8</a>)</li>
<li>Effect size and power (Section <a href="hypothesistesting.html#effectsize">5.10</a>)</li>
<li>A few issues to consider regarding hypothesis testing (Section <a href="hypothesistesting.html#nhstmess">5.11</a>)</li>
</ul>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Cohen1988" class="csl-entry">
Cohen, J. 1988. <em>Statistical Power Analysis for the Behavioral Sciences</em>. 2nd ed. Lawrence Erlbaum.
</div>
<div id="ref-Ellis2010" class="csl-entry">
Ellis, P. D. 2010. <em>The Essential Guide to Effect Sizes: Statistical Power, Meta-Analysis, and the Interpretation of Research Results</em>. Cambridge, UK: Cambridge University Press.
</div>
<div id="ref-Gelman2006" class="csl-entry">
Gelman, A., and H. Stern. 2006. <span>“The Difference Between <span>‘Significant’</span> and <span>‘Not Significant’</span> Is Not Itself Statistically Significant.”</span> <em>The American Statistician</em> 60: 328–31.
</div>
<div id="ref-Lehmann2011" class="csl-entry">
Lehmann, Erich L. 2011. <em>Fisher, <span>N</span>eyman, and the Creation of Classical Statistics</em>. Springer.
</div>
<div id="ref-Navarro2018" class="csl-entry">
Navarro, D. 2018. <em>Learning Statistics with r: A Tutorial for Psychology Students and Other Beginners (Version 0.6)</em>. <a href="https://learningstatisticswithr.com">https://learningstatisticswithr.com</a>.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="90">
<li id="fn90"><p>The quote comes from Wittgenstein’s (1922) text, <em>Tractatus Logico-Philosphicus</em>.<a href="hypothesistesting.html#fnref90" class="footnote-back">↩︎</a></p></li>
<li id="fn91"><p>A technical note. The description below differs subtly from the standard description given in a lot of introductory texts. The orthodox theory of null hypothesis testing emerged from the work of Sir Ronald Fisher and Jerzy Neyman in the early 20th century; but Fisher and Neyman actually had very different views about how it should work. The standard treatment of hypothesis testing that most texts use is a hybrid of the two approaches. The treatment here is a little more Neyman-style than the orthodox view, especially as regards the meaning of the <span class="math inline">\(p\)</span> value.<a href="hypothesistesting.html#fnref91" class="footnote-back">↩︎</a></p></li>
<li id="fn92"><p>My apologies to anyone who actually believes in this stuff, but on my reading of the literature on ESP, it’s just not reasonable to think this is real. To be fair, though, some of the studies are rigorously designed; so it’s actually an interesting area for thinking about psychological research design. And of course it’s a free country, so you can spend your own time and effort proving me wrong if you like, but I wouldn’t think that’s a terribly practical use of your intellect.<a href="hypothesistesting.html#fnref92" class="footnote-back">↩︎</a></p></li>
<li id="fn93"><p>This analogy only works if you’re from an adversarial legal system like UK/US/Australia. As I understand these things, the French inquisitorial system is quite different.<a href="hypothesistesting.html#fnref93" class="footnote-back">↩︎</a></p></li>
<li id="fn94"><p>An aside regarding the language you use to talk about hypothesis testing. Firstly, one thing you really want to avoid is the word “prove”: a statistical test really doesn’t <em>prove</em> that a hypothesis is true or false. Proof implies certainty, and as the saying goes, statistics means never having to say you’re certain. On that point almost everyone would agree. However, beyond that there’s a fair amount of confusion. Some people argue that you’re only allowed to make statements like “rejected the null,” “failed to reject the null,” or possibly “retained the null.” According to this line of thinking, you can’t say things like “accept the alternative” or “accept the null.” Personally I think this is too strong: in my opinion, this conflates null hypothesis testing with Karl Popper’s falsificationist view of the scientific process. While there are similarities between falsificationism and null hypothesis testing, they aren’t equivalent. However, while I personally think it’s fine to talk about accepting a hypothesis (on the proviso that “acceptance” doesn’t actually mean that it’s necessarily true, especially in the case of the null hypothesis), many people will disagree. And more to the point, you should be aware that this particular weirdness exists, so that you’re not caught unawares by it when writing up your own results.<a href="hypothesistesting.html#fnref94" class="footnote-back">↩︎</a></p></li>
<li id="fn95"><p>Strictly speaking, the test I just constructed has <span class="math inline">\(\alpha = .057\)</span>, which is a bit too generous. However, if I’d chosen 39 and 61 to be the boundaries for the critical region, then the critical region only covers 3.5% of the distribution. I figured that it makes more sense to use 40 and 60 as my critical values, and be willing to tolerate a 5.7% type I error rate, since that’s as close as I can get to a value of <span class="math inline">\(\alpha = .05\)</span>.<a href="hypothesistesting.html#fnref95" class="footnote-back">↩︎</a></p></li>
<li id="fn96"><p>The internet seems fairly convinced that Ashley said this, though I can’t for the life of me find anyone willing to give a source for the claim.<a href="hypothesistesting.html#fnref96" class="footnote-back">↩︎</a></p></li>
<li id="fn97"><p>That’s <span class="math inline">\(p = .000000000000000000000000136\)</span> for folks that don’t like scientific notation!<a href="hypothesistesting.html#fnref97" class="footnote-back">↩︎</a></p></li>
<li id="fn98"><p>Note that the <code>p</code> here has nothing to do with a <span class="math inline">\(p\)</span> value. The <code>p</code> argument in the <code>binom.test()</code> function corresponds to the probability of making a correct response, according to the null hypothesis. In other words, it’s the <span class="math inline">\(\theta\)</span> value.<a href="hypothesistesting.html#fnref98" class="footnote-back">↩︎</a></p></li>
<li id="fn99"><p>There’s an R package called <code>compute.es</code> that can be used for calculating a very broad range of effect size measures; but for the purposes of the current book we won’t need it: all of the effect size measures that I’ll talk about here have functions in the <code>lsr</code> package<a href="hypothesistesting.html#fnref99" class="footnote-back">↩︎</a></p></li>
<li id="fn100"><p>Although in practice a very small effect size is worrying, because even very minor methodological flaws might be responsible for the effect; and in practice no experiment is perfect, so there are always methodological issues to worry about.<a href="hypothesistesting.html#fnref100" class="footnote-back">↩︎</a></p></li>
<li id="fn101"><p>Notice that the true population parameter <span class="math inline">\(\theta\)</span> doesn’t necessarily correspond to an immutable fact of nature. In this context <span class="math inline">\(\theta\)</span> is just the true probability that people would correctly guess the colour of the card in the other room. As such the population parameter can be influenced by all sorts of things. Of course, this is all on the assumption that ESP actually exists!<a href="hypothesistesting.html#fnref101" class="footnote-back">↩︎</a></p></li>
<li id="fn102"><p>Although this book describes both Neyman’s and Fisher’s definition of the <span class="math inline">\(p\)</span> value, most don’t. Most introductory textbooks will only give you the Fisher version.<a href="hypothesistesting.html#fnref102" class="footnote-back">↩︎</a></p></li>
<li id="fn103"><p>In this case, the Pearson chi-square test of independence (Chapter <a href="#chisquare"><strong>??</strong></a>; <code>chisq.test()</code> in R) is what we use; see also the <code>prop.test()</code> function.<a href="hypothesistesting.html#fnref103" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="inferential-statistics-the-central-limit-theorem.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="issues-in-hypothesis-testing.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/05-hypothesis-testing.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["schuster-statistics-remix.pdf", "schuster-statistics-remix.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
