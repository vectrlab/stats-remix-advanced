<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8 Regression | Advanced Statistics Remix</title>
  <meta name="description" content="A textbook for advanced statistics" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 8 Regression | Advanced Statistics Remix" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="A textbook for advanced statistics" />
  <meta name="github-repo" content="vectrlab/stat-course-pack" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 Regression | Advanced Statistics Remix" />
  
  <meta name="twitter:description" content="A textbook for advanced statistics" />
  

<meta name="author" content="David Schuster" />


<meta name="date" content="2021-11-15" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="data-cleaning-and-missing-values-analysis.html"/>
<link rel="next" href="ttest.html"/>
<script src="libs/header-attrs-2.10/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Course Pack for Statistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About this Book</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#attribution"><i class="fa fa-check"></i>Attribution</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i>License</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="statistics-for-research.html"><a href="statistics-for-research.html"><i class="fa fa-check"></i><b>1</b> Statistics for Research</a>
<ul>
<li class="chapter" data-level="1.1" data-path="statistics-for-research.html"><a href="statistics-for-research.html#introduction"><i class="fa fa-check"></i><b>1.1</b> Introduction</a></li>
<li class="chapter" data-level="1.2" data-path="statistics-for-research.html"><a href="statistics-for-research.html#measurement"><i class="fa fa-check"></i><b>1.2</b> Measurement</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="statistics-for-research.html"><a href="statistics-for-research.html#level-of-measurement"><i class="fa fa-check"></i><b>1.2.1</b> Level of Measurement</a></li>
<li class="chapter" data-level="1.2.2" data-path="statistics-for-research.html"><a href="statistics-for-research.html#continuous-or-discrete"><i class="fa fa-check"></i><b>1.2.2</b> Continuous or Discrete</a></li>
<li class="chapter" data-level="1.2.3" data-path="statistics-for-research.html"><a href="statistics-for-research.html#qualitative-or-quantitative"><i class="fa fa-check"></i><b>1.2.3</b> Qualitative or Quantitative</a></li>
<li class="chapter" data-level="1.2.4" data-path="statistics-for-research.html"><a href="statistics-for-research.html#distribution-a-collection-of-our-observations"><i class="fa fa-check"></i><b>1.2.4</b> Distribution: A collection of our observations</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="statistics-for-research.html"><a href="statistics-for-research.html#descriptive-statistics-summarizing-our-observations"><i class="fa fa-check"></i><b>1.3</b> Descriptive Statistics: Summarizing our observations</a></li>
<li class="chapter" data-level="1.4" data-path="statistics-for-research.html"><a href="statistics-for-research.html#inferential-statistics-generalizing-from-our-observations"><i class="fa fa-check"></i><b>1.4</b> Inferential Statistics: Generalizing from our observations</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="statistics-for-research.html"><a href="statistics-for-research.html#populations-and-samples-who-or-what-the-research-is-about"><i class="fa fa-check"></i><b>1.4.1</b> Populations and Samples: Who (or what) the research is about</a></li>
<li class="chapter" data-level="1.4.2" data-path="statistics-for-research.html"><a href="statistics-for-research.html#constructs-provide-the-context"><i class="fa fa-check"></i><b>1.4.2</b> Constructs provide the context</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="statistics-for-research.html"><a href="statistics-for-research.html#the-cautionary-tale-of-simpsons-paradox"><i class="fa fa-check"></i><b>1.5</b> The cautionary tale of Simpson’s paradox</a></li>
<li class="chapter" data-level="1.6" data-path="statistics-for-research.html"><a href="statistics-for-research.html#studydesign"><i class="fa fa-check"></i><b>1.6</b> A brief introduction to research design</a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="statistics-for-research.html"><a href="statistics-for-research.html#some-thoughts-about-psychological-measurement"><i class="fa fa-check"></i><b>1.6.1</b> Some thoughts about psychological measurement</a></li>
<li class="chapter" data-level="1.6.2" data-path="statistics-for-research.html"><a href="statistics-for-research.html#operationalisation-defining-your-measurement"><i class="fa fa-check"></i><b>1.6.2</b> Operationalisation: defining your measurement</a></li>
<li class="chapter" data-level="1.6.3" data-path="statistics-for-research.html"><a href="statistics-for-research.html#ivdv"><i class="fa fa-check"></i><b>1.6.3</b> The “role” of variables: predictors and outcomes</a></li>
<li class="chapter" data-level="1.6.4" data-path="statistics-for-research.html"><a href="statistics-for-research.html#researchdesigns"><i class="fa fa-check"></i><b>1.6.4</b> Experimental and non-experimental research</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="statistics-for-research.html"><a href="statistics-for-research.html#causality-research-and-statistics"><i class="fa fa-check"></i><b>1.7</b> Causality, Research, and Statistics</a>
<ul>
<li class="chapter" data-level="1.7.1" data-path="statistics-for-research.html"><a href="statistics-for-research.html#experimental-quasi-experimental-and-non-experimental-studies"><i class="fa fa-check"></i><b>1.7.1</b> Experimental, Quasi-Experimental, and Non-Experimental Studies</a></li>
<li class="chapter" data-level="1.7.2" data-path="statistics-for-research.html"><a href="statistics-for-research.html#demonstrating-causality"><i class="fa fa-check"></i><b>1.7.2</b> Demonstrating Causality</a></li>
<li class="chapter" data-level="1.7.3" data-path="statistics-for-research.html"><a href="statistics-for-research.html#statistics-and-causality"><i class="fa fa-check"></i><b>1.7.3</b> Statistics and Causality</a></li>
<li class="chapter" data-level="1.7.4" data-path="statistics-for-research.html"><a href="statistics-for-research.html#validity-and-reliability"><i class="fa fa-check"></i><b>1.7.4</b> Validity and Reliability</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introR.html"><a href="introR.html"><i class="fa fa-check"></i><b>2</b> Getting started with R</a>
<ul>
<li class="chapter" data-level="2.1" data-path="introR.html"><a href="introR.html#videos"><i class="fa fa-check"></i><b>2.1</b> Videos</a></li>
<li class="chapter" data-level="2.2" data-path="introR.html"><a href="introR.html#introduction-1"><i class="fa fa-check"></i><b>2.2</b> Introduction</a></li>
<li class="chapter" data-level="2.3" data-path="introR.html"><a href="introR.html#gettingR"><i class="fa fa-check"></i><b>2.3</b> Installing R</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="introR.html"><a href="introR.html#installing-r-on-a-windows-computer"><i class="fa fa-check"></i><b>2.3.1</b> Installing R on a Windows computer</a></li>
<li class="chapter" data-level="2.3.2" data-path="introR.html"><a href="introR.html#installing-r-on-a-mac"><i class="fa fa-check"></i><b>2.3.2</b> Installing R on a Mac</a></li>
<li class="chapter" data-level="2.3.3" data-path="introR.html"><a href="introR.html#installing-r-on-a-linux-computer"><i class="fa fa-check"></i><b>2.3.3</b> Installing R on a Linux computer</a></li>
<li class="chapter" data-level="2.3.4" data-path="introR.html"><a href="introR.html#installingrstudio"><i class="fa fa-check"></i><b>2.3.4</b> Downloading and installing RStudio</a></li>
<li class="chapter" data-level="2.3.5" data-path="introR.html"><a href="introR.html#startingR"><i class="fa fa-check"></i><b>2.3.5</b> Starting up R</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="introR.html"><a href="introR.html#firstcommand"><i class="fa fa-check"></i><b>2.4</b> Typing commands at the R console</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="introR.html"><a href="introR.html#an-important-digression-about-formatting"><i class="fa fa-check"></i><b>2.4.1</b> An important digression about formatting</a></li>
<li class="chapter" data-level="2.4.2" data-path="introR.html"><a href="introR.html#be-very-careful-to-avoid-typos"><i class="fa fa-check"></i><b>2.4.2</b> Be very careful to avoid typos</a></li>
<li class="chapter" data-level="2.4.3" data-path="introR.html"><a href="introR.html#r-is-a-bit-flexible-with-spacing"><i class="fa fa-check"></i><b>2.4.3</b> R is (a bit) flexible with spacing</a></li>
<li class="chapter" data-level="2.4.4" data-path="introR.html"><a href="introR.html#r-can-sometimes-tell-that-youre-not-finished-yet-but-not-often"><i class="fa fa-check"></i><b>2.4.4</b> R can sometimes tell that you’re not finished yet (but not often)</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="introR.html"><a href="introR.html#arithmetic"><i class="fa fa-check"></i><b>2.5</b> Doing simple calculations with R</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="introR.html"><a href="introR.html#adding-subtracting-multiplying-and-dividing"><i class="fa fa-check"></i><b>2.5.1</b> Adding, subtracting, multiplying and dividing</a></li>
<li class="chapter" data-level="2.5.2" data-path="introR.html"><a href="introR.html#taking-powers"><i class="fa fa-check"></i><b>2.5.2</b> Taking powers</a></li>
<li class="chapter" data-level="2.5.3" data-path="introR.html"><a href="introR.html#bedmas"><i class="fa fa-check"></i><b>2.5.3</b> Doing calculations in the right order</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="introR.html"><a href="introR.html#assign"><i class="fa fa-check"></i><b>2.6</b> Storing a number as a variable</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="introR.html"><a href="introR.html#variable-assignment-using---and--"><i class="fa fa-check"></i><b>2.6.1</b> Variable assignment using <code>&lt;-</code> and <code>-&gt;</code></a></li>
<li class="chapter" data-level="2.6.2" data-path="introR.html"><a href="introR.html#doing-calculations-using-variables"><i class="fa fa-check"></i><b>2.6.2</b> Doing calculations using variables</a></li>
<li class="chapter" data-level="2.6.3" data-path="introR.html"><a href="introR.html#rules-and-conventions-for-naming-variables"><i class="fa fa-check"></i><b>2.6.3</b> Rules and conventions for naming variables</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="introR.html"><a href="introR.html#usingfunctions"><i class="fa fa-check"></i><b>2.7</b> Using functions to do calculations</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="introR.html"><a href="introR.html#functionarguments"><i class="fa fa-check"></i><b>2.7.1</b> Function arguments, their names and their defaults</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="introR.html"><a href="introR.html#RStudio1"><i class="fa fa-check"></i><b>2.8</b> Letting RStudio help you with your commands</a>
<ul>
<li class="chapter" data-level="2.8.1" data-path="introR.html"><a href="introR.html#autocomplete-using-tab"><i class="fa fa-check"></i><b>2.8.1</b> Autocomplete using “tab”</a></li>
<li class="chapter" data-level="2.8.2" data-path="introR.html"><a href="introR.html#browsing-your-command-history"><i class="fa fa-check"></i><b>2.8.2</b> Browsing your command history</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="introR.html"><a href="introR.html#vectors"><i class="fa fa-check"></i><b>2.9</b> Storing many numbers as a vector</a>
<ul>
<li class="chapter" data-level="2.9.1" data-path="introR.html"><a href="introR.html#creating-a-vector"><i class="fa fa-check"></i><b>2.9.1</b> Creating a vector</a></li>
<li class="chapter" data-level="2.9.2" data-path="introR.html"><a href="introR.html#a-handy-digression"><i class="fa fa-check"></i><b>2.9.2</b> A handy digression</a></li>
<li class="chapter" data-level="2.9.3" data-path="introR.html"><a href="introR.html#vectorsubset"><i class="fa fa-check"></i><b>2.9.3</b> Getting information out of vectors</a></li>
<li class="chapter" data-level="2.9.4" data-path="introR.html"><a href="introR.html#altering-the-elements-of-a-vector"><i class="fa fa-check"></i><b>2.9.4</b> Altering the elements of a vector</a></li>
<li class="chapter" data-level="2.9.5" data-path="introR.html"><a href="introR.html#veclength"><i class="fa fa-check"></i><b>2.9.5</b> Useful things to know about vectors</a></li>
</ul></li>
<li class="chapter" data-level="2.10" data-path="introR.html"><a href="introR.html#text"><i class="fa fa-check"></i><b>2.10</b> Storing text data</a>
<ul>
<li class="chapter" data-level="2.10.1" data-path="introR.html"><a href="introR.html#simpletext"><i class="fa fa-check"></i><b>2.10.1</b> Working with text</a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="introR.html"><a href="introR.html#logicals"><i class="fa fa-check"></i><b>2.11</b> Storing “true or false” data</a>
<ul>
<li class="chapter" data-level="2.11.1" data-path="introR.html"><a href="introR.html#assessing-mathematical-truths"><i class="fa fa-check"></i><b>2.11.1</b> Assessing mathematical truths</a></li>
<li class="chapter" data-level="2.11.2" data-path="introR.html"><a href="introR.html#logical-operations"><i class="fa fa-check"></i><b>2.11.2</b> Logical operations</a></li>
<li class="chapter" data-level="2.11.3" data-path="introR.html"><a href="introR.html#storing-and-using-logical-data"><i class="fa fa-check"></i><b>2.11.3</b> Storing and using logical data</a></li>
<li class="chapter" data-level="2.11.4" data-path="introR.html"><a href="introR.html#vectors-of-logicals"><i class="fa fa-check"></i><b>2.11.4</b> Vectors of logicals</a></li>
<li class="chapter" data-level="2.11.5" data-path="introR.html"><a href="introR.html#logictext"><i class="fa fa-check"></i><b>2.11.5</b> Applying logical operation to text</a></li>
</ul></li>
<li class="chapter" data-level="2.12" data-path="introR.html"><a href="introR.html#indexing"><i class="fa fa-check"></i><b>2.12</b> Indexing vectors</a>
<ul>
<li class="chapter" data-level="2.12.1" data-path="introR.html"><a href="introR.html#extracting-multiple-elements"><i class="fa fa-check"></i><b>2.12.1</b> Extracting multiple elements</a></li>
<li class="chapter" data-level="2.12.2" data-path="introR.html"><a href="introR.html#logical-indexing"><i class="fa fa-check"></i><b>2.12.2</b> Logical indexing</a></li>
</ul></li>
<li class="chapter" data-level="2.13" data-path="introR.html"><a href="introR.html#quitting-r"><i class="fa fa-check"></i><b>2.13</b> Quitting R</a></li>
<li class="chapter" data-level="2.14" data-path="introR.html"><a href="introR.html#summary"><i class="fa fa-check"></i><b>2.14</b> Summary</a></li>
<li class="chapter" data-level="2.15" data-path="introR.html"><a href="introR.html#mechanics"><i class="fa fa-check"></i><b>2.15</b> Additional R concepts</a></li>
<li class="chapter" data-level="2.16" data-path="introR.html"><a href="introR.html#comments"><i class="fa fa-check"></i><b>2.16</b> Using comments</a></li>
<li class="chapter" data-level="2.17" data-path="introR.html"><a href="introR.html#packageinstall"><i class="fa fa-check"></i><b>2.17</b> Installing and loading packages</a>
<ul>
<li class="chapter" data-level="2.17.1" data-path="introR.html"><a href="introR.html#the-package-panel-in-rstudio"><i class="fa fa-check"></i><b>2.17.1</b> The package panel in RStudio</a></li>
<li class="chapter" data-level="2.17.2" data-path="introR.html"><a href="introR.html#packageload"><i class="fa fa-check"></i><b>2.17.2</b> Loading a package</a></li>
<li class="chapter" data-level="2.17.3" data-path="introR.html"><a href="introR.html#packageunload"><i class="fa fa-check"></i><b>2.17.3</b> Unloading a package</a></li>
<li class="chapter" data-level="2.17.4" data-path="introR.html"><a href="introR.html#a-few-extra-comments"><i class="fa fa-check"></i><b>2.17.4</b> A few extra comments</a></li>
<li class="chapter" data-level="2.17.5" data-path="introR.html"><a href="introR.html#downloading-new-packages"><i class="fa fa-check"></i><b>2.17.5</b> Downloading new packages</a></li>
<li class="chapter" data-level="2.17.6" data-path="introR.html"><a href="introR.html#updating-r-and-r-packages"><i class="fa fa-check"></i><b>2.17.6</b> Updating R and R packages</a></li>
<li class="chapter" data-level="2.17.7" data-path="introR.html"><a href="introR.html#what-packages-does-this-book-use"><i class="fa fa-check"></i><b>2.17.7</b> What packages does this book use?</a></li>
</ul></li>
<li class="chapter" data-level="2.18" data-path="introR.html"><a href="introR.html#workspace"><i class="fa fa-check"></i><b>2.18</b> Managing the workspace</a>
<ul>
<li class="chapter" data-level="2.18.1" data-path="introR.html"><a href="introR.html#listing-the-contents-of-the-workspace"><i class="fa fa-check"></i><b>2.18.1</b> Listing the contents of the workspace</a></li>
<li class="chapter" data-level="2.18.2" data-path="introR.html"><a href="introR.html#removing-variables-from-the-workspace"><i class="fa fa-check"></i><b>2.18.2</b> Removing variables from the workspace</a></li>
</ul></li>
<li class="chapter" data-level="2.19" data-path="introR.html"><a href="introR.html#navigation"><i class="fa fa-check"></i><b>2.19</b> Navigating the file system</a>
<ul>
<li class="chapter" data-level="2.19.1" data-path="introR.html"><a href="introR.html#filesystem"><i class="fa fa-check"></i><b>2.19.1</b> The file system itself</a></li>
<li class="chapter" data-level="2.19.2" data-path="introR.html"><a href="introR.html#navigationR"><i class="fa fa-check"></i><b>2.19.2</b> Navigating the file system using the R console</a></li>
<li class="chapter" data-level="2.19.3" data-path="introR.html"><a href="introR.html#why-do-the-windows-paths-use-the-wrong-slash"><i class="fa fa-check"></i><b>2.19.3</b> Why do the Windows paths use the wrong slash?</a></li>
<li class="chapter" data-level="2.19.4" data-path="introR.html"><a href="introR.html#nav3"><i class="fa fa-check"></i><b>2.19.4</b> Navigating the file system using the RStudio file panel</a></li>
</ul></li>
<li class="chapter" data-level="2.20" data-path="introR.html"><a href="introR.html#load"><i class="fa fa-check"></i><b>2.20</b> Loading and saving data</a>
<ul>
<li class="chapter" data-level="2.20.1" data-path="introR.html"><a href="introR.html#loading-workspace-files-using-r"><i class="fa fa-check"></i><b>2.20.1</b> Loading workspace files using R</a></li>
<li class="chapter" data-level="2.20.2" data-path="introR.html"><a href="introR.html#loading-workspace-files-using-rstudio"><i class="fa fa-check"></i><b>2.20.2</b> Loading workspace files using RStudio</a></li>
<li class="chapter" data-level="2.20.3" data-path="introR.html"><a href="introR.html#loadingcsv"><i class="fa fa-check"></i><b>2.20.3</b> Importing data from CSV files using loadingcsv</a></li>
<li class="chapter" data-level="2.20.4" data-path="introR.html"><a href="introR.html#importing-data-from-csv-files-using-rstudio"><i class="fa fa-check"></i><b>2.20.4</b> Importing data from CSV files using RStudio</a></li>
<li class="chapter" data-level="2.20.5" data-path="introR.html"><a href="introR.html#saving-a-workspace-file-using-save"><i class="fa fa-check"></i><b>2.20.5</b> Saving a workspace file using <code>save</code></a></li>
<li class="chapter" data-level="2.20.6" data-path="introR.html"><a href="introR.html#save1"><i class="fa fa-check"></i><b>2.20.6</b> Saving a workspace file using RStudio</a></li>
<li class="chapter" data-level="2.20.7" data-path="introR.html"><a href="introR.html#other-things-you-might-want-to-save"><i class="fa fa-check"></i><b>2.20.7</b> Other things you might want to save</a></li>
</ul></li>
<li class="chapter" data-level="2.21" data-path="introR.html"><a href="introR.html#useful"><i class="fa fa-check"></i><b>2.21</b> Useful things to know about variables</a>
<ul>
<li class="chapter" data-level="2.21.1" data-path="introR.html"><a href="introR.html#specials"><i class="fa fa-check"></i><b>2.21.1</b> Special values</a></li>
<li class="chapter" data-level="2.21.2" data-path="introR.html"><a href="introR.html#names"><i class="fa fa-check"></i><b>2.21.2</b> Assigning names to vector elements</a></li>
<li class="chapter" data-level="2.21.3" data-path="introR.html"><a href="introR.html#variable-classes"><i class="fa fa-check"></i><b>2.21.3</b> Variable classes</a></li>
</ul></li>
<li class="chapter" data-level="2.22" data-path="introR.html"><a href="introR.html#factors"><i class="fa fa-check"></i><b>2.22</b> Factors</a>
<ul>
<li class="chapter" data-level="2.22.1" data-path="introR.html"><a href="introR.html#introducing-factors"><i class="fa fa-check"></i><b>2.22.1</b> Introducing factors</a></li>
<li class="chapter" data-level="2.22.2" data-path="introR.html"><a href="introR.html#labelling-the-factor-levels"><i class="fa fa-check"></i><b>2.22.2</b> Labelling the factor levels</a></li>
<li class="chapter" data-level="2.22.3" data-path="introR.html"><a href="introR.html#moving-on"><i class="fa fa-check"></i><b>2.22.3</b> Moving on…</a></li>
</ul></li>
<li class="chapter" data-level="2.23" data-path="introR.html"><a href="introR.html#dataframes"><i class="fa fa-check"></i><b>2.23</b> Data frames</a>
<ul>
<li class="chapter" data-level="2.23.1" data-path="introR.html"><a href="introR.html#introducing-data-frames"><i class="fa fa-check"></i><b>2.23.1</b> Introducing data frames</a></li>
<li class="chapter" data-level="2.23.2" data-path="introR.html"><a href="introR.html#pulling-out-the-contents-of-the-data-frame-using"><i class="fa fa-check"></i><b>2.23.2</b> Pulling out the contents of the data frame using <code>$</code></a></li>
<li class="chapter" data-level="2.23.3" data-path="introR.html"><a href="introR.html#getting-information-about-a-data-frame"><i class="fa fa-check"></i><b>2.23.3</b> Getting information about a data frame</a></li>
<li class="chapter" data-level="2.23.4" data-path="introR.html"><a href="introR.html#looking-for-more-on-data-frames"><i class="fa fa-check"></i><b>2.23.4</b> Looking for more on data frames?</a></li>
</ul></li>
<li class="chapter" data-level="2.24" data-path="introR.html"><a href="introR.html#lists"><i class="fa fa-check"></i><b>2.24</b> Lists</a></li>
<li class="chapter" data-level="2.25" data-path="introR.html"><a href="introR.html#formulas"><i class="fa fa-check"></i><b>2.25</b> Formulas</a></li>
<li class="chapter" data-level="2.26" data-path="introR.html"><a href="introR.html#generics"><i class="fa fa-check"></i><b>2.26</b> Generic functions</a></li>
<li class="chapter" data-level="2.27" data-path="introR.html"><a href="introR.html#help"><i class="fa fa-check"></i><b>2.27</b> Getting help</a>
<ul>
<li class="chapter" data-level="2.27.1" data-path="introR.html"><a href="introR.html#how-to-read-the-help-documentation"><i class="fa fa-check"></i><b>2.27.1</b> How to read the help documentation</a></li>
<li class="chapter" data-level="2.27.2" data-path="introR.html"><a href="introR.html#other-resources"><i class="fa fa-check"></i><b>2.27.2</b> Other resources</a></li>
</ul></li>
<li class="chapter" data-level="2.28" data-path="introR.html"><a href="introR.html#summary-1"><i class="fa fa-check"></i><b>2.28</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="descriptives.html"><a href="descriptives.html"><i class="fa fa-check"></i><b>3</b> Descriptive statistics</a>
<ul>
<li class="chapter" data-level="3.1" data-path="descriptives.html"><a href="descriptives.html#videos-1"><i class="fa fa-check"></i><b>3.1</b> Videos</a></li>
<li class="chapter" data-level="3.2" data-path="descriptives.html"><a href="descriptives.html#introduction-2"><i class="fa fa-check"></i><b>3.2</b> Introduction</a></li>
<li class="chapter" data-level="3.3" data-path="descriptives.html"><a href="descriptives.html#centraltendency"><i class="fa fa-check"></i><b>3.3</b> Measures of central tendency</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="descriptives.html"><a href="descriptives.html#mean"><i class="fa fa-check"></i><b>3.3.1</b> The mean</a></li>
<li class="chapter" data-level="3.3.2" data-path="descriptives.html"><a href="descriptives.html#calculating-the-mean-in-r"><i class="fa fa-check"></i><b>3.3.2</b> Calculating the mean in R</a></li>
<li class="chapter" data-level="3.3.3" data-path="descriptives.html"><a href="descriptives.html#median"><i class="fa fa-check"></i><b>3.3.3</b> The median</a></li>
<li class="chapter" data-level="3.3.4" data-path="descriptives.html"><a href="descriptives.html#mean-or-median-whats-the-difference"><i class="fa fa-check"></i><b>3.3.4</b> Mean or median? What’s the difference?</a></li>
<li class="chapter" data-level="3.3.5" data-path="descriptives.html"><a href="descriptives.html#housingpriceexample"><i class="fa fa-check"></i><b>3.3.5</b> A real life example</a></li>
<li class="chapter" data-level="3.3.6" data-path="descriptives.html"><a href="descriptives.html#trimmedmean"><i class="fa fa-check"></i><b>3.3.6</b> Trimmed mean</a></li>
<li class="chapter" data-level="3.3.7" data-path="descriptives.html"><a href="descriptives.html#mode"><i class="fa fa-check"></i><b>3.3.7</b> Mode</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="descriptives.html"><a href="descriptives.html#var"><i class="fa fa-check"></i><b>3.4</b> Measures of variability</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="descriptives.html"><a href="descriptives.html#range"><i class="fa fa-check"></i><b>3.4.1</b> Range</a></li>
<li class="chapter" data-level="3.4.2" data-path="descriptives.html"><a href="descriptives.html#interquartile-range"><i class="fa fa-check"></i><b>3.4.2</b> Interquartile range</a></li>
<li class="chapter" data-level="3.4.3" data-path="descriptives.html"><a href="descriptives.html#aad"><i class="fa fa-check"></i><b>3.4.3</b> Mean absolute deviation</a></li>
<li class="chapter" data-level="3.4.4" data-path="descriptives.html"><a href="descriptives.html#variance"><i class="fa fa-check"></i><b>3.4.4</b> Variance</a></li>
<li class="chapter" data-level="3.4.5" data-path="descriptives.html"><a href="descriptives.html#sd"><i class="fa fa-check"></i><b>3.4.5</b> Standard deviation</a></li>
<li class="chapter" data-level="3.4.6" data-path="descriptives.html"><a href="descriptives.html#mad"><i class="fa fa-check"></i><b>3.4.6</b> Median absolute deviation</a></li>
<li class="chapter" data-level="3.4.7" data-path="descriptives.html"><a href="descriptives.html#which-measure-to-use"><i class="fa fa-check"></i><b>3.4.7</b> Which measure to use?</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="descriptives.html"><a href="descriptives.html#skewandkurtosis"><i class="fa fa-check"></i><b>3.5</b> Skew and kurtosis</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="descriptives.html"><a href="descriptives.html#more-detail-on-skewness-measures"><i class="fa fa-check"></i><b>3.5.1</b> More detail on skewness measures</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="descriptives.html"><a href="descriptives.html#descriptive-summary"><i class="fa fa-check"></i><b>3.6</b> Getting an overall summary of a variable</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="descriptives.html"><a href="descriptives.html#summarising-a-variable"><i class="fa fa-check"></i><b>3.6.1</b> “Summarising” a variable</a></li>
<li class="chapter" data-level="3.6.2" data-path="descriptives.html"><a href="descriptives.html#summarising-a-data-frame"><i class="fa fa-check"></i><b>3.6.2</b> “Summarising” a data frame</a></li>
<li class="chapter" data-level="3.6.3" data-path="descriptives.html"><a href="descriptives.html#describing-a-data-frame"><i class="fa fa-check"></i><b>3.6.3</b> “Describing” a data frame</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="descriptives.html"><a href="descriptives.html#groupdescriptives"><i class="fa fa-check"></i><b>3.7</b> Descriptive statistics separately for each group</a></li>
<li class="chapter" data-level="3.8" data-path="descriptives.html"><a href="descriptives.html#good-descriptive-statistics-are-descriptive"><i class="fa fa-check"></i><b>3.8</b> Good descriptive statistics are descriptive!</a></li>
<li class="chapter" data-level="3.9" data-path="descriptives.html"><a href="descriptives.html#graphics"><i class="fa fa-check"></i><b>3.9</b> Drawing graphs</a>
<ul>
<li class="chapter" data-level="3.9.1" data-path="descriptives.html"><a href="descriptives.html#introplotting"><i class="fa fa-check"></i><b>3.9.1</b> An introduction to plotting</a></li>
<li class="chapter" data-level="3.9.2" data-path="descriptives.html"><a href="descriptives.html#hist"><i class="fa fa-check"></i><b>3.9.2</b> Histograms</a></li>
<li class="chapter" data-level="3.9.3" data-path="descriptives.html"><a href="descriptives.html#boxplots"><i class="fa fa-check"></i><b>3.9.3</b> Boxplots</a></li>
<li class="chapter" data-level="3.9.4" data-path="descriptives.html"><a href="descriptives.html#bargraph"><i class="fa fa-check"></i><b>3.9.4</b> Bar graphs</a></li>
<li class="chapter" data-level="3.9.5" data-path="descriptives.html"><a href="descriptives.html#saveimage"><i class="fa fa-check"></i><b>3.9.5</b> Saving image files using R and Rstudio</a></li>
<li class="chapter" data-level="3.9.6" data-path="descriptives.html"><a href="descriptives.html#summary-2"><i class="fa fa-check"></i><b>3.9.6</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html"><i class="fa fa-check"></i><b>4</b> Inferential statistics: The Central Limit Theorem</a>
<ul>
<li class="chapter" data-level="4.1" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#videos-2"><i class="fa fa-check"></i><b>4.1</b> Videos</a></li>
<li class="chapter" data-level="4.2" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#estimation"><i class="fa fa-check"></i><b>4.2</b> Introduction</a></li>
<li class="chapter" data-level="4.3" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#probstats"><i class="fa fa-check"></i><b>4.3</b> How are probability and statistics different?</a></li>
<li class="chapter" data-level="4.4" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#probmeaning"><i class="fa fa-check"></i><b>4.4</b> What does probability mean?</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#the-frequentist-view"><i class="fa fa-check"></i><b>4.4.1</b> The frequentist view</a></li>
<li class="chapter" data-level="4.4.2" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#the-bayesian-view"><i class="fa fa-check"></i><b>4.4.2</b> The Bayesian view</a></li>
<li class="chapter" data-level="4.4.3" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#whats-the-difference-and-who-is-right"><i class="fa fa-check"></i><b>4.4.3</b> What’s the difference? And who is right?</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#srs"><i class="fa fa-check"></i><b>4.5</b> Samples, populations and sampling</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#pop"><i class="fa fa-check"></i><b>4.5.1</b> Defining a population</a></li>
<li class="chapter" data-level="4.5.2" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#simple-random-samples"><i class="fa fa-check"></i><b>4.5.2</b> Simple random samples</a></li>
<li class="chapter" data-level="4.5.3" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#most-samples-are-not-simple-random-samples"><i class="fa fa-check"></i><b>4.5.3</b> Most samples are not simple random samples</a></li>
<li class="chapter" data-level="4.5.4" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#how-much-does-it-matter-if-you-dont-have-a-simple-random-sample"><i class="fa fa-check"></i><b>4.5.4</b> How much does it matter if you don’t have a simple random sample?</a></li>
<li class="chapter" data-level="4.5.5" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#population-parameters-and-sample-statistics"><i class="fa fa-check"></i><b>4.5.5</b> Population parameters and sample statistics</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#lawlargenumbers"><i class="fa fa-check"></i><b>4.6</b> The law of large numbers</a></li>
<li class="chapter" data-level="4.7" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#samplesandclt"><i class="fa fa-check"></i><b>4.7</b> Sampling distributions and the central limit theorem</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#samplingdists"><i class="fa fa-check"></i><b>4.7.1</b> Sampling distribution of the mean</a></li>
<li class="chapter" data-level="4.7.2" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#sample-size-and-population-size"><i class="fa fa-check"></i><b>4.7.2</b> Sample size and population size</a></li>
<li class="chapter" data-level="4.7.3" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#sampling-error"><i class="fa fa-check"></i><b>4.7.3</b> Sampling error</a></li>
<li class="chapter" data-level="4.7.4" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#another-sampling-distribution"><i class="fa fa-check"></i><b>4.7.4</b> Another sampling distribution</a></li>
<li class="chapter" data-level="4.7.5" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#defining-the-central-limit-theorem"><i class="fa fa-check"></i><b>4.7.5</b> Defining the central limit theorem</a></li>
<li class="chapter" data-level="4.7.6" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#more-on-standard-error"><i class="fa fa-check"></i><b>4.7.6</b> More on standard error</a></li>
<li class="chapter" data-level="4.7.7" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#the-sampling-distribution-tells-us-about-the-probability-of-sample-means"><i class="fa fa-check"></i><b>4.7.7</b> The sampling distribution tells us about the probability of sample means</a></li>
<li class="chapter" data-level="4.7.8" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#sampling-distributions-exist-for-any-sample-statistic"><i class="fa fa-check"></i><b>4.7.8</b> Sampling distributions exist for any sample statistic!</a></li>
<li class="chapter" data-level="4.7.9" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#clt"><i class="fa fa-check"></i><b>4.7.9</b> The central limit theorem</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#pointestimates"><i class="fa fa-check"></i><b>4.8</b> Estimating population parameters</a>
<ul>
<li class="chapter" data-level="4.8.1" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#estimating-the-population-mean"><i class="fa fa-check"></i><b>4.8.1</b> Estimating the population mean</a></li>
<li class="chapter" data-level="4.8.2" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#estimating-the-population-standard-deviation"><i class="fa fa-check"></i><b>4.8.2</b> Estimating the population standard deviation</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#ci"><i class="fa fa-check"></i><b>4.9</b> Estimating a confidence interval</a>
<ul>
<li class="chapter" data-level="4.9.1" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#a-slight-mistake-in-the-formula"><i class="fa fa-check"></i><b>4.9.1</b> A slight mistake in the formula</a></li>
<li class="chapter" data-level="4.9.2" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#interpreting-a-confidence-interval"><i class="fa fa-check"></i><b>4.9.2</b> Interpreting a confidence interval</a></li>
<li class="chapter" data-level="4.9.3" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#calculating-confidence-intervals-in-r"><i class="fa fa-check"></i><b>4.9.3</b> Calculating confidence intervals in R</a></li>
<li class="chapter" data-level="4.9.4" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#ciplots"><i class="fa fa-check"></i><b>4.9.4</b> Plotting confidence intervals in R</a></li>
</ul></li>
<li class="chapter" data-level="4.10" data-path="inferential-statistics-the-central-limit-theorem.html"><a href="inferential-statistics-the-central-limit-theorem.html#summary-3"><i class="fa fa-check"></i><b>4.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="hypothesistesting.html"><a href="hypothesistesting.html"><i class="fa fa-check"></i><b>5</b> Hypothesis testing</a>
<ul>
<li class="chapter" data-level="5.1" data-path="hypothesistesting.html"><a href="hypothesistesting.html#videos-3"><i class="fa fa-check"></i><b>5.1</b> Videos</a></li>
<li class="chapter" data-level="5.2" data-path="hypothesistesting.html"><a href="hypothesistesting.html#introduction-3"><i class="fa fa-check"></i><b>5.2</b> Introduction</a></li>
<li class="chapter" data-level="5.3" data-path="hypothesistesting.html"><a href="hypothesistesting.html#hypotheses"><i class="fa fa-check"></i><b>5.3</b> A menagerie of hypotheses</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="hypothesistesting.html"><a href="hypothesistesting.html#research-hypotheses-versus-statistical-hypotheses"><i class="fa fa-check"></i><b>5.3.1</b> Research hypotheses versus statistical hypotheses</a></li>
<li class="chapter" data-level="5.3.2" data-path="hypothesistesting.html"><a href="hypothesistesting.html#null-hypotheses-and-alternative-hypotheses"><i class="fa fa-check"></i><b>5.3.2</b> Null hypotheses and alternative hypotheses</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="hypothesistesting.html"><a href="hypothesistesting.html#errortypes"><i class="fa fa-check"></i><b>5.4</b> Two types of errors</a></li>
<li class="chapter" data-level="5.5" data-path="hypothesistesting.html"><a href="hypothesistesting.html#teststatistics"><i class="fa fa-check"></i><b>5.5</b> Test statistics and sampling distributions</a></li>
<li class="chapter" data-level="5.6" data-path="hypothesistesting.html"><a href="hypothesistesting.html#decisionmaking"><i class="fa fa-check"></i><b>5.6</b> Making decisions</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="hypothesistesting.html"><a href="hypothesistesting.html#critical-regions-and-critical-values"><i class="fa fa-check"></i><b>5.6.1</b> Critical regions and critical values</a></li>
<li class="chapter" data-level="5.6.2" data-path="hypothesistesting.html"><a href="hypothesistesting.html#a-note-on-statistical-significance"><i class="fa fa-check"></i><b>5.6.2</b> A note on statistical “significance”</a></li>
<li class="chapter" data-level="5.6.3" data-path="hypothesistesting.html"><a href="hypothesistesting.html#onesidedtests"><i class="fa fa-check"></i><b>5.6.3</b> The difference between one sided and two sided tests</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="hypothesistesting.html"><a href="hypothesistesting.html#pvalue"><i class="fa fa-check"></i><b>5.7</b> The <span class="math inline">\(p\)</span> value of a test</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="hypothesistesting.html"><a href="hypothesistesting.html#a-softer-view-of-decision-making"><i class="fa fa-check"></i><b>5.7.1</b> A softer view of decision making</a></li>
<li class="chapter" data-level="5.7.2" data-path="hypothesistesting.html"><a href="hypothesistesting.html#the-probability-of-extreme-data"><i class="fa fa-check"></i><b>5.7.2</b> The probability of extreme data</a></li>
<li class="chapter" data-level="5.7.3" data-path="hypothesistesting.html"><a href="hypothesistesting.html#a-common-mistake"><i class="fa fa-check"></i><b>5.7.3</b> A common mistake</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="hypothesistesting.html"><a href="hypothesistesting.html#writeup"><i class="fa fa-check"></i><b>5.8</b> Reporting the results of a hypothesis test</a>
<ul>
<li class="chapter" data-level="5.8.1" data-path="hypothesistesting.html"><a href="hypothesistesting.html#the-issue"><i class="fa fa-check"></i><b>5.8.1</b> The issue</a></li>
<li class="chapter" data-level="5.8.2" data-path="hypothesistesting.html"><a href="hypothesistesting.html#two-proposed-solutions"><i class="fa fa-check"></i><b>5.8.2</b> Two proposed solutions</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="hypothesistesting.html"><a href="hypothesistesting.html#running-the-hypothesis-test-in-practice"><i class="fa fa-check"></i><b>5.9</b> Running the hypothesis test in practice</a></li>
<li class="chapter" data-level="5.10" data-path="hypothesistesting.html"><a href="hypothesistesting.html#effectsize"><i class="fa fa-check"></i><b>5.10</b> Effect size, sample size and power</a>
<ul>
<li class="chapter" data-level="5.10.1" data-path="hypothesistesting.html"><a href="hypothesistesting.html#the-power-function"><i class="fa fa-check"></i><b>5.10.1</b> The power function</a></li>
<li class="chapter" data-level="5.10.2" data-path="hypothesistesting.html"><a href="hypothesistesting.html#effect-size"><i class="fa fa-check"></i><b>5.10.2</b> Effect size</a></li>
<li class="chapter" data-level="5.10.3" data-path="hypothesistesting.html"><a href="hypothesistesting.html#increasing-the-power-of-your-study"><i class="fa fa-check"></i><b>5.10.3</b> Increasing the power of your study</a></li>
</ul></li>
<li class="chapter" data-level="5.11" data-path="hypothesistesting.html"><a href="hypothesistesting.html#nhstmess"><i class="fa fa-check"></i><b>5.11</b> Some issues to consider</a>
<ul>
<li class="chapter" data-level="5.11.1" data-path="hypothesistesting.html"><a href="hypothesistesting.html#neyman-versus-fisher"><i class="fa fa-check"></i><b>5.11.1</b> Neyman versus Fisher</a></li>
<li class="chapter" data-level="5.11.2" data-path="hypothesistesting.html"><a href="hypothesistesting.html#bayesians-versus-frequentists"><i class="fa fa-check"></i><b>5.11.2</b> Bayesians versus frequentists</a></li>
<li class="chapter" data-level="5.11.3" data-path="hypothesistesting.html"><a href="hypothesistesting.html#traps"><i class="fa fa-check"></i><b>5.11.3</b> Traps</a></li>
</ul></li>
<li class="chapter" data-level="5.12" data-path="hypothesistesting.html"><a href="hypothesistesting.html#summary-4"><i class="fa fa-check"></i><b>5.12</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="issues-in-hypothesis-testing.html"><a href="issues-in-hypothesis-testing.html"><i class="fa fa-check"></i><b>6</b> Issues in Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="6.1" data-path="issues-in-hypothesis-testing.html"><a href="issues-in-hypothesis-testing.html#videos-4"><i class="fa fa-check"></i><b>6.1</b> Videos</a></li>
<li class="chapter" data-level="6.2" data-path="issues-in-hypothesis-testing.html"><a href="issues-in-hypothesis-testing.html#introduction-4"><i class="fa fa-check"></i><b>6.2</b> Introduction</a></li>
<li class="chapter" data-level="6.3" data-path="issues-in-hypothesis-testing.html"><a href="issues-in-hypothesis-testing.html#the-researcher-affects-nhst-outcomes"><i class="fa fa-check"></i><b>6.3</b> The researcher affects NHST outcomes</a></li>
<li class="chapter" data-level="6.4" data-path="issues-in-hypothesis-testing.html"><a href="issues-in-hypothesis-testing.html#nhst-misunderstandings"><i class="fa fa-check"></i><b>6.4</b> NHST Misunderstandings</a></li>
<li class="chapter" data-level="6.5" data-path="issues-in-hypothesis-testing.html"><a href="issues-in-hypothesis-testing.html#nhst-issues"><i class="fa fa-check"></i><b>6.5</b> NHST Issues</a></li>
<li class="chapter" data-level="6.6" data-path="issues-in-hypothesis-testing.html"><a href="issues-in-hypothesis-testing.html#conclusions"><i class="fa fa-check"></i><b>6.6</b> Conclusions</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="data-cleaning-and-missing-values-analysis.html"><a href="data-cleaning-and-missing-values-analysis.html"><i class="fa fa-check"></i><b>7</b> Data Cleaning and Missing Values Analysis</a>
<ul>
<li class="chapter" data-level="7.1" data-path="data-cleaning-and-missing-values-analysis.html"><a href="data-cleaning-and-missing-values-analysis.html#videos-5"><i class="fa fa-check"></i><b>7.1</b> Videos</a></li>
<li class="chapter" data-level="7.2" data-path="data-cleaning-and-missing-values-analysis.html"><a href="data-cleaning-and-missing-values-analysis.html#introduction-dealing-with-the-unexpected"><i class="fa fa-check"></i><b>7.2</b> Introduction: Dealing with the Unexpected</a></li>
<li class="chapter" data-level="7.3" data-path="data-cleaning-and-missing-values-analysis.html"><a href="data-cleaning-and-missing-values-analysis.html#data-cleaning"><i class="fa fa-check"></i><b>7.3</b> Data Cleaning</a></li>
<li class="chapter" data-level="7.4" data-path="data-cleaning-and-missing-values-analysis.html"><a href="data-cleaning-and-missing-values-analysis.html#a-general-plan-for-data-cleaning"><i class="fa fa-check"></i><b>7.4</b> A General Plan for Data Cleaning</a></li>
<li class="chapter" data-level="7.5" data-path="data-cleaning-and-missing-values-analysis.html"><a href="data-cleaning-and-missing-values-analysis.html#step-0.-design-your-research-to-mimimize-data-problems"><i class="fa fa-check"></i><b>7.5</b> Step 0. Design your Research to Mimimize Data Problems</a></li>
<li class="chapter" data-level="7.6" data-path="data-cleaning-and-missing-values-analysis.html"><a href="data-cleaning-and-missing-values-analysis.html#step-1.-examine-your-data"><i class="fa fa-check"></i><b>7.6</b> Step 1. Examine Your Data</a>
<ul>
<li class="chapter" data-level="7.6.1" data-path="data-cleaning-and-missing-values-analysis.html"><a href="data-cleaning-and-missing-values-analysis.html#sort"><i class="fa fa-check"></i><b>7.6.1</b> Sorting, flipping and merging data</a></li>
<li class="chapter" data-level="7.6.2" data-path="data-cleaning-and-missing-values-analysis.html"><a href="data-cleaning-and-missing-values-analysis.html#binding-vectors-together"><i class="fa fa-check"></i><b>7.6.2</b> Binding vectors together</a></li>
<li class="chapter" data-level="7.6.3" data-path="data-cleaning-and-missing-values-analysis.html"><a href="data-cleaning-and-missing-values-analysis.html#reshape"><i class="fa fa-check"></i><b>7.6.3</b> Reshaping a data frame</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="data-cleaning-and-missing-values-analysis.html"><a href="data-cleaning-and-missing-values-analysis.html#step-2.-outlier-analysis"><i class="fa fa-check"></i><b>7.7</b> Step 2. Outlier Analysis</a></li>
<li class="chapter" data-level="7.8" data-path="data-cleaning-and-missing-values-analysis.html"><a href="data-cleaning-and-missing-values-analysis.html#step-3.-missing-values-analysis"><i class="fa fa-check"></i><b>7.8</b> Step 3. Missing values analysis</a>
<ul>
<li class="chapter" data-level="7.8.1" data-path="introR.html"><a href="introR.html#specials"><i class="fa fa-check"></i><b>7.8.1</b> Special values in R</a></li>
<li class="chapter" data-level="7.8.2" data-path="data-cleaning-and-missing-values-analysis.html"><a href="data-cleaning-and-missing-values-analysis.html#missing"><i class="fa fa-check"></i><b>7.8.2</b> Handling missing values in R</a></li>
<li class="chapter" data-level="7.8.3" data-path="data-cleaning-and-missing-values-analysis.html"><a href="data-cleaning-and-missing-values-analysis.html#why-values-are-missing-mcar-mar-and-mnar"><i class="fa fa-check"></i><b>7.8.3</b> Why values are missing: MCAR, MAR, and MNAR</a></li>
</ul></li>
<li class="chapter" data-level="7.9" data-path="data-cleaning-and-missing-values-analysis.html"><a href="data-cleaning-and-missing-values-analysis.html#step-4.-test-specific-assumption-checking"><i class="fa fa-check"></i><b>7.9</b> Step 4. Test-specific assumption checking</a></li>
<li class="chapter" data-level="7.10" data-path="data-cleaning-and-missing-values-analysis.html"><a href="data-cleaning-and-missing-values-analysis.html#communicate-results-of-data-cleaning-in-apa-style"><i class="fa fa-check"></i><b>7.10</b> Communicate results of data cleaning in APA style</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>8</b> Regression</a>
<ul>
<li class="chapter" data-level="8.1" data-path="regression.html"><a href="regression.html#videos-6"><i class="fa fa-check"></i><b>8.1</b> Videos</a></li>
<li class="chapter" data-level="8.2" data-path="regression.html"><a href="regression.html#introduction-5"><i class="fa fa-check"></i><b>8.2</b> Introduction</a></li>
<li class="chapter" data-level="8.3" data-path="regression.html"><a href="regression.html#the-general-linear-model-glm"><i class="fa fa-check"></i><b>8.3</b> The General Linear Model (GLM)</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="regression.html"><a href="regression.html#the-traditional-approach-two-kinds-of-parametric-statistical-tests"><i class="fa fa-check"></i><b>8.3.1</b> The Traditional approach: Two kinds of parametric statistical tests</a></li>
<li class="chapter" data-level="8.3.2" data-path="regression.html"><a href="regression.html#the-glm-approach"><i class="fa fa-check"></i><b>8.3.2</b> The GLM approach</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="regression.html"><a href="regression.html#correl"><i class="fa fa-check"></i><b>8.4</b> Correlations</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="regression.html"><a href="regression.html#the-data"><i class="fa fa-check"></i><b>8.4.1</b> The data</a></li>
<li class="chapter" data-level="8.4.2" data-path="regression.html"><a href="regression.html#the-strength-and-direction-of-a-relationship"><i class="fa fa-check"></i><b>8.4.2</b> The strength and direction of a relationship</a></li>
<li class="chapter" data-level="8.4.3" data-path="regression.html"><a href="regression.html#the-correlation-coefficient"><i class="fa fa-check"></i><b>8.4.3</b> The correlation coefficient</a></li>
<li class="chapter" data-level="8.4.4" data-path="regression.html"><a href="regression.html#calculating-correlations-in-r"><i class="fa fa-check"></i><b>8.4.4</b> Calculating correlations in R</a></li>
<li class="chapter" data-level="8.4.5" data-path="regression.html"><a href="regression.html#interpretingcorrelations"><i class="fa fa-check"></i><b>8.4.5</b> Interpreting a correlation</a></li>
<li class="chapter" data-level="8.4.6" data-path="regression.html"><a href="regression.html#spearmans-rank-correlations"><i class="fa fa-check"></i><b>8.4.6</b> Spearman’s rank correlations</a></li>
<li class="chapter" data-level="8.4.7" data-path="regression.html"><a href="regression.html#the-correlate-function"><i class="fa fa-check"></i><b>8.4.7</b> The <code>correlate()</code> function</a></li>
<li class="chapter" data-level="8.4.8" data-path="regression.html"><a href="regression.html#missing-values-in-pairwise-calculations-1"><i class="fa fa-check"></i><b>8.4.8</b> Missing values in pairwise calculations</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="regression.html"><a href="regression.html#introregression"><i class="fa fa-check"></i><b>8.5</b> Linear regression</a></li>
<li class="chapter" data-level="8.6" data-path="regression.html"><a href="regression.html#regressionestimation"><i class="fa fa-check"></i><b>8.6</b> Estimating a linear regression model</a>
<ul>
<li class="chapter" data-level="8.6.1" data-path="regression.html"><a href="regression.html#lm"><i class="fa fa-check"></i><b>8.6.1</b> Using the <code>lm()</code> function</a></li>
<li class="chapter" data-level="8.6.2" data-path="regression.html"><a href="regression.html#interpreting-the-estimated-model"><i class="fa fa-check"></i><b>8.6.2</b> Interpreting the estimated model</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="regression.html"><a href="regression.html#multipleregression"><i class="fa fa-check"></i><b>8.7</b> Multiple linear regression</a>
<ul>
<li class="chapter" data-level="8.7.1" data-path="regression.html"><a href="regression.html#doing-it-in-r"><i class="fa fa-check"></i><b>8.7.1</b> Doing it in R</a></li>
<li class="chapter" data-level="8.7.2" data-path="regression.html"><a href="regression.html#formula-for-the-general-case"><i class="fa fa-check"></i><b>8.7.2</b> Formula for the general case</a></li>
</ul></li>
<li class="chapter" data-level="8.8" data-path="regression.html"><a href="regression.html#r2"><i class="fa fa-check"></i><b>8.8</b> Quantifying the fit of the regression model</a>
<ul>
<li class="chapter" data-level="8.8.1" data-path="regression.html"><a href="regression.html#the-r2-value"><i class="fa fa-check"></i><b>8.8.1</b> The <span class="math inline">\(R^2\)</span> value</a></li>
<li class="chapter" data-level="8.8.2" data-path="regression.html"><a href="regression.html#the-relationship-between-regression-and-correlation"><i class="fa fa-check"></i><b>8.8.2</b> The relationship between regression and correlation</a></li>
<li class="chapter" data-level="8.8.3" data-path="regression.html"><a href="regression.html#the-adjusted-r2-value"><i class="fa fa-check"></i><b>8.8.3</b> The adjusted <span class="math inline">\(R^2\)</span> value</a></li>
</ul></li>
<li class="chapter" data-level="8.9" data-path="regression.html"><a href="regression.html#regressiontests"><i class="fa fa-check"></i><b>8.9</b> Hypothesis tests for regression models</a>
<ul>
<li class="chapter" data-level="8.9.1" data-path="regression.html"><a href="regression.html#testing-the-model-as-a-whole-the-omnibus-test"><i class="fa fa-check"></i><b>8.9.1</b> Testing the model as a whole: The omnibus test</a></li>
<li class="chapter" data-level="8.9.2" data-path="regression.html"><a href="regression.html#tests-for-individual-coefficients"><i class="fa fa-check"></i><b>8.9.2</b> Tests for individual coefficients</a></li>
<li class="chapter" data-level="8.9.3" data-path="regression.html"><a href="regression.html#regressionsummary"><i class="fa fa-check"></i><b>8.9.3</b> Running the hypothesis tests in R</a></li>
</ul></li>
<li class="chapter" data-level="8.10" data-path="regression.html"><a href="regression.html#corrhyp"><i class="fa fa-check"></i><b>8.10</b> Testing the significance of a correlation</a>
<ul>
<li class="chapter" data-level="8.10.1" data-path="regression.html"><a href="regression.html#hypothesis-tests-for-a-single-correlation"><i class="fa fa-check"></i><b>8.10.1</b> Hypothesis tests for a single correlation</a></li>
<li class="chapter" data-level="8.10.2" data-path="regression.html"><a href="regression.html#corrhyp2"><i class="fa fa-check"></i><b>8.10.2</b> Hypothesis tests for all pairwise correlations</a></li>
</ul></li>
<li class="chapter" data-level="8.11" data-path="regression.html"><a href="regression.html#regressioncoefs"><i class="fa fa-check"></i><b>8.11</b> Regarding regression coefficients</a>
<ul>
<li class="chapter" data-level="8.11.1" data-path="regression.html"><a href="regression.html#confidence-intervals-for-the-coefficients"><i class="fa fa-check"></i><b>8.11.1</b> Confidence intervals for the coefficients</a></li>
<li class="chapter" data-level="8.11.2" data-path="regression.html"><a href="regression.html#calculating-standardised-regression-coefficients"><i class="fa fa-check"></i><b>8.11.2</b> Calculating standardised regression coefficients</a></li>
</ul></li>
<li class="chapter" data-level="8.12" data-path="regression.html"><a href="regression.html#regressionassumptions"><i class="fa fa-check"></i><b>8.12</b> Assumptions of regression</a></li>
<li class="chapter" data-level="8.13" data-path="regression.html"><a href="regression.html#regressiondiagnostics"><i class="fa fa-check"></i><b>8.13</b> Model checking</a>
<ul>
<li class="chapter" data-level="8.13.1" data-path="regression.html"><a href="regression.html#three-kinds-of-residuals"><i class="fa fa-check"></i><b>8.13.1</b> Three kinds of residuals</a></li>
<li class="chapter" data-level="8.13.2" data-path="regression.html"><a href="regression.html#regressionoutliers"><i class="fa fa-check"></i><b>8.13.2</b> Three kinds of anomalous data</a></li>
<li class="chapter" data-level="8.13.3" data-path="regression.html"><a href="regression.html#regressionnormality"><i class="fa fa-check"></i><b>8.13.3</b> Checking the normality of the residuals</a></li>
<li class="chapter" data-level="8.13.4" data-path="regression.html"><a href="regression.html#regressionlinearity"><i class="fa fa-check"></i><b>8.13.4</b> Checking the linearity of the relationship</a></li>
<li class="chapter" data-level="8.13.5" data-path="regression.html"><a href="regression.html#regressionhomogeneity"><i class="fa fa-check"></i><b>8.13.5</b> Checking the homogeneity of variance</a></li>
<li class="chapter" data-level="8.13.6" data-path="regression.html"><a href="regression.html#regressioncollinearity"><i class="fa fa-check"></i><b>8.13.6</b> Checking for collinearity</a></li>
</ul></li>
<li class="chapter" data-level="8.14" data-path="regression.html"><a href="regression.html#modelselreg"><i class="fa fa-check"></i><b>8.14</b> Model selection</a>
<ul>
<li class="chapter" data-level="8.14.1" data-path="regression.html"><a href="regression.html#backward-elimination"><i class="fa fa-check"></i><b>8.14.1</b> Backward elimination</a></li>
<li class="chapter" data-level="8.14.2" data-path="regression.html"><a href="regression.html#forward-selection"><i class="fa fa-check"></i><b>8.14.2</b> Forward selection</a></li>
<li class="chapter" data-level="8.14.3" data-path="regression.html"><a href="regression.html#a-caveat"><i class="fa fa-check"></i><b>8.14.3</b> A caveat</a></li>
<li class="chapter" data-level="8.14.4" data-path="regression.html"><a href="regression.html#comparing-two-regression-models"><i class="fa fa-check"></i><b>8.14.4</b> Comparing two regression models</a></li>
</ul></li>
<li class="chapter" data-level="8.15" data-path="regression.html"><a href="regression.html#practical-issues-in-correlation-and-regression"><i class="fa fa-check"></i><b>8.15</b> Practical Issues in Correlation and Regression</a>
<ul>
<li class="chapter" data-level="8.15.1" data-path="regression.html"><a href="regression.html#correlation-is-not-causation"><i class="fa fa-check"></i><b>8.15.1</b> Correlation is not causation</a></li>
<li class="chapter" data-level="8.15.2" data-path="regression.html"><a href="regression.html#interpreting-nhst-in-big-data"><i class="fa fa-check"></i><b>8.15.2</b> Interpreting NHST in Big Data</a></li>
<li class="chapter" data-level="8.15.3" data-path="regression.html"><a href="regression.html#outliers"><i class="fa fa-check"></i><b>8.15.3</b> Outliers</a></li>
<li class="chapter" data-level="8.15.4" data-path="regression.html"><a href="regression.html#restriction-of-range"><i class="fa fa-check"></i><b>8.15.4</b> Restriction of Range</a></li>
<li class="chapter" data-level="8.15.5" data-path="regression.html"><a href="regression.html#regression-toward-the-mean"><i class="fa fa-check"></i><b>8.15.5</b> Regression Toward the Mean</a></li>
<li class="chapter" data-level="8.15.6" data-path="regression.html"><a href="regression.html#report-effect-size"><i class="fa fa-check"></i><b>8.15.6</b> Report Effect Size</a></li>
<li class="chapter" data-level="8.15.7" data-path="regression.html"><a href="regression.html#what-are-degrees-of-freedom-again"><i class="fa fa-check"></i><b>8.15.7</b> What are Degrees of Freedom, again?</a></li>
</ul></li>
<li class="chapter" data-level="8.16" data-path="regression.html"><a href="regression.html#summary-5"><i class="fa fa-check"></i><b>8.16</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="ttest.html"><a href="ttest.html"><i class="fa fa-check"></i><b>9</b> T-Tests</a>
<ul>
<li class="chapter" data-level="9.1" data-path="ttest.html"><a href="ttest.html#videos-7"><i class="fa fa-check"></i><b>9.1</b> Videos</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="ttest.html"><a href="ttest.html#t-tests"><i class="fa fa-check"></i><b>9.1.1</b> T-Tests</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="ttest.html"><a href="ttest.html#groupcomparisonscompared"><i class="fa fa-check"></i><b>9.2</b> Comparison of tests that compare groups</a></li>
<li class="chapter" data-level="9.3" data-path="ttest.html"><a href="ttest.html#introduction-6"><i class="fa fa-check"></i><b>9.3</b> Introduction</a></li>
<li class="chapter" data-level="9.4" data-path="ttest.html"><a href="ttest.html#onesamplettest"><i class="fa fa-check"></i><b>9.4</b> The one-sample <span class="math inline">\(t\)</span>-test</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="ttest.html"><a href="ttest.html#introducing-the-t-test"><i class="fa fa-check"></i><b>9.4.1</b> Introducing the <span class="math inline">\(t\)</span>-test</a></li>
<li class="chapter" data-level="9.4.2" data-path="ttest.html"><a href="ttest.html#doing-the-test-in-r"><i class="fa fa-check"></i><b>9.4.2</b> Doing the test in R</a></li>
<li class="chapter" data-level="9.4.3" data-path="ttest.html"><a href="ttest.html#ttestoneassumptions"><i class="fa fa-check"></i><b>9.4.3</b> Assumptions of the one sample <span class="math inline">\(t\)</span>-test</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="ttest.html"><a href="ttest.html#studentttest"><i class="fa fa-check"></i><b>9.5</b> The independent samples <span class="math inline">\(t\)</span>-test (Student test)</a>
<ul>
<li class="chapter" data-level="9.5.1" data-path="ttest.html"><a href="ttest.html#the-data-1"><i class="fa fa-check"></i><b>9.5.1</b> The data</a></li>
<li class="chapter" data-level="9.5.2" data-path="ttest.html"><a href="ttest.html#introducing-the-test"><i class="fa fa-check"></i><b>9.5.2</b> Introducing the test</a></li>
<li class="chapter" data-level="9.5.3" data-path="ttest.html"><a href="ttest.html#a-pooled-estimate-of-the-standard-deviation"><i class="fa fa-check"></i><b>9.5.3</b> A “pooled estimate” of the standard deviation</a></li>
<li class="chapter" data-level="9.5.4" data-path="ttest.html"><a href="ttest.html#the-same-pooled-estimate-described-differently"><i class="fa fa-check"></i><b>9.5.4</b> The same pooled estimate, described differently</a></li>
<li class="chapter" data-level="9.5.5" data-path="ttest.html"><a href="ttest.html#completing-the-test"><i class="fa fa-check"></i><b>9.5.5</b> Completing the test</a></li>
<li class="chapter" data-level="9.5.6" data-path="ttest.html"><a href="ttest.html#doing-the-test-in-r-1"><i class="fa fa-check"></i><b>9.5.6</b> Doing the test in R</a></li>
<li class="chapter" data-level="9.5.7" data-path="ttest.html"><a href="ttest.html#positive-and-negative-t-values"><i class="fa fa-check"></i><b>9.5.7</b> Positive and negative <span class="math inline">\(t\)</span> values</a></li>
<li class="chapter" data-level="9.5.8" data-path="ttest.html"><a href="ttest.html#studentassumptions"><i class="fa fa-check"></i><b>9.5.8</b> Assumptions of the test</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="ttest.html"><a href="ttest.html#welchttest"><i class="fa fa-check"></i><b>9.6</b> The independent samples <span class="math inline">\(t\)</span>-test (Welch test)</a>
<ul>
<li class="chapter" data-level="9.6.1" data-path="ttest.html"><a href="ttest.html#doing-the-test-in-r-2"><i class="fa fa-check"></i><b>9.6.1</b> Doing the test in R</a></li>
<li class="chapter" data-level="9.6.2" data-path="ttest.html"><a href="ttest.html#assumptions-of-the-test"><i class="fa fa-check"></i><b>9.6.2</b> Assumptions of the test</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="ttest.html"><a href="ttest.html#pairedsamplesttest"><i class="fa fa-check"></i><b>9.7</b> The paired-samples <span class="math inline">\(t\)</span>-test</a>
<ul>
<li class="chapter" data-level="9.7.1" data-path="ttest.html"><a href="ttest.html#the-data-2"><i class="fa fa-check"></i><b>9.7.1</b> The data</a></li>
<li class="chapter" data-level="9.7.2" data-path="ttest.html"><a href="ttest.html#what-is-the-paired-samples-t-test"><i class="fa fa-check"></i><b>9.7.2</b> What is the paired samples <span class="math inline">\(t\)</span>-test?</a></li>
<li class="chapter" data-level="9.7.3" data-path="ttest.html"><a href="ttest.html#doing-the-test-in-r-part-1"><i class="fa fa-check"></i><b>9.7.3</b> Doing the test in R, part 1</a></li>
<li class="chapter" data-level="9.7.4" data-path="ttest.html"><a href="ttest.html#doing-the-test-in-r-part-2"><i class="fa fa-check"></i><b>9.7.4</b> Doing the test in R, part 2</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="ttest.html"><a href="ttest.html#one-sided-tests"><i class="fa fa-check"></i><b>9.8</b> One sided tests</a></li>
<li class="chapter" data-level="9.9" data-path="ttest.html"><a href="ttest.html#ttestfunction"><i class="fa fa-check"></i><b>9.9</b> Using the t.test() function</a></li>
<li class="chapter" data-level="9.10" data-path="ttest.html"><a href="ttest.html#cohensd"><i class="fa fa-check"></i><b>9.10</b> Effect size</a>
<ul>
<li class="chapter" data-level="9.10.1" data-path="ttest.html"><a href="ttest.html#cohens-d-from-one-sample"><i class="fa fa-check"></i><b>9.10.1</b> Cohen’s <span class="math inline">\(d\)</span> from one sample</a></li>
<li class="chapter" data-level="9.10.2" data-path="ttest.html"><a href="ttest.html#cohens-d-from-a-student-t-test"><i class="fa fa-check"></i><b>9.10.2</b> Cohen’s <span class="math inline">\(d\)</span> from a Student <span class="math inline">\(t\)</span> test</a></li>
<li class="chapter" data-level="9.10.3" data-path="ttest.html"><a href="ttest.html#cohens-d-from-a-welch-test"><i class="fa fa-check"></i><b>9.10.3</b> Cohen’s <span class="math inline">\(d\)</span> from a Welch test</a></li>
<li class="chapter" data-level="9.10.4" data-path="ttest.html"><a href="ttest.html#cohens-d-from-a-paired-samples-test"><i class="fa fa-check"></i><b>9.10.4</b> Cohen’s <span class="math inline">\(d\)</span> from a paired-samples test</a></li>
</ul></li>
<li class="chapter" data-level="9.11" data-path="ttest.html"><a href="ttest.html#shapiro"><i class="fa fa-check"></i><b>9.11</b> Checking the normality of a sample</a>
<ul>
<li class="chapter" data-level="9.11.1" data-path="ttest.html"><a href="ttest.html#qq-plots"><i class="fa fa-check"></i><b>9.11.1</b> QQ plots</a></li>
<li class="chapter" data-level="9.11.2" data-path="ttest.html"><a href="ttest.html#shapiro-wilk-tests"><i class="fa fa-check"></i><b>9.11.2</b> Shapiro-Wilk tests</a></li>
</ul></li>
<li class="chapter" data-level="9.12" data-path="ttest.html"><a href="ttest.html#wilcox"><i class="fa fa-check"></i><b>9.12</b> Testing non-normal data with Wilcoxon tests</a>
<ul>
<li class="chapter" data-level="9.12.1" data-path="ttest.html"><a href="ttest.html#two-sample-wilcoxon-test"><i class="fa fa-check"></i><b>9.12.1</b> Two sample Wilcoxon test</a></li>
<li class="chapter" data-level="9.12.2" data-path="ttest.html"><a href="ttest.html#one-sample-wilcoxon-test"><i class="fa fa-check"></i><b>9.12.2</b> One sample Wilcoxon test</a></li>
</ul></li>
<li class="chapter" data-level="9.13" data-path="ttest.html"><a href="ttest.html#summary-6"><i class="fa fa-check"></i><b>9.13</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="anova.html"><a href="anova.html"><i class="fa fa-check"></i><b>10</b> One-way ANOVA</a>
<ul>
<li class="chapter" data-level="10.1" data-path="anova.html"><a href="anova.html#videos-8"><i class="fa fa-check"></i><b>10.1</b> Videos</a>
<ul>
<li class="chapter" data-level="10.1.1" data-path="anova.html"><a href="anova.html#one-way-between-subjects-anova"><i class="fa fa-check"></i><b>10.1.1</b> One-Way, Between-Subjects ANOVA</a></li>
<li class="chapter" data-level="10.1.2" data-path="anova.html"><a href="anova.html#repeated-measures-anova"><i class="fa fa-check"></i><b>10.1.2</b> Repeated Measures ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="anova.html"><a href="anova.html#anxifree"><i class="fa fa-check"></i><b>10.2</b> An illustrative data set</a></li>
<li class="chapter" data-level="10.3" data-path="anova.html"><a href="anova.html#anovaintro"><i class="fa fa-check"></i><b>10.3</b> How ANOVA works</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="anova.html"><a href="anova.html#two-formulas-for-the-variance-of-y"><i class="fa fa-check"></i><b>10.3.1</b> Two formulas for the variance of <span class="math inline">\(Y\)</span></a></li>
<li class="chapter" data-level="10.3.2" data-path="anova.html"><a href="anova.html#from-variances-to-sums-of-squares"><i class="fa fa-check"></i><b>10.3.2</b> From variances to sums of squares</a></li>
<li class="chapter" data-level="10.3.3" data-path="anova.html"><a href="anova.html#from-sums-of-squares-to-the-f-test"><i class="fa fa-check"></i><b>10.3.3</b> From sums of squares to the <span class="math inline">\(F\)</span>-test</a></li>
<li class="chapter" data-level="10.3.4" data-path="anova.html"><a href="anova.html#anovamodel"><i class="fa fa-check"></i><b>10.3.4</b> The model for the data and the meaning of <span class="math inline">\(F\)</span> (advanced)</a></li>
<li class="chapter" data-level="10.3.5" data-path="anova.html"><a href="anova.html#anovacalc"><i class="fa fa-check"></i><b>10.3.5</b> A worked example</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="anova.html"><a href="anova.html#introduceaov"><i class="fa fa-check"></i><b>10.4</b> Running an ANOVA in R</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="anova.html"><a href="anova.html#using-the-aov-function-to-specify-your-anova"><i class="fa fa-check"></i><b>10.4.1</b> Using the <code>aov()</code> function to specify your ANOVA</a></li>
<li class="chapter" data-level="10.4.2" data-path="anova.html"><a href="anova.html#aovobjects"><i class="fa fa-check"></i><b>10.4.2</b> Understanding what the <code>aov()</code> function produces</a></li>
<li class="chapter" data-level="10.4.3" data-path="anova.html"><a href="anova.html#running-the-hypothesis-tests-for-the-anova"><i class="fa fa-check"></i><b>10.4.3</b> Running the hypothesis tests for the ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="anova.html"><a href="anova.html#etasquared"><i class="fa fa-check"></i><b>10.5</b> Effect size</a></li>
<li class="chapter" data-level="10.6" data-path="anova.html"><a href="anova.html#anovaandt"><i class="fa fa-check"></i><b>10.6</b> On the relationship between ANOVA and the Student <span class="math inline">\(t\)</span> test</a></li>
<li class="chapter" data-level="10.7" data-path="anova.html"><a href="anova.html#summary-7"><i class="fa fa-check"></i><b>10.7</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="multiple-comparisons.html"><a href="multiple-comparisons.html"><i class="fa fa-check"></i><b>11</b> Multiple comparisons</a>
<ul>
<li class="chapter" data-level="11.1" data-path="multiple-comparisons.html"><a href="multiple-comparisons.html#posthoc"><i class="fa fa-check"></i><b>11.1</b> Multiple comparisons and post hoc tests</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="multiple-comparisons.html"><a href="multiple-comparisons.html#running-pairwise-t-tests"><i class="fa fa-check"></i><b>11.1.1</b> Running “pairwise” <span class="math inline">\(t\)</span>-tests</a></li>
<li class="chapter" data-level="11.1.2" data-path="multiple-comparisons.html"><a href="multiple-comparisons.html#corrections-for-multiple-testing"><i class="fa fa-check"></i><b>11.1.2</b> Corrections for multiple testing</a></li>
<li class="chapter" data-level="11.1.3" data-path="multiple-comparisons.html"><a href="multiple-comparisons.html#bonferroni-corrections"><i class="fa fa-check"></i><b>11.1.3</b> Bonferroni corrections</a></li>
<li class="chapter" data-level="11.1.4" data-path="multiple-comparisons.html"><a href="multiple-comparisons.html#holm-corrections"><i class="fa fa-check"></i><b>11.1.4</b> Holm corrections</a></li>
<li class="chapter" data-level="11.1.5" data-path="multiple-comparisons.html"><a href="multiple-comparisons.html#writing-up-the-post-hoc-test"><i class="fa fa-check"></i><b>11.1.5</b> Writing up the post hoc test</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="multiple-comparisons.html"><a href="multiple-comparisons.html#anovaassumptions"><i class="fa fa-check"></i><b>11.2</b> Assumptions of one-way ANOVA</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="multiple-comparisons.html"><a href="multiple-comparisons.html#how-robust-is-anova"><i class="fa fa-check"></i><b>11.2.1</b> How robust is ANOVA?</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="multiple-comparisons.html"><a href="multiple-comparisons.html#levene"><i class="fa fa-check"></i><b>11.3</b> Checking the homogeneity of variance assumption</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="multiple-comparisons.html"><a href="multiple-comparisons.html#running-the-levenes-test-in-r"><i class="fa fa-check"></i><b>11.3.1</b> Running the Levene’s test in R</a></li>
<li class="chapter" data-level="11.3.2" data-path="multiple-comparisons.html"><a href="multiple-comparisons.html#additional-comments"><i class="fa fa-check"></i><b>11.3.2</b> Additional comments</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="multiple-comparisons.html"><a href="multiple-comparisons.html#welchoneway"><i class="fa fa-check"></i><b>11.4</b> Removing the homogeneity of variance assumption</a></li>
<li class="chapter" data-level="11.5" data-path="multiple-comparisons.html"><a href="multiple-comparisons.html#anovanormality"><i class="fa fa-check"></i><b>11.5</b> Checking the normality assumption</a></li>
<li class="chapter" data-level="11.6" data-path="multiple-comparisons.html"><a href="multiple-comparisons.html#kruskalwallis"><i class="fa fa-check"></i><b>11.6</b> Removing the normality assumption</a>
<ul>
<li class="chapter" data-level="11.6.1" data-path="multiple-comparisons.html"><a href="multiple-comparisons.html#the-logic-behind-the-kruskal-wallis-test"><i class="fa fa-check"></i><b>11.6.1</b> The logic behind the Kruskal-Wallis test</a></li>
<li class="chapter" data-level="11.6.2" data-path="multiple-comparisons.html"><a href="multiple-comparisons.html#additional-details"><i class="fa fa-check"></i><b>11.6.2</b> Additional details</a></li>
<li class="chapter" data-level="11.6.3" data-path="multiple-comparisons.html"><a href="multiple-comparisons.html#how-to-run-the-kruskal-wallis-test-in-r"><i class="fa fa-check"></i><b>11.6.3</b> How to run the Kruskal-Wallis test in R</a></li>
</ul></li>
<li class="chapter" data-level="11.7" data-path="multiple-comparisons.html"><a href="multiple-comparisons.html#summary-8"><i class="fa fa-check"></i><b>11.7</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="factorial-analysis-of-variance-anova.html"><a href="factorial-analysis-of-variance-anova.html"><i class="fa fa-check"></i><b>12</b> Factorial Analysis of Variance (ANOVA)</a>
<ul>
<li class="chapter" data-level="12.1" data-path="factorial-analysis-of-variance-anova.html"><a href="factorial-analysis-of-variance-anova.html#video"><i class="fa fa-check"></i><b>12.1</b> Video</a></li>
<li class="chapter" data-level="12.2" data-path="factorial-analysis-of-variance-anova.html"><a href="factorial-analysis-of-variance-anova.html#introduction-7"><i class="fa fa-check"></i><b>12.2</b> Introduction</a></li>
<li class="chapter" data-level="12.3" data-path="factorial-analysis-of-variance-anova.html"><a href="factorial-analysis-of-variance-anova.html#why-do-a-factorial-anova"><i class="fa fa-check"></i><b>12.3</b> Why do a factorial ANOVA?</a></li>
<li class="chapter" data-level="12.4" data-path="factorial-analysis-of-variance-anova.html"><a href="factorial-analysis-of-variance-anova.html#interactions"><i class="fa fa-check"></i><b>12.4</b> Interactions</a></li>
<li class="chapter" data-level="12.5" data-path="factorial-analysis-of-variance-anova.html"><a href="factorial-analysis-of-variance-anova.html#main-effects"><i class="fa fa-check"></i><b>12.5</b> Main Effects</a></li>
<li class="chapter" data-level="12.6" data-path="factorial-analysis-of-variance-anova.html"><a href="factorial-analysis-of-variance-anova.html#simple-effects"><i class="fa fa-check"></i><b>12.6</b> Simple Effects</a></li>
<li class="chapter" data-level="12.7" data-path="factorial-analysis-of-variance-anova.html"><a href="factorial-analysis-of-variance-anova.html#interaction-effect"><i class="fa fa-check"></i><b>12.7</b> Interaction Effect</a></li>
<li class="chapter" data-level="12.8" data-path="factorial-analysis-of-variance-anova.html"><a href="factorial-analysis-of-variance-anova.html#factorial-anova-is-really-3-anovas"><i class="fa fa-check"></i><b>12.8</b> Factorial ANOVA is really 3 ANOVAs</a></li>
<li class="chapter" data-level="12.9" data-path="factorial-analysis-of-variance-anova.html"><a href="factorial-analysis-of-variance-anova.html#interpret-interaction-effects-first"><i class="fa fa-check"></i><b>12.9</b> Interpret Interaction Effects First</a></li>
<li class="chapter" data-level="12.10" data-path="factorial-analysis-of-variance-anova.html"><a href="factorial-analysis-of-variance-anova.html#graphing-factorial-anova-means"><i class="fa fa-check"></i><b>12.10</b> Graphing Factorial ANOVA Means</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="statistics-reference.html"><a href="statistics-reference.html"><i class="fa fa-check"></i><b>13</b> Statistics Reference</a>
<ul>
<li class="chapter" data-level="13.1" data-path="statistics-reference.html"><a href="statistics-reference.html#one-sample-z-test"><i class="fa fa-check"></i><b>13.1</b> One-Sample <em>z</em>-Test</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="statistics-reference.html"><a href="statistics-reference.html#definition"><i class="fa fa-check"></i><b>13.1.1</b> Definition</a></li>
<li class="chapter" data-level="13.1.2" data-path="statistics-reference.html"><a href="statistics-reference.html#test-statistic"><i class="fa fa-check"></i><b>13.1.2</b> Test Statistic</a></li>
<li class="chapter" data-level="13.1.3" data-path="statistics-reference.html"><a href="statistics-reference.html#assumptions-required-data"><i class="fa fa-check"></i><b>13.1.3</b> Assumptions &amp; Required Data</a></li>
<li class="chapter" data-level="13.1.4" data-path="statistics-reference.html"><a href="statistics-reference.html#when-to-use-it"><i class="fa fa-check"></i><b>13.1.4</b> When to use it</a></li>
<li class="chapter" data-level="13.1.5" data-path="statistics-reference.html"><a href="statistics-reference.html#example"><i class="fa fa-check"></i><b>13.1.5</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="statistics-reference.html"><a href="statistics-reference.html#correlation"><i class="fa fa-check"></i><b>13.2</b> Correlation</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="statistics-reference.html"><a href="statistics-reference.html#definition-1"><i class="fa fa-check"></i><b>13.2.1</b> Definition</a></li>
<li class="chapter" data-level="13.2.2" data-path="statistics-reference.html"><a href="statistics-reference.html#test-statistic-1"><i class="fa fa-check"></i><b>13.2.2</b> Test Statistic</a></li>
<li class="chapter" data-level="13.2.3" data-path="statistics-reference.html"><a href="statistics-reference.html#assumptions-required-data-1"><i class="fa fa-check"></i><b>13.2.3</b> Assumptions &amp; Required Data</a></li>
<li class="chapter" data-level="13.2.4" data-path="statistics-reference.html"><a href="statistics-reference.html#when-to-use-it-1"><i class="fa fa-check"></i><b>13.2.4</b> When to use it</a></li>
<li class="chapter" data-level="13.2.5" data-path="statistics-reference.html"><a href="statistics-reference.html#example-1"><i class="fa fa-check"></i><b>13.2.5</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="statistics-reference.html"><a href="statistics-reference.html#linear-regression"><i class="fa fa-check"></i><b>13.3</b> Linear Regression</a>
<ul>
<li class="chapter" data-level="13.3.1" data-path="statistics-reference.html"><a href="statistics-reference.html#definition-2"><i class="fa fa-check"></i><b>13.3.1</b> Definition</a></li>
<li class="chapter" data-level="13.3.2" data-path="statistics-reference.html"><a href="statistics-reference.html#test-statistic-2"><i class="fa fa-check"></i><b>13.3.2</b> Test Statistic</a></li>
<li class="chapter" data-level="13.3.3" data-path="statistics-reference.html"><a href="statistics-reference.html#assumptions-required-data-2"><i class="fa fa-check"></i><b>13.3.3</b> Assumptions &amp; Required Data</a></li>
<li class="chapter" data-level="13.3.4" data-path="statistics-reference.html"><a href="statistics-reference.html#when-to-use-it-2"><i class="fa fa-check"></i><b>13.3.4</b> When to use it</a></li>
<li class="chapter" data-level="13.3.5" data-path="statistics-reference.html"><a href="statistics-reference.html#example-2"><i class="fa fa-check"></i><b>13.3.5</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="statistics-reference.html"><a href="statistics-reference.html#one-sample-t-test"><i class="fa fa-check"></i><b>13.4</b> One-Sample <em>t</em>-Test</a>
<ul>
<li class="chapter" data-level="13.4.1" data-path="statistics-reference.html"><a href="statistics-reference.html#definition-3"><i class="fa fa-check"></i><b>13.4.1</b> Definition</a></li>
<li class="chapter" data-level="13.4.2" data-path="statistics-reference.html"><a href="statistics-reference.html#test-statistic-3"><i class="fa fa-check"></i><b>13.4.2</b> Test Statistic</a></li>
<li class="chapter" data-level="13.4.3" data-path="statistics-reference.html"><a href="statistics-reference.html#assumptions-required-data-3"><i class="fa fa-check"></i><b>13.4.3</b> Assumptions &amp; Required Data</a></li>
<li class="chapter" data-level="13.4.4" data-path="statistics-reference.html"><a href="statistics-reference.html#when-to-use-it-3"><i class="fa fa-check"></i><b>13.4.4</b> When to use it</a></li>
<li class="chapter" data-level="13.4.5" data-path="statistics-reference.html"><a href="statistics-reference.html#example-3"><i class="fa fa-check"></i><b>13.4.5</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="statistics-reference.html"><a href="statistics-reference.html#paired-samples-t-test"><i class="fa fa-check"></i><b>13.5</b> Paired Samples T-Test</a>
<ul>
<li class="chapter" data-level="13.5.1" data-path="statistics-reference.html"><a href="statistics-reference.html#definition-4"><i class="fa fa-check"></i><b>13.5.1</b> Definition</a></li>
<li class="chapter" data-level="13.5.2" data-path="statistics-reference.html"><a href="statistics-reference.html#test-statistic-4"><i class="fa fa-check"></i><b>13.5.2</b> Test Statistic</a></li>
<li class="chapter" data-level="13.5.3" data-path="statistics-reference.html"><a href="statistics-reference.html#assumptions-required-data-4"><i class="fa fa-check"></i><b>13.5.3</b> Assumptions &amp; Required Data</a></li>
<li class="chapter" data-level="13.5.4" data-path="statistics-reference.html"><a href="statistics-reference.html#when-to-use-it-4"><i class="fa fa-check"></i><b>13.5.4</b> When to use it</a></li>
<li class="chapter" data-level="13.5.5" data-path="statistics-reference.html"><a href="statistics-reference.html#example-4"><i class="fa fa-check"></i><b>13.5.5</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="statistics-reference.html"><a href="statistics-reference.html#independent-samples-t-test"><i class="fa fa-check"></i><b>13.6</b> Independent Samples <span class="math inline">\(t\)</span>-Test</a>
<ul>
<li class="chapter" data-level="13.6.1" data-path="statistics-reference.html"><a href="statistics-reference.html#definition-5"><i class="fa fa-check"></i><b>13.6.1</b> Definition</a></li>
<li class="chapter" data-level="13.6.2" data-path="statistics-reference.html"><a href="statistics-reference.html#test-statistic-5"><i class="fa fa-check"></i><b>13.6.2</b> Test Statistic</a></li>
<li class="chapter" data-level="13.6.3" data-path="statistics-reference.html"><a href="statistics-reference.html#assumptions-required-data-5"><i class="fa fa-check"></i><b>13.6.3</b> Assumptions &amp; Required Data</a></li>
<li class="chapter" data-level="13.6.4" data-path="statistics-reference.html"><a href="statistics-reference.html#when-to-use-it-5"><i class="fa fa-check"></i><b>13.6.4</b> When to use it</a></li>
<li class="chapter" data-level="13.6.5" data-path="statistics-reference.html"><a href="statistics-reference.html#example-5"><i class="fa fa-check"></i><b>13.6.5</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="statistics-reference.html"><a href="statistics-reference.html#one-way-anova"><i class="fa fa-check"></i><b>13.7</b> One-Way ANOVA</a>
<ul>
<li class="chapter" data-level="13.7.1" data-path="statistics-reference.html"><a href="statistics-reference.html#definition-6"><i class="fa fa-check"></i><b>13.7.1</b> Definition</a></li>
<li class="chapter" data-level="13.7.2" data-path="statistics-reference.html"><a href="statistics-reference.html#test-statistic-6"><i class="fa fa-check"></i><b>13.7.2</b> Test Statistic</a></li>
<li class="chapter" data-level="13.7.3" data-path="statistics-reference.html"><a href="statistics-reference.html#assumptions-required-data-6"><i class="fa fa-check"></i><b>13.7.3</b> Assumptions &amp; Required Data</a></li>
<li class="chapter" data-level="13.7.4" data-path="statistics-reference.html"><a href="statistics-reference.html#when-to-use-it-6"><i class="fa fa-check"></i><b>13.7.4</b> When to use it</a></li>
<li class="chapter" data-level="13.7.5" data-path="statistics-reference.html"><a href="statistics-reference.html#example-1-between-subjects"><i class="fa fa-check"></i><b>13.7.5</b> Example 1: Between-Subjects</a></li>
<li class="chapter" data-level="13.7.6" data-path="statistics-reference.html"><a href="statistics-reference.html#example-2-within-subjects"><i class="fa fa-check"></i><b>13.7.6</b> Example 2: Within-Subjects</a></li>
</ul></li>
<li class="chapter" data-level="13.8" data-path="statistics-reference.html"><a href="statistics-reference.html#statguide-factorial"><i class="fa fa-check"></i><b>13.8</b> Factorial ANOVA</a>
<ul>
<li class="chapter" data-level="13.8.1" data-path="statistics-reference.html"><a href="statistics-reference.html#definition-7"><i class="fa fa-check"></i><b>13.8.1</b> Definition</a></li>
<li class="chapter" data-level="13.8.2" data-path="statistics-reference.html"><a href="statistics-reference.html#test-statistic-7"><i class="fa fa-check"></i><b>13.8.2</b> Test Statistic</a></li>
<li class="chapter" data-level="13.8.3" data-path="statistics-reference.html"><a href="statistics-reference.html#between-within-and-mixed-anova"><i class="fa fa-check"></i><b>13.8.3</b> Between, Within, and Mixed ANOVA</a></li>
<li class="chapter" data-level="13.8.4" data-path="statistics-reference.html"><a href="statistics-reference.html#assumptions-required-data-7"><i class="fa fa-check"></i><b>13.8.4</b> Assumptions &amp; Required Data</a></li>
<li class="chapter" data-level="13.8.5" data-path="statistics-reference.html"><a href="statistics-reference.html#when-to-use-it-7"><i class="fa fa-check"></i><b>13.8.5</b> When to use it</a></li>
<li class="chapter" data-level="13.8.6" data-path="statistics-reference.html"><a href="statistics-reference.html#example-mixed-anova"><i class="fa fa-check"></i><b>13.8.6</b> Example: Mixed ANOVA</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Advanced Statistics Remix</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regression" class="section level1" number="8">
<h1><span class="header-section-number">Chapter 8</span> Regression</h1>
<p>Text by <span class="citation"><a href="#ref-Navarro2018" role="doc-biblioref">Navarro</a> (<a href="#ref-Navarro2018" role="doc-biblioref">2018</a>)</span></p>
<div id="videos-6" class="section level2" number="8.1">
<h2><span class="header-section-number">8.1</span> Videos</h2>
<p>While I had intended on producing my own videos for this chapter, the existing videos I found were just too good.</p>
<ul>
<li><p><a href="https://www.youtube.com/watch?v=GtV-VYdNt_g">Video: Correlation Does’t Equal Causation - 13 min</a> Quick note about this video: This one and the one on regression are excellent. However, I was surprised to see Karl Pearson’s work on the heights of fathers and sons included without editorial comment. The reason <em>why</em> Pearson was studying this problem is worth discussing. Oddly, I found an excellent video describing this from another channel from the same organization. Pearson isn’t discussed specifically, but both <a href="https://en.wikipedia.org/wiki/Karl_Pearson">Pearson</a> and <a href="https://en.wikipedia.org/wiki/Francis_Galton">Galton</a> were British mathematicians who developed many of techniques we are using today (see their Wikipedia pages for a list). Therefore, their legacy lives on in our work, and you’ll find connections between modern psychology and these two in this video: <a href="https://www.youtube.com/watch?v=JeCKftkNKJ0">Video: Eugenics and Francis Galton - 13 min</a></p></li>
<li><p><a href="https://www.youtube.com/watch?v=WWqE7YHR4Jc">Video: Regression - 13 min</a>. This is an excellent introduction to GLM and regression. Worth watching more than once.</p></li>
<li><p><a href="https://www.youtube.com/watch?v=66z_MRwtFJM">Video: How to do regression in R - 6 min</a></p></li>
</ul>
<p>Because it’s longer and lower quality than the videos above, you don’t need to watch the following video. But I am including it here to provide as many resources as possible.</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=sUXh5mY7P9Y">Dave’s Video: Correlation Basics - 20 min</a></li>
</ul>
</div>
<div id="introduction-5" class="section level2" number="8.2">
<h2><span class="header-section-number">8.2</span> Introduction</h2>
<p>Text by <span class="citation"><a href="#ref-Navarro2018" role="doc-biblioref">Navarro</a> (<a href="#ref-Navarro2018" role="doc-biblioref">2018</a>)</span></p>
<p>The goal in this chapter is to introduce <strong><em>linear regression</em></strong>, the standard tool that statisticians rely on when analysing the relationship between interval scale predictors and interval scale outcomes. Stripped to its bare essentials, linear regression models are basically a slightly fancier version of the Pearson correlation (Section <a href="regression.html#correl">8.4</a>) though as we’ll see, regression models are much more powerful tools.</p>
</div>
<div id="the-general-linear-model-glm" class="section level2" number="8.3">
<h2><span class="header-section-number">8.3</span> The General Linear Model (GLM)</h2>
<p>Text by David Schuster</p>
<p>We have not even done <span class="math inline">\(t\)</span>-tests in R yet and we are already doing multiple regression! This is intentional. One theme of this course is that statistical concepts that seem to be independent are sometimes mathematically related. Such is the case with regression. Regression is the mathematical basis of most of the inferential statistical procedures we use, including <span class="math inline">\(t\)</span>-tests, <span class="math inline">\(F\)</span>-tests (including all variations of analysis of variance, also called ANOVA), and, of course, correlation and multiple regression. Besides introducing linear regression, which is powerful and versatile, this chapter will introduce the mathematical concept that underlies all of these procedures, the <strong>general linear model</strong> (GLM). As an example of GLM, we will see that linear regression is perhaps the more powerful and versatile technique we will discuss in this class, because everything that follows can be implemented as a GLM model.</p>
<p>The key concept of the GLM is that it measures a linear relationship between one or more outcome variables (in our research they will usually be the dependent variable) and one or more predictor variables (usually independent variables). GLM models that have a single outcome variable are <strong>univariate statistics</strong>. GLM modeling with a single predictor is called <strong>simple regression</strong>. In this chapter, we will discover that we can add unlimited number of predictors in a linear model (although we will discover that there are situations where we can have too many variables), which is what defines <strong>multiple regression</strong> (it contains <em>multiple</em> independent variables). Our course will focus on univariate statistics. You should be aware, however, that a more complex version exists; we can combine multiple dependent variables on the left side of the equation. Including more than one outcome variable in a model is called <strong>multivariate statistics</strong>.</p>
<div id="the-traditional-approach-two-kinds-of-parametric-statistical-tests" class="section level3" number="8.3.1">
<h3><span class="header-section-number">8.3.1</span> The Traditional approach: Two kinds of parametric statistical tests</h3>
<p>Statistics teachers and textbook authors sometimes de-emphasize GLM in favor of presenting statistical tests as being one of two types:</p>
<ul>
<li><p>Statistical tests that measure linear relationships (e.g., correlation)</p></li>
<li><p>Statistical tests that measure differences between groups (e.g., z-test)</p></li>
</ul>
<p>This characterization can be useful, as researchers find themselves wanting to measure a linear relationship (i.e., Is there a correlation between A and B?) or measure differences between groups (i.e., Is there a difference between Condition A and Condition B?). Consequently, linear relationships are useful when researchers are doing observational research. You can measure two quantitative variables and see if they are related. Meanwhile, experimental researchers want to see if their discrete manipulation (participants are either in Condition A or Condition B; it is a discrete IV) causes a change in their dependent variable. This makes multiple regression more convenient for researchers because they have continuous variables. Meanwhile, experimental researchers often have a discrete IV, so software designed to run an ANOVA can make this analysis more convenient than adapting it to run as a multiple regression. It is important to note that this is merely convenience. Both procedures use GLM.</p>
<p>This distinction between linear relationships and group differences can be misleading when researchers think that these two techniques are separate. Any t-test or ANOVA could be run as a multiple regression. Or, worse, researchers may mix up the <em>research design</em> and the <em>statistical analysis</em>. Therefore, it is important to state:</p>
<ul>
<li><p>Regression can be used with continuous or discrete predictor variables. There is a rule about this; discrete predictors in a regression must be <strong>dichotomous</strong> (only having two possible values). A yes/no question could be added to a regression model. A question asking participants their favorite color could not be added to a regression model without an additional step. Non-dichotomous, discrete predictors are added to regression models using a technique called <strong>dummy coding</strong>, which turns a categorical variable into a series of dichotomous variables (we will learn how to do this later on). Dummy coding would lead researchers to the same answer as running an ANOVA.</p></li>
<li><p>ANOVA is designed for discrete predictor variables (with as many categories or levels as you would like). ANOVA can be used with continuous predictor variables if we make them discrete, but that is rarely advisable. One way to do this is a <strong>median split</strong> (scores below the median are 0 and scores above the median are 1). If one participant scores just below the median and another very far below the median, they will both be scored the same. Therefore, this throws away variance and can reduce statistical power. For this reason, multiple regression is the more adaptable technique. That said, we will see that we use some ANOVA calculations on our regression models when we do null hypothesis significance testing (NHST).</p></li>
<li><p>All GLM procedures can be used with non-experimental, quasi-experimental, or experimental research designs. <strong>Whether inferences can be made about causality is affected by the research design, not the choice of statistical technique</strong>. Both regression and ANOVA can show us if two variables are related. Thus, which one is the right statistic depends on the type of measurement used in the study, not whether the study is an experiment, quasi-experiment, or non-experiment. If you have two continuous variables, you should use a correlation. If you have a discrete IV and a continuous DV, you could use either correlation or a t-test. You can use a correlation to analyze experiments, quasi-experiments, or non-experiments. You can use a t-test to analyze experiments, quasi-experiments, or non-experiments. A correlation analysis is not the same thing as a “correlational research design.” For this reason, “experiment, quasi-experiment, and non-experiment” are much clearer labels.</p></li>
<li><p>The same model and data tested as a regression or as an ANOVA will yield the same mean differences, effect size, and p-value. <strong>Regression and ANOVA, because they are both GLM, have the same statistical power</strong>.</p></li>
</ul>
<p>All GLM procedures are <strong>parametric statistics</strong>, which means that they make assumptions about the population distributions from which we sample. In practice, this means that parametric statistics typically have more assumptions than their <strong>nonparametric</strong> alternatives. We need to learn which assumptions are important and how we can check our data to see if assumptions are met. It is an oversimplification to say that nonparametric stats have no assumptions; they just have fewer assumptions.</p>
</div>
<div id="the-glm-approach" class="section level3" number="8.3.2">
<h3><span class="header-section-number">8.3.2</span> The GLM approach</h3>
<p><span class="math display">\[
Y = BX + I + E
\]</span>
With GLM, we are using the equation for a line to assess and describe the relationship between variables. You will see this equation several times in this chapter with different letters labeling each element. The elements are:</p>
<ul>
<li><p><span class="math inline">\(Y\)</span>, an outcome variable, which can also be called the regressand, response variable, predicted variable, or output variable</p></li>
<li><p><span class="math inline">\(X\)</span>, a predictor variable, which can also be called the regressor, independent variable, treatment variable, or factor</p></li>
<li><p><span class="math inline">\(B\)</span>, a model parameter, also called the weight, slope or the coefficient, which defines the relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span></p></li>
<li><p><span class="math inline">\(I\)</span>, the intercept, which gives the estimated value of Y when <span class="math inline">\(X = 0\)</span>. It is also the y-intercept of the regression line.</p></li>
<li><p><span class="math inline">\(E\)</span>, error, also called the residual</p></li>
</ul>
<p>The techniques in this chapter expand upon this approach of creating a model and using it to estimate an outcome variable. For example, each of these elements can actually be a list (a matrix) of observations. Further, we can expand this model such that <span class="math inline">\(XB\)</span> is a combination of several predictor variables, not just one. Or, we could create an even simpler model with no predictors.</p>
<p>It might help to illustrate this with an example. In <span class="citation"><a href="#ref-Kozma1983" role="doc-biblioref">Kozma and Stones</a> (<a href="#ref-Kozma1983" role="doc-biblioref">1983</a>)</span> found that good health predicted happiness. Imagine that I wanted to predict your score on this measure right now. However, I know nothing about you. What could serve as my best guess of your happiness? I could use the population mean. That would result in a model <span class="math inline">\(Y = I + E\)</span>, where Y is your predicted happiness, predicted on the basis of the population mean (<span class="math inline">\(I = \mu)\)</span> and nothing else. As there are no variables, there would be no <span class="math inline">\(XB\)</span> term. I could probably do a bit better if I also knew your health score. This new model would be <span class="math inline">\(Y = XB + I + E\)</span>, where <span class="math inline">\(X\)</span> is your health score, and <span class="math inline">\(B\)</span> is a model parameter that explains how to adjust your predicted happiness score based on the health score (technically, it says to increase the predicted <span class="math inline">\(Y\)</span> by <span class="math inline">\(B\)</span> units for every 1 unit increase of <span class="math inline">\(X\)</span>). We have just created a linear regression model, which is a use of the GLM. We have also shown how the addition of predictors (assuming they are good predictors that independently correlate with our outcome variable) can increase the predictive power of a model. This pattern could continue if we added an additional explanatory variable, leading to a multiple regression equation in the form <span class="math inline">\(Y = X_1B_1 + X_2B_2 + I +E\)</span>.</p>
</div>
</div>
<div id="correl" class="section level2" number="8.4">
<h2><span class="header-section-number">8.4</span> Correlations</h2>
<p>Text by <span class="citation"><a href="#ref-Navarro2018" role="doc-biblioref">Navarro</a> (<a href="#ref-Navarro2018" role="doc-biblioref">2018</a>)</span></p>
<p>Up to this point we have focused entirely on how to construct descriptive statistics for a single variable. What we haven’t done is talked about how to describe the relationships <em>between</em> variables in the data. To do that, we want to talk mostly about the <strong><em>correlation</em></strong> between variables. But first, we need some data.</p>
<div id="the-data" class="section level3" number="8.4.1">
<h3><span class="header-section-number">8.4.1</span> The data</h3>
<p>After spending so much time looking at the AFL data, I’m starting to get bored with sports. Instead, let’s turn to a topic close to every parent’s heart: sleep. The following data set is fictitious, but based on real events. Suppose I’m curious to find out how much my infant son’s sleeping habits affect my mood. Let’s say that I can rate my grumpiness very precisely, on a scale from 0 (not at all grumpy) to 100 (grumpy as a very, very grumpy old man). And, lets also assume that I’ve been measuring my grumpiness, my sleeping patterns and my son’s sleeping patterns for quite some time now. Let’s say, for 100 days. And, being a nerd, I’ve saved the data as a file called <code>parenthood.Rdata</code>. If we load the data…</p>
<div class="sourceCode" id="cb641"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb641-1"><a href="regression.html#cb641-1" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>( <span class="st">&quot;./data/parenthood.Rdata&quot;</span> )</span>
<span id="cb641-2"><a href="regression.html#cb641-2" aria-hidden="true" tabindex="-1"></a><span class="fu">who</span>(<span class="cn">TRUE</span>)</span></code></pre></div>
<pre><code>##    -- Name --     -- Class --   -- Size --
##    parenthood     data.frame    100 x 4   
##     $dan.sleep    numeric       100       
##     $baby.sleep   numeric       100       
##     $dan.grump    numeric       100       
##     $day          integer       100</code></pre>
<p>… we see that the file contains a single data frame called <code>parenthood</code>, which contains four variables <code>dan.sleep</code>, <code>baby.sleep</code>, <code>dan.grump</code> and <code>day</code>. If we peek at the data using <code>head()</code> out the data, here’s what we get:</p>
<div class="sourceCode" id="cb643"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb643-1"><a href="regression.html#cb643-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(parenthood,<span class="dv">10</span>)</span></code></pre></div>
<pre><code>##    dan.sleep baby.sleep dan.grump day
## 1       7.59      10.18        56   1
## 2       7.91      11.66        60   2
## 3       5.14       7.92        82   3
## 4       7.71       9.61        55   4
## 5       6.68       9.75        67   5
## 6       5.99       5.04        72   6
## 7       8.19      10.45        53   7
## 8       7.19       8.27        60   8
## 9       7.40       6.06        60   9
## 10      6.58       7.09        71  10</code></pre>
<p>Next, I’ll calculate some basic descriptive statistics:</p>
<div class="sourceCode" id="cb645"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb645-1"><a href="regression.html#cb645-1" aria-hidden="true" tabindex="-1"></a><span class="fu">describe</span>( parenthood )</span></code></pre></div>
<pre><code>##            vars   n  mean    sd median trimmed   mad   min    max range
## dan.sleep     1 100  6.97  1.02   7.03    7.00  1.09  4.84   9.00  4.16
## baby.sleep    2 100  8.05  2.07   7.95    8.05  2.33  3.25  12.07  8.82
## dan.grump     3 100 63.71 10.05  62.00   63.16  9.64 41.00  91.00 50.00
## day           4 100 50.50 29.01  50.50   50.50 37.06  1.00 100.00 99.00
##             skew kurtosis   se
## dan.sleep  -0.29    -0.72 0.10
## baby.sleep -0.02    -0.69 0.21
## dan.grump   0.43    -0.16 1.00
## day         0.00    -1.24 2.90</code></pre>
<p>Finally, to give a graphical depiction of what each of the three interesting variables looks like, Figure <a href="regression.html#fig:parenthood">8.1</a> plots histograms.</p>
<div class="figure"><span style="display:block;" id="fig:parenthood"></span>
<img src="schuster-statistics-remix_files/figure-html/parenthood-1.png" alt="Histograms for the three interesting variables in the `parenthood` data set" width="672" />
<p class="caption">
Figure 8.1: Histograms for the three interesting variables in the <code>parenthood</code> data set
</p>
</div>
<p>One thing to note: just because R can calculate dozens of different statistics doesn’t mean you should report all of them. If I were writing this up for a report, I’d probably pick out those statistics that are of most interest to me (and to my readership), and then put them into a nice, simple table like the one in Table <a href="regression.html#tab:parenthoodtab">8.1</a>.<a href="#fn111" class="footnote-ref" id="fnref111"><sup>111</sup></a> Notice that when I put it into a table, I gave everything “human readable” names. This is always good practice. Notice also that I’m not getting enough sleep. This isn’t good practice, but other parents tell me that it’s standard practice.</p>
<table>
<caption><span id="tab:parenthoodtab">Table 8.1: </span>Descriptive statistics for the parenthood data.</caption>
<thead>
<tr class="header">
<th align="left">variable</th>
<th align="left">min</th>
<th align="left">max</th>
<th align="left">mean</th>
<th align="left">median</th>
<th align="left">std. dev</th>
<th align="left">IQR</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Dan’s grumpiness</td>
<td align="left">41</td>
<td align="left">91</td>
<td align="left">63.71</td>
<td align="left">62</td>
<td align="left">10.05</td>
<td align="left">14</td>
</tr>
<tr class="even">
<td align="left">Dan’s hours slept</td>
<td align="left">4.84</td>
<td align="left">9</td>
<td align="left">6.97</td>
<td align="left">7.03</td>
<td align="left">1.02</td>
<td align="left">1.45</td>
</tr>
<tr class="odd">
<td align="left">Dan’s son’s hours slept</td>
<td align="left">3.25</td>
<td align="left">12.07</td>
<td align="left">8.05</td>
<td align="left">7.95</td>
<td align="left">2.07</td>
<td align="left">3.21</td>
</tr>
</tbody>
</table>
</div>
<div id="the-strength-and-direction-of-a-relationship" class="section level3" number="8.4.2">
<h3><span class="header-section-number">8.4.2</span> The strength and direction of a relationship</h3>
<div class="figure"><span style="display:block;" id="fig:scatterparent1a"></span>
<img src="schuster-statistics-remix_files/figure-html/scatterparent1a-1.png" alt="Scatterplot showing the relationship between `dan.sleep` and `dan.grump`" width="672" />
<p class="caption">
Figure 8.2: Scatterplot showing the relationship between <code>dan.sleep</code> and <code>dan.grump</code>
</p>
</div>
<div class="figure"><span style="display:block;" id="fig:scatterparent1b"></span>
<img src="schuster-statistics-remix_files/figure-html/scatterparent1b-1.png" alt="Scatterplot showing the relationship between `baby.sleep` and `dan.grump`" width="672" />
<p class="caption">
Figure 8.3: Scatterplot showing the relationship between <code>baby.sleep</code> and <code>dan.grump</code>
</p>
</div>
<p>We can draw scatterplots to give us a general sense of how closely related two variables are. Ideally though, we might want to say a bit more about it than that. For instance, let’s compare the relationship between <code>dan.sleep</code> and <code>dan.grump</code> (Figure <a href="regression.html#fig:scatterparent1a">8.2</a> with that between <code>baby.sleep</code> and <code>dan.grump</code> (Figure <a href="regression.html#fig:scatterparent1b">8.3</a>. When looking at these two plots side by side, it’s clear that the relationship is <em>qualitatively</em> the same in both cases: more sleep equals less grump! However, it’s also pretty obvious that the relationship between <code>dan.sleep</code> and <code>dan.grump</code> is <em>stronger</em> than the relationship between <code>baby.sleep</code> and <code>dan.grump</code>. The plot on the left is “neater” than the one on the right. What it feels like is that if you want to predict what my mood is, it’d help you a little bit to know how many hours my son slept, but it’d be <em>more</em> helpful to know how many hours I slept.</p>
<p>In contrast, let’s consider Figure <a href="regression.html#fig:scatterparent1b">8.3</a> vs. Figure <a href="regression.html#fig:scatterparent2">8.4</a>. If we compare the scatterplot of “<code>baby.sleep</code> v <code>dan.grump</code>” to the scatterplot of “`<code>baby.sleep</code> v <code>dan.sleep</code>,” the overall strength of the relationship is the same, but the direction is different. That is, if my son sleeps more, I get <em>more</em> sleep (positive relationship, but if he sleeps more then I get <em>less</em> grumpy (negative relationship).</p>
<div class="figure"><span style="display:block;" id="fig:scatterparent2"></span>
<img src="schuster-statistics-remix_files/figure-html/scatterparent2-1.png" alt="Scatterplot showing the relationship between `baby.sleep` and `dan.sleep`" width="672" />
<p class="caption">
Figure 8.4: Scatterplot showing the relationship between <code>baby.sleep</code> and <code>dan.sleep</code>
</p>
</div>
</div>
<div id="the-correlation-coefficient" class="section level3" number="8.4.3">
<h3><span class="header-section-number">8.4.3</span> The correlation coefficient</h3>
<p>We can make these ideas a bit more explicit by introducing the idea of a <strong><em>correlation coefficient</em></strong> (or, more specifically, Pearson’s correlation coefficient), which is traditionally denoted by <span class="math inline">\(r\)</span>. The correlation coefficient between two variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> (sometimes denoted <span class="math inline">\(r_{XY}\)</span>), which we’ll define more precisely in the next section, is a measure that varies from <span class="math inline">\(-1\)</span> to <span class="math inline">\(1\)</span>. When <span class="math inline">\(r = -1\)</span> it means that we have a perfect negative relationship, and when <span class="math inline">\(r = 1\)</span> it means we have a perfect positive relationship. When <span class="math inline">\(r = 0\)</span>, there’s no relationship at all. If you look at Figure <a href="regression.html#fig:corr">8.5</a>, you can see several plots showing what different correlations look like.</p>
<div class="figure"><span style="display:block;" id="fig:corr"></span>
<img src="schuster-statistics-remix_files/figure-html/corr-1.png" alt="Illustration of the effect of varying the strength and direction of a correlation" width="672" />
<p class="caption">
Figure 8.5: Illustration of the effect of varying the strength and direction of a correlation
</p>
</div>
<p>The formula for the Pearson’s correlation coefficient can be written in several different ways. I think the simplest way to write down the formula is to break it into two steps. Firstly, let’s introduce the idea of a <strong><em>covariance</em></strong>. The covariance between two variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is a generalisation of the notion of the variance; it’s a mathematically simple way of describing the relationship between two variables that isn’t terribly informative to humans:
<span class="math display">\[
\mbox{Cov}(X,Y) = \frac{1}{N-1} \sum_{i=1}^N \left( X_i - \bar{X} \right) \left( Y_i - \bar{Y} \right)
\]</span>
Because we’re multiplying (i.e., taking the “product” of) a quantity that depends on <span class="math inline">\(X\)</span> by a quantity that depends on <span class="math inline">\(Y\)</span> and then averaging<a href="#fn112" class="footnote-ref" id="fnref112"><sup>112</sup></a>, you can think of the formula for the covariance as an “average cross product” between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. The covariance has the nice property that, if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are entirely unrelated, then the covariance is exactly zero. If the relationship between them is positive (in the sense shown in <a href="mailto:Figure@reffig" class="email">Figure@reffig</a>:corr) then the covariance is also positive; and if the relationship is negative then the covariance is also negative. In other words, the covariance captures the basic qualitative idea of correlation. Unfortunately, the raw magnitude of the covariance isn’t easy to interpret: it depends on the units in which <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are expressed, and worse yet, the actual units that the covariance itself is expressed in are really weird. For instance, if <span class="math inline">\(X\)</span> refers to the <code>dan.sleep</code> variable (units: hours) and <span class="math inline">\(Y\)</span> refers to the <code>dan.grump</code> variable (units: grumps), then the units for their covariance are “hours <span class="math inline">\(\times\)</span> grumps.” And I have no freaking idea what that would even mean.</p>
<p>The Pearson correlation coefficient <span class="math inline">\(r\)</span> fixes this interpretation problem by standardising the covariance, in pretty much the exact same way that the <span class="math inline">\(z\)</span>-score standardises a raw score: by dividing by the standard deviation. However, because we have two variables that contribute to the covariance, the standardisation only works if we divide by both standard deviations.<a href="#fn113" class="footnote-ref" id="fnref113"><sup>113</sup></a> In other words, the correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> can be written as follows:
<span class="math display">\[
r_{XY}  = \frac{\mbox{Cov}(X,Y)}{ \hat{\sigma}_X \ \hat{\sigma}_Y}
\]</span>
By doing this standardisation, not only do we keep all of the nice properties of the covariance discussed earlier, but the actual values of <span class="math inline">\(r\)</span> are on a meaningful scale: <span class="math inline">\(r= 1\)</span> implies a perfect positive relationship, and <span class="math inline">\(r = -1\)</span> implies a perfect negative relationship. I’ll expand a little more on this point later, in <a href="mailto:Section@refsec" class="email">Section@refsec</a>:interpretingcorrelations. But before I do, let’s look at how to calculate correlations in R.</p>
</div>
<div id="calculating-correlations-in-r" class="section level3" number="8.4.4">
<h3><span class="header-section-number">8.4.4</span> Calculating correlations in R</h3>
<p>Calculating correlations in R can be done using the <code>cor()</code> command. The simplest way to use the command is to specify two input arguments <code>x</code> and <code>y</code>, each one corresponding to one of the variables. The following extract illustrates the basic usage of the function:<a href="#fn114" class="footnote-ref" id="fnref114"><sup>114</sup></a></p>
<div class="sourceCode" id="cb647"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb647-1"><a href="regression.html#cb647-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>( <span class="at">x =</span> parenthood<span class="sc">$</span>dan.sleep, <span class="at">y =</span> parenthood<span class="sc">$</span>dan.grump )</span></code></pre></div>
<pre><code>## [1] -0.903384</code></pre>
<p>However, the <code>cor()</code> function is a bit more powerful than this simple example suggests. For example, you can also calculate a complete “correlation matrix,” between all pairs of variables in the data frame:<a href="#fn115" class="footnote-ref" id="fnref115"><sup>115</sup></a></p>
<div class="sourceCode" id="cb649"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb649-1"><a href="regression.html#cb649-1" aria-hidden="true" tabindex="-1"></a><span class="co"># correlate all pairs of variables in &quot;parenthood&quot;:</span></span>
<span id="cb649-2"><a href="regression.html#cb649-2" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>( <span class="at">x =</span> parenthood )  </span></code></pre></div>
<pre><code>##              dan.sleep  baby.sleep   dan.grump         day
## dan.sleep   1.00000000  0.62794934 -0.90338404 -0.09840768
## baby.sleep  0.62794934  1.00000000 -0.56596373 -0.01043394
## dan.grump  -0.90338404 -0.56596373  1.00000000  0.07647926
## day        -0.09840768 -0.01043394  0.07647926  1.00000000</code></pre>
</div>
<div id="interpretingcorrelations" class="section level3" number="8.4.5">
<h3><span class="header-section-number">8.4.5</span> Interpreting a correlation</h3>
<p>Naturally, in real life you don’t see many correlations of 1. So how should you interpret a correlation of, say <span class="math inline">\(r= .4\)</span>? The honest answer is that it really depends on what you want to use the data for, and on how strong the correlations in your field tend to be. A friend of mine in engineering once argued that any correlation less than <span class="math inline">\(.95\)</span> is completely useless (I think he was exaggerating, even for engineering). On the other hand there are real cases – even in psychology – where you should really expect correlations that strong. For instance, one of the benchmark data sets used to test theories of how people judge similarities is so clean that any theory that can’t achieve a correlation of at least <span class="math inline">\(.9\)</span> really isn’t deemed to be successful. However, when looking for (say) elementary correlates of intelligence (e.g., inspection time, response time), if you get a correlation above <span class="math inline">\(.3\)</span> you’re doing very very well. In short, the interpretation of a correlation depends a lot on the context. That said, the rough guide in Table <a href="regression.html#tab:interpretingcorrelations">8.2</a> is pretty typical.</p>
<table>
<caption><span id="tab:interpretingcorrelations">Table 8.2: </span>Rough guide to interpreting correlations</caption>
<thead>
<tr class="header">
<th align="left">Correlation</th>
<th align="left">Strength</th>
<th align="left">Direction</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">-1.0 to -0.9</td>
<td align="left">Very strong</td>
<td align="left">Negative</td>
</tr>
<tr class="even">
<td align="left">-0.9 to -0.7</td>
<td align="left">Strong</td>
<td align="left">Negative</td>
</tr>
<tr class="odd">
<td align="left">-0.7 to -0.4</td>
<td align="left">Moderate</td>
<td align="left">Negative</td>
</tr>
<tr class="even">
<td align="left">-0.4 to -0.2</td>
<td align="left">Weak</td>
<td align="left">Negative</td>
</tr>
<tr class="odd">
<td align="left">-0.2 to 0</td>
<td align="left">Negligible</td>
<td align="left">Negative</td>
</tr>
<tr class="even">
<td align="left">0 to 0.2</td>
<td align="left">Negligible</td>
<td align="left">Positive</td>
</tr>
<tr class="odd">
<td align="left">0.2 to 0.4</td>
<td align="left">Weak</td>
<td align="left">Positive</td>
</tr>
<tr class="even">
<td align="left">0.4 to 0.7</td>
<td align="left">Moderate</td>
<td align="left">Positive</td>
</tr>
<tr class="odd">
<td align="left">0.7 to 0.9</td>
<td align="left">Strong</td>
<td align="left">Positive</td>
</tr>
<tr class="even">
<td align="left">0.9 to 1.0</td>
<td align="left">Very strong</td>
<td align="left">Positive</td>
</tr>
</tbody>
</table>
<p>However, something that can never be stressed enough is that you should <em>always</em> look at the scatterplot before attaching any interpretation to the data. A correlation might not mean what you think it means. The classic illustration of this is “Anscombe’s Quartet” <span class="citation">(<a href="#ref-Anscombe1973" role="doc-biblioref">Anscombe 1973</a>)</span>, which is a collection of four data sets. Each data set has two variables, an <span class="math inline">\(X\)</span> and a <span class="math inline">\(Y\)</span>. For all four data sets the mean value for <span class="math inline">\(X\)</span> is 9 and the mean for <span class="math inline">\(Y\)</span> is 7.5. The, standard deviations for all <span class="math inline">\(X\)</span> variables are almost identical, as are those for the the <span class="math inline">\(Y\)</span> variables. And in each case the correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is <span class="math inline">\(r = 0.816\)</span>. You can verify this yourself, since the dataset comes distributed with R. The commands would be:</p>
<div class="sourceCode" id="cb651"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb651-1"><a href="regression.html#cb651-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>( anscombe<span class="sc">$</span>x1, anscombe<span class="sc">$</span>y1 )</span></code></pre></div>
<pre><code>## [1] 0.8164205</code></pre>
<div class="sourceCode" id="cb653"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb653-1"><a href="regression.html#cb653-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>( anscombe<span class="sc">$</span>x2, anscombe<span class="sc">$</span>y2 )</span></code></pre></div>
<pre><code>## [1] 0.8162365</code></pre>
<p>and so on.</p>
You’d think that these four data sets would look pretty similar to one another. They do not. If we draw scatterplots of <span class="math inline">\(X\)</span> against <span class="math inline">\(Y\)</span> for all four variables, as shown in Figure <a href="regression.html#fig:anscombe">8.6</a> we see that all four of these are <em>spectacularly</em> different to each other.
<div class="figure"><span style="display:block;" id="fig:anscombe"></span>
<img src="schuster-statistics-remix_files/figure-html/anscombe-1.png" alt="Anscombe's quartet. All four of these data sets have a Pearson correlation of $r = .816$, but they are qualitatively different from one another." width="672" />
<p class="caption">
Figure 8.6: Anscombe’s quartet. All four of these data sets have a Pearson correlation of <span class="math inline">\(r = .816\)</span>, but they are qualitatively different from one another.
</p>
</div>
<p>The lesson here, which so very many people seem to forget in real life is “<em>always graph your raw data</em>.” This will be the focus of Chapter <a href="descriptives.html#graphics">3.9</a>.</p>
</div>
<div id="spearmans-rank-correlations" class="section level3" number="8.4.6">
<h3><span class="header-section-number">8.4.6</span> Spearman’s rank correlations</h3>
<div class="figure"><span style="display:block;" id="fig:rankcorrpic"></span>
<img src="schuster-statistics-remix_files/figure-html/rankcorrpic-1.png" alt="The relationship between hours worked and grade received, for a toy data set consisting of only 10 students (each circle corresponds to one student). The dashed line through the middle shows the linear relationship between the two variables. This produces a strong Pearson correlation of $r = .91$. However, the interesting thing to note here is that there's actually a perfect monotonic relationship between the two variables: in this toy example at least, increasing the hours worked always increases the grade received, as illustrated by the solid line. This is reflected in a Spearman correlation of $rho = 1$. With such a small data set, however, it's an open question as to which version better describes the actual relationship involved. " width="672" />
<p class="caption">
Figure 8.7: The relationship between hours worked and grade received, for a toy data set consisting of only 10 students (each circle corresponds to one student). The dashed line through the middle shows the linear relationship between the two variables. This produces a strong Pearson correlation of <span class="math inline">\(r = .91\)</span>. However, the interesting thing to note here is that there’s actually a perfect monotonic relationship between the two variables: in this toy example at least, increasing the hours worked always increases the grade received, as illustrated by the solid line. This is reflected in a Spearman correlation of <span class="math inline">\(rho = 1\)</span>. With such a small data set, however, it’s an open question as to which version better describes the actual relationship involved.
</p>
</div>
<p>The Pearson correlation coefficient is useful for a lot of things, but it does have shortcomings. One issue in particular stands out: what it actually measures is the strength of the <em>linear</em> relationship between two variables. In other words, what it gives you is a measure of the extent to which the data all tend to fall on a single, perfectly straight line. Often, this is a pretty good approximation to what we mean when we say “relationship,” and so the Pearson correlation is a good thing to calculation. Sometimes, it isn’t.</p>
<p>One very common situation where the Pearson correlation isn’t quite the right thing to use arises when an increase in one variable <span class="math inline">\(X\)</span> really is reflected in an increase in another variable <span class="math inline">\(Y\)</span>, but the nature of the relationship isn’t necessarily linear. An example of this might be the relationship between effort and reward when studying for an exam. If you put in zero effort (<span class="math inline">\(X\)</span>) into learning a subject, then you should expect a grade of 0% (<span class="math inline">\(Y\)</span>). However, a little bit of effort will cause a <em>massive</em> improvement: just turning up to lectures means that you learn a fair bit, and if you just turn up to classes, and scribble a few things down so your grade might rise to 35%, all without a lot of effort. However, you just don’t get the same effect at the other end of the scale. As everyone knows, it takes <em>a lot</em> more effort to get a grade of 90% than it takes to get a grade of 55%. What this means is that, if I’ve got data looking at study effort and grades, there’s a pretty good chance that Pearson correlations will be misleading.</p>
<p>To illustrate, consider the data plotted in Figure <a href="regression.html#fig:rankcorrpic">8.7</a>, showing the relationship between hours worked and grade received for 10 students taking some class. The curious thing about this – highly fictitious – data set is that increasing your effort <em>always</em> increases your grade. It might be by a lot or it might be by a little, but increasing effort will never decrease your grade. The data are stored in <code>effort.Rdata</code>:</p>
<pre><code>&gt; load( &quot;effort.Rdata&quot; )
&gt; who(TRUE)
   -- Name --   -- Class --   -- Size --
   effort       data.frame    10 x 2    
    $hours      numeric       10        
    $grade      numeric       10        </code></pre>
<p>The raw data look like this:</p>
<pre><code>&gt; effort
   hours grade
1      2    13
2     76    91
3     40    79
4      6    14
5     16    21
6     28    74
7     27    47
8     59    85
9     46    84
10    68    88</code></pre>
<p>If we run a standard Pearson correlation, it shows a strong relationship between hours worked and grade received,</p>
<pre><code>&gt; cor( effort$hours, effort$grade )
[1] 0.909402</code></pre>
<p>but this doesn’t actually capture the observation that increasing hours worked <em>always</em> increases the grade. There’s a sense here in which we want to be able to say that the correlation is <em>perfect</em> but for a somewhat different notion of what a “relationship” is. What we’re looking for is something that captures the fact that there is a perfect <strong><em>ordinal relationship</em></strong> here. That is, if student 1 works more hours than student 2, then we can guarantee that student 1 will get the better grade. That’s not what a correlation of <span class="math inline">\(r = .91\)</span> says at all.</p>
<p>How should we address this? Actually, it’s really easy: if we’re looking for ordinal relationships, all we have to do is treat the data as if it were ordinal scale! So, instead of measuring effort in terms of “hours worked,” lets rank all 10 of our students in order of hours worked. That is, student 1 did the least work out of anyone (2 hours) so they get the lowest rank (rank = 1). Student 4 was the next laziest, putting in only 6 hours of work in over the whole semester, so they get the next lowest rank (rank = 2). Notice that I’m using “rank =1” to mean “low rank.” Sometimes in everyday language we talk about “rank = 1” to mean “top rank” rather than “bottom rank.” So be careful: you can rank “from smallest value to largest value” (i.e., small equals rank 1) or you can rank “from largest value to smallest value” (i.e., large equals rank 1). In this case, I’m ranking from smallest to largest, because that’s the default way that R does it. But in real life, it’s really easy to forget which way you set things up, so you have to put a bit of effort into remembering!</p>
<p>Okay, so let’s have a look at our students when we rank them from worst to best in terms of effort and reward:</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>rank (hours worked)</th>
<th>rank (grade received)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>student</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="even">
<td>student</td>
<td>2</td>
<td>10</td>
</tr>
<tr class="odd">
<td>student</td>
<td>3</td>
<td>6</td>
</tr>
<tr class="even">
<td>student</td>
<td>4</td>
<td>2</td>
</tr>
<tr class="odd">
<td>student</td>
<td>5</td>
<td>3</td>
</tr>
<tr class="even">
<td>student</td>
<td>6</td>
<td>5</td>
</tr>
<tr class="odd">
<td>student</td>
<td>7</td>
<td>4</td>
</tr>
<tr class="even">
<td>student</td>
<td>8</td>
<td>8</td>
</tr>
<tr class="odd">
<td>student</td>
<td>9</td>
<td>7</td>
</tr>
<tr class="even">
<td>student</td>
<td>10</td>
<td>9</td>
</tr>
</tbody>
</table>
<p>Hm. These are <em>identical</em>. The student who put in the most effort got the best grade, the student with the least effort got the worst grade, etc. We can get R to construct these rankings using the <code>rank()</code> function, like this:</p>
<pre><code>&gt; hours.rank &lt;- rank( effort$hours )   # rank students by hours worked
&gt; grade.rank &lt;- rank( effort$grade )   # rank students by grade received</code></pre>
<p>As the table above shows, these two rankings are identical, so if we now correlate them we get a perfect relationship:</p>
<pre><code>&gt; cor( hours.rank, grade.rank )
[1] 1</code></pre>
<p>What we’ve just re-invented is <strong><em>Spearman’s rank order correlation</em></strong>, usually denoted <span class="math inline">\(\rho\)</span> to distinguish it from the Pearson correlation <span class="math inline">\(r\)</span>. We can calculate Spearman’s <span class="math inline">\(\rho\)</span> using R in two different ways. Firstly we could do it the way I just showed, using the <code>rank()</code> function to construct the rankings, and then calculate the Pearson correlation on these ranks. However, that’s way too much effort to do every time. It’s much easier to just specify the <code>method</code> argument of the <code>cor()</code> function.</p>
<pre><code>&gt; cor( effort$hours, effort$grade, method = &quot;spearman&quot;)
[1] 1</code></pre>
<p>The default value of the <code>method</code> argument is <code>"pearson"</code>, which is why we didn’t have to specify it earlier on when we were doing Pearson correlations.</p>
</div>
<div id="the-correlate-function" class="section level3" number="8.4.7">
<h3><span class="header-section-number">8.4.7</span> The <code>correlate()</code> function</h3>
<p>As we’ve seen, the <code>cor()</code> function works pretty well, and handles many of the situations that you might be interested in. One thing that many beginners find frustrating, however, is the fact that it’s not built to handle non-numeric variables. From a statistical perspective, this is perfectly sensible: Pearson and Spearman correlations are only designed to work for numeric variables, so the <code>cor()</code> function spits out an error.</p>
<p>Here’s what I mean. Suppose you were keeping track of how many <code>hours</code> you worked in any given day, and counted how many <code>tasks</code> you completed. If you were doing the tasks for money, you might also want to keep track of how much <code>pay</code> you got for each job. It would also be sensible to keep track of the <code>weekday</code> on which you actually did the work: most of us don’t work as much on Saturdays or Sundays. If you did this for 7 weeks, you might end up with a data set that looks like this one:</p>
<pre><code>&gt; load(&quot;work.Rdata&quot;)

&gt; who(TRUE)
   -- Name --   -- Class --   -- Size --
   work         data.frame    49 x 7    
    $hours      numeric       49        
    $tasks      numeric       49        
    $pay        numeric       49        
    $day        integer       49        
    $weekday    factor        49        
    $week       numeric       49        
    $day.type   factor        49   
    
&gt; head(work)
  hours tasks pay day   weekday week day.type
1   7.2    14  41   1   Tuesday    1  weekday
2   7.4    11  39   2 Wednesday    1  weekday
3   6.6    14  13   3  Thursday    1  weekday
4   6.5    22  47   4    Friday    1  weekday
5   3.1     5   4   5  Saturday    1  weekend
6   3.0     7  12   6    Sunday    1  weekend</code></pre>
<p>Obviously, I’d like to know something about how all these variables correlate with one another. I could correlate <code>hours</code> with <code>pay</code> quite using <code>cor()</code>, like so:</p>
<pre><code>&gt; cor(work$hours,work$pay)
[1] 0.7604283</code></pre>
<p>But what if I wanted a quick and easy way to calculate all pairwise correlations between the numeric variables? I can’t just input the <code>work</code> data frame, because it contains two factor variables, <code>weekday</code> and <code>day.type</code>. If I try this, I get an error:</p>
<pre><code>&gt; cor(work)
Error in cor(work) : &#39;x&#39; must be numeric</code></pre>
<p>It order to get the correlations that I want using the <code>cor()</code> function, is create a new data frame that doesn’t contain the factor variables, and then feed that new data frame into the <code>cor()</code> function. It’s not actually very hard to do that, and I’ll talk about how to do it properly in Section <a href="#subsetdataframe"><strong>??</strong></a>. But it would be nice to have some function that is smart enough to just ignore the factor variables. That’s where the <code>correlate()</code> function in the <code>lsr</code> package can be handy. If you feed it a data frame that contains factors, it knows to ignore them, and returns the pairwise correlations only between the numeric variables:</p>
<pre><code>&gt; correlate(work)

CORRELATIONS
============
- correlation type:  pearson 
- correlations shown only when both variables are numeric

          hours  tasks   pay    day weekday   week day.type
hours         .  0.800 0.760 -0.049       .  0.018        .
tasks     0.800      . 0.720 -0.072       . -0.013        .
pay       0.760  0.720     .  0.137       .  0.196        .
day      -0.049 -0.072 0.137      .       .  0.990        .
weekday       .      .     .      .       .      .        .
week      0.018 -0.013 0.196  0.990       .      .        .
day.type      .      .     .      .       .      .        .</code></pre>
<p>The output here shows a <code>.</code> whenever one of the variables is non-numeric. It also shows a <code>.</code> whenever a variable is correlated with itself (it’s not a meaningful thing to do). The <code>correlate()</code> function can also do Spearman correlations, by specifying the <code>corr.method</code> to use:</p>
<pre><code>&gt; correlate( work, corr.method=&quot;spearman&quot; )

CORRELATIONS
============
- correlation type:  spearman 
- correlations shown only when both variables are numeric

          hours  tasks   pay    day weekday   week day.type
hours         .  0.805 0.745 -0.047       .  0.010        .
tasks     0.805      . 0.730 -0.068       . -0.008        .
pay       0.745  0.730     .  0.094       .  0.154        .
day      -0.047 -0.068 0.094      .       .  0.990        .
weekday       .      .     .      .       .      .        .
week      0.010 -0.008 0.154  0.990       .      .        .
day.type      .      .     .      .       .      .        .</code></pre>
<p>Obviously, there’s no new functionality in the <code>correlate()</code> function, and any advanced R user would be perfectly capable of using the <code>cor()</code> function to get these numbers out. But if you’re not yet comfortable with extracting a subset of a data frame, the <code>correlate()</code> function is for you.</p>
</div>
<div id="missing-values-in-pairwise-calculations-1" class="section level3" number="8.4.8">
<h3><span class="header-section-number">8.4.8</span> Missing values in pairwise calculations</h3>
<p>I mentioned earlier that the <code>cor()</code> function is a special case. It doesn’t have an <code>na.rm</code> argument, because the story becomes a lot more complicated when more than one variable is involved. What it does have is an argument called <code>use</code> which does roughly the same thing, but you need to think little more carefully about what you want this time. To illustrate the issues, let’s open up a data set that has missing values, <code>parenthood2.Rdata</code>. This file contains the same data as the original parenthood data, but with some values deleted. It contains a single data frame, <code>parenthood2</code>:</p>
<pre><code>&gt; load( &quot;parenthood2.Rdata&quot; )
&gt; print( parenthood2 )
  dan.sleep baby.sleep dan.grump day
1      7.59         NA        56   1
2      7.91      11.66        60   2
3      5.14       7.92        82   3
4      7.71       9.61        55   4
5      6.68       9.75        NA   5
6      5.99       5.04        72   6
BLAH BLAH BLAH</code></pre>
<p>If I calculate my descriptive statistics using the <code>describe()</code> function</p>
<pre><code>&gt; describe( parenthood2 )
           var   n  mean    sd median trimmed   mad   min    max    BLAH
dan.sleep    1  91  6.98  1.02   7.03    7.02  1.13  4.84   9.00    BLAH
baby.sleep   2  89  8.11  2.05   8.20    8.13  2.28  3.25  12.07    BLAH
dan.grump    3  92 63.15  9.85  61.00   62.66 10.38 41.00  89.00    BLAH
day          4 100 50.50 29.01  50.50   50.50 37.06  1.00 100.00    BLAH</code></pre>
<p>we can see from the <code>n</code> column that there are 9 missing values for <code>dan.sleep</code>, 11 missing values for <code>baby.sleep</code> and 8 missing values for <code>dan.grump</code>.<a href="#fn116" class="footnote-ref" id="fnref116"><sup>116</sup></a> Suppose what I would like is a correlation matrix. And let’s also suppose that I don’t bother to tell R how to handle those missing values. Here’s what happens:</p>
<pre><code>&gt; cor( parenthood2 )
           dan.sleep baby.sleep dan.grump day
dan.sleep          1         NA        NA  NA
baby.sleep        NA          1        NA  NA
dan.grump         NA         NA         1  NA
day               NA         NA        NA   1</code></pre>
<p>Annoying, but it kind of makes sense. If I don’t <em>know</em> what some of the values of <code>dan.sleep</code> and <code>baby.sleep</code> actually are, then I can’t possibly <em>know</em> what the correlation between these two variables is either, since the formula for the correlation coefficient makes use of every single observation in the data set. Once again, it makes sense: it’s just not particularly <em>helpful</em>.</p>
<p>To make R behave more sensibly in this situation, you need to specify the <code>use</code> argument to the <code>cor()</code> function. There are several different values that you can specify for this, but the two that we care most about in practice tend to be <code>"complete.obs"</code> and <code>"pairwise.complete.obs"</code>. If we specify <code>use = "complete.obs"</code>, R will completely ignore all cases (i.e., all rows in our <code>parenthood2</code> data frame) that have any missing values at all. So, for instance, if you look back at the extract earlier when I used the <code>head()</code> function, notice that observation 1 (i.e., day 1) of the <code>parenthood2</code> data set is missing the value for <code>baby.sleep</code>, but is otherwise complete? Well, if you choose <code>use = "complete.obs"</code> R will ignore that row completely: that is, even when it’s trying to calculate the correlation between <code>dan.sleep</code> and <code>dan.grump</code>, observation 1 will be ignored, because the value of <code>baby.sleep</code> is missing for that observation. Here’s what we get:</p>
<pre><code>&gt; cor(parenthood2, use = &quot;complete.obs&quot;)
             dan.sleep baby.sleep   dan.grump         day
dan.sleep   1.00000000  0.6394985 -0.89951468  0.06132891
baby.sleep  0.63949845  1.0000000 -0.58656066  0.14555814
dan.grump  -0.89951468 -0.5865607  1.00000000 -0.06816586
day         0.06132891  0.1455581 -0.06816586  1.00000000</code></pre>
<p>The other possibility that we care about, and the one that tends to get used more often in practice, is to set <code>use = "pairwise.complete.obs"</code>. When we do that, R only looks at the variables that it’s trying to correlate when determining what to drop. So, for instance, since the only missing value for observation 1 of <code>parenthood2</code> is for <code>baby.sleep</code> R will only drop observation 1 when <code>baby.sleep</code> is one of the variables involved: and so R keeps observation 1 when trying to correlate <code>dan.sleep</code> and <code>dan.grump</code>. When we do it this way, here’s what we get:</p>
<pre><code>&gt; cor(parenthood2, use = &quot;pairwise.complete.obs&quot;) 
             dan.sleep  baby.sleep    dan.grump          day
dan.sleep   1.00000000  0.61472303 -0.903442442 -0.076796665
baby.sleep  0.61472303  1.00000000 -0.567802669  0.058309485
dan.grump  -0.90344244 -0.56780267  1.000000000  0.005833399
day        -0.07679667  0.05830949  0.005833399  1.000000000</code></pre>
<p>Similar, but not quite the same. It’s also worth noting that the <code>correlate()</code> function (in the <code>lsr</code> package) automatically uses the “pairwise complete” method:</p>
<pre><code>&gt; correlate(parenthood2)

CORRELATIONS
============
- correlation type:  pearson 
- correlations shown only when both variables are numeric

           dan.sleep baby.sleep dan.grump    day
dan.sleep          .      0.615    -0.903 -0.077
baby.sleep     0.615          .    -0.568  0.058
dan.grump     -0.903     -0.568         .  0.006
day           -0.077      0.058     0.006      .</code></pre>
<p>The two approaches have different strengths and weaknesses. The “pairwise complete” approach has the advantage that it keeps more observations, so you’re making use of more of your data and (as we’ll discuss in tedious detail in Chapter <a href="inferential-statistics-the-central-limit-theorem.html#estimation">4.2</a> and it improves the reliability of your estimated correlation. On the other hand, it means that every correlation in your correlation matrix is being computed from a slightly different set of observations, which can be awkward when you want to compare the different correlations that you’ve got.</p>
<p>So which method should you use? It depends a lot on <em>why</em> you think your values are missing, and probably depends a little on how paranoid you are. For instance, if you think that the missing values were “chosen” completely randomly<a href="#fn117" class="footnote-ref" id="fnref117"><sup>117</sup></a> then you’ll probably want to use the pairwise method. If you think that missing data are a cue to thinking that the whole observation might be rubbish (e.g., someone just selecting arbitrary responses in your questionnaire), but that there’s no pattern to which observations are “rubbish” then it’s probably safer to keep only those observations that are complete. If you think there’s something systematic going on, in that some observations are more likely to be missing than others, then you have a much trickier problem to solve, and one that is beyond the scope of this book.</p>
</div>
</div>
<div id="introregression" class="section level2" number="8.5">
<h2><span class="header-section-number">8.5</span> Linear regression</h2>
<p>Text by <span class="citation"><a href="#ref-Navarro2018" role="doc-biblioref">Navarro</a> (<a href="#ref-Navarro2018" role="doc-biblioref">2018</a>)</span></p>
<div class="figure"><span style="display:block;" id="fig:regression0"></span>
<img src="schuster-statistics-remix_files/figure-html/regression0-1.png" alt="Scatterplot showing grumpiness as a function of hours slept." width="672" />
<p class="caption">
Figure 8.8: Scatterplot showing grumpiness as a function of hours slept.
</p>
</div>
<div class="figure"><span style="display:block;" id="fig:regression1a"></span>
<img src="schuster-statistics-remix_files/figure-html/regression1a-1.png" alt="Panel a shows the sleep-grumpiness scatterplot from above with the best fitting regression line drawn over the top. Not surprisingly, the line goes through the middle of the data. " width="672" />
<p class="caption">
Figure 8.9: Panel a shows the sleep-grumpiness scatterplot from above with the best fitting regression line drawn over the top. Not surprisingly, the line goes through the middle of the data.
</p>
</div>
<div class="figure"><span style="display:block;" id="fig:regression1b"></span>
<img src="schuster-statistics-remix_files/figure-html/regression1b-1.png" alt="In contrast, this plot shows the same data, but with a very poor choice of regression line drawn over the top." width="672" />
<p class="caption">
Figure 8.10: In contrast, this plot shows the same data, but with a very poor choice of regression line drawn over the top.
</p>
</div>
<p>Since the basic ideas in regression are closely tied to correlation, we’ll return to the <code>parenthood.Rdata</code> file that we were using to illustrate how correlations work. Recall that, in this data set, we were trying to find out why Dan is so very grumpy all the time, and our working hypothesis was that I’m not getting enough sleep. We drew some scatterplots to help us examine the relationship between the amount of sleep I get, and my grumpiness the following day. The actual scatterplot that we draw is the one shown in Figure <a href="regression.html#fig:regression0">8.8</a>, and as we saw previously this corresponds to a correlation of <span class="math inline">\(r=-.90\)</span>, but what we find ourselves secretly imagining is something that looks closer to Figure <a href="regression.html#fig:regression1a">8.9</a>. That is, we mentally draw a straight line through the middle of the data. In statistics, this line that we’re drawing is called a <strong><em>regression line</em></strong>. Notice that the regression line goes through the middle of the data. We don’t find ourselves imagining anything like the rather silly plot shown in Figure <a href="regression.html#fig:regression1b">8.10</a>.</p>
<p>This is not highly surprising: the line that I’ve drawn in Figure <a href="regression.html#fig:regression1b">8.10</a> doesn’t “fit” the data very well, so it doesn’t make a lot of sense to propose it as a way of summarising the data, right? This is a very simple observation to make, but it turns out to be very powerful when we start trying to wrap just a little bit of maths around it. To do so, let’s start with a refresher of some high school maths. The formula for a straight line is usually written like this:
<span class="math display">\[
y = mx + c
\]</span>
Or, at least, that’s what it was when I went to high school all those years ago. The two <em>variables</em> are <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, and we have two <em>coefficients</em>, <span class="math inline">\(m\)</span> and <span class="math inline">\(c\)</span>. The coefficient <span class="math inline">\(m\)</span> represents the <em>slope</em> of the line, and the coefficient <span class="math inline">\(c\)</span> represents the <em><span class="math inline">\(y\)</span>-intercept</em> of the line. Digging further back into our decaying memories of high school (sorry, for some of us high school was a long time ago), we remember that the intercept is interpreted as “the value of <span class="math inline">\(y\)</span> that you get when <span class="math inline">\(x=0\)</span>.” Similarly, a slope of <span class="math inline">\(m\)</span> means that if you increase the <span class="math inline">\(x\)</span>-value by 1 unit, then the <span class="math inline">\(y\)</span>-value goes up by <span class="math inline">\(m\)</span> units; a negative slope means that the <span class="math inline">\(y\)</span>-value would go down rather than up. Ah yes, it’s all coming back to me now.</p>
<p>Now that we’ve remembered that, it should come as no surprise to discover that we use the exact same formula to describe a regression line. If <span class="math inline">\(Y\)</span> is the outcome variable (the DV) and <span class="math inline">\(X\)</span> is the predictor variable (the IV), then the formula that describes our regression is written like this:
<span class="math display">\[
\hat{Y_i} = b_1 X_i + b_0
\]</span>
Hm. Looks like the same formula, but there’s some extra frilly bits in this version. Let’s make sure we understand them. Firstly, notice that I’ve written <span class="math inline">\(X_i\)</span> and <span class="math inline">\(Y_i\)</span> rather than just plain old <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. This is because we want to remember that we’re dealing with actual data. In this equation, <span class="math inline">\(X_i\)</span> is the value of predictor variable for the <span class="math inline">\(i\)</span>th observation (i.e., the number of hours of sleep that I got on day <span class="math inline">\(i\)</span> of my little study), and <span class="math inline">\(Y_i\)</span> is the corresponding value of the outcome variable (i.e., my grumpiness on that day). And although I haven’t said so explicitly in the equation, what we’re assuming is that this formula works for all observations in the data set (i.e., for all <span class="math inline">\(i\)</span>). Secondly, notice that I wrote <span class="math inline">\(\hat{Y}_i\)</span> and not <span class="math inline">\(Y_i\)</span>. This is because we want to make the distinction between the <em>actual data</em> <span class="math inline">\(Y_i\)</span>, and the <em>estimate</em> <span class="math inline">\(\hat{Y}_i\)</span> (i.e., the prediction that our regression line is making). Thirdly, I changed the letters used to describe the coefficients from <span class="math inline">\(m\)</span> and <span class="math inline">\(c\)</span> to <span class="math inline">\(b_1\)</span> and <span class="math inline">\(b_0\)</span>. That’s just the way that statisticians like to refer to the coefficients in a regression model. I’ve no idea why they chose <span class="math inline">\(b\)</span>, but that’s what they did. In any case <span class="math inline">\(b_0\)</span> always refers to the intercept term, and <span class="math inline">\(b_1\)</span> refers to the slope.</p>
<p>Excellent, excellent. Next, I can’t help but notice that – regardless of whether we’re talking about the good regression line or the bad one – the data don’t fall perfectly on the line. Or, to say it another way, the data <span class="math inline">\(Y_i\)</span> are not identical to the predictions of the regression model <span class="math inline">\(\hat{Y_i}\)</span>. Since statisticians love to attach letters, names and numbers to everything, let’s refer to the difference between the model prediction and that actual data point as a <em>residual</em>, and we’ll refer to it as <span class="math inline">\(\epsilon_i\)</span>.<a href="#fn118" class="footnote-ref" id="fnref118"><sup>118</sup></a> Written using mathematics, the residuals are defined as:
<span class="math display">\[
\epsilon_i = Y_i - \hat{Y}_i
\]</span>
which in turn means that we can write down the complete linear regression model as:
<span class="math display">\[
Y_i = b_1 X_i + b_0 + \epsilon_i
\]</span></p>
</div>
<div id="regressionestimation" class="section level2" number="8.6">
<h2><span class="header-section-number">8.6</span> Estimating a linear regression model</h2>
<div class="figure"><span style="display:block;" id="fig:regression3a"></span>
<img src="schuster-statistics-remix_files/figure-html/regression3a-1.png" alt="A depiction of the residuals associated with the best fitting regression line" width="672" />
<p class="caption">
Figure 8.11: A depiction of the residuals associated with the best fitting regression line
</p>
</div>
<div class="figure"><span style="display:block;" id="fig:regression3b"></span>
<img src="schuster-statistics-remix_files/figure-html/regression3b-1.png" alt="The residuals associated with a poor regression line" width="672" />
<p class="caption">
Figure 8.12: The residuals associated with a poor regression line
</p>
</div>
<p>Okay, now let’s redraw our pictures, but this time I’ll add some lines to show the size of the residual for all observations. When the regression line is good, our residuals (the lengths of the solid black lines) all look pretty small, as shown in Figure <a href="regression.html#fig:regression3a">8.11</a>, but when the regression line is a bad one, the residuals are a lot larger, as you can see from looking at Figure <a href="regression.html#fig:regression3b">8.12</a>. Hm. Maybe what we “want” in a regression model is <em>small</em> residuals. Yes, that does seem to make sense. In fact, I think I’ll go so far as to say that the “best fitting” regression line is the one that has the smallest residuals. Or, better yet, since statisticians seem to like to take squares of everything why not say that …</p>
<blockquote>
<p>The estimated regression coefficients, <span class="math inline">\(\hat{b}_0\)</span> and <span class="math inline">\(\hat{b}_1\)</span> are those that minimise the sum of the squared residuals, which we could either write as <span class="math inline">\(\sum_i (Y_i - \hat{Y}_i)^2\)</span> or as <span class="math inline">\(\sum_i {\epsilon_i}^2\)</span>.</p>
</blockquote>
<p>Yes, yes that sounds even better. And since I’ve indented it like that, it probably means that this is the right answer. And since this is the right answer, it’s probably worth making a note of the fact that our regression coefficients are <em>estimates</em> (we’re trying to guess the parameters that describe a population!), which is why I’ve added the little hats, so that we get <span class="math inline">\(\hat{b}_0\)</span> and <span class="math inline">\(\hat{b}_1\)</span> rather than <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span>. Finally, I should also note that – since there’s actually more than one way to estimate a regression model – the more technical name for this estimation process is <strong><em>ordinary least squares (OLS) regression</em></strong>.</p>
<p>At this point, we now have a concrete definition for what counts as our “best” choice of regression coefficients, <span class="math inline">\(\hat{b}_0\)</span> and <span class="math inline">\(\hat{b}_1\)</span>. The natural question to ask next is, if our optimal regression coefficients are those that minimise the sum squared residuals, how do we <em>find</em> these wonderful numbers? The actual answer to this question is complicated, and it doesn’t help you understand the logic of regression.<a href="#fn119" class="footnote-ref" id="fnref119"><sup>119</sup></a> As a result, this time I’m going to let you off the hook. Instead of showing you how to do it the long and tedious way first, and then “revealing” the wonderful shortcut that R provides you with, let’s cut straight to the chase… and use the <code>lm()</code> function (short for “linear model”) to do all the heavy lifting.</p>
<div id="lm" class="section level3" number="8.6.1">
<h3><span class="header-section-number">8.6.1</span> Using the <code>lm()</code> function</h3>
<p>The <code>lm()</code> function is a fairly complicated one: if you type <code>?lm</code>, the help files will reveal that there are a lot of arguments that you can specify, and most of them won’t make a lot of sense to you. At this stage however, there’s really only two of them that you care about, and as it turns out you’ve seen them before:</p>
<ul>
<li><code>formula</code>. A formula that specifies the regression model. For the simple linear regression models that we’ve talked about so far, in which you have a single predictor variable as well as an intercept term, this formula is of the form <code>outcome ~ predictor</code>. However, more complicated formulas are allowed, and we’ll discuss them later.</li>
<li><code>data</code>. The data frame containing the variables.</li>
</ul>
<p>The output of the <code>lm()</code> function is a fairly complicated object, with quite a lot of technical information buried under the hood. Because this technical information is used by other functions, it’s generally a good idea to create a variable that stores the results of your regression. With this in mind, to run my linear regression, the command I want to use is this:</p>
<div class="sourceCode" id="cb672"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb672-1"><a href="regression.html#cb672-1" aria-hidden="true" tabindex="-1"></a>regression<span class="fl">.1</span> <span class="ot">&lt;-</span> <span class="fu">lm</span>( <span class="at">formula =</span> dan.grump <span class="sc">~</span> dan.sleep,  </span>
<span id="cb672-2"><a href="regression.html#cb672-2" aria-hidden="true" tabindex="-1"></a>                    <span class="at">data =</span> parenthood )  </span></code></pre></div>
<p>Note that I used <code>dan.grump ~ dan.sleep</code> as the formula: in the model that I’m trying to estimate, <code>dan.grump</code> is the <em>outcome</em> variable, and <code>dan.sleep</code> is the predictor variable. It’s always a good idea to remember which one is which! Anyway, what this does is create an “<code>lm</code> object” (i.e., a variable whose class is <code>"lm"</code>) called <code>regression.1</code>. Let’s have a look at what happens when we <code>print()</code> it out:</p>
<div class="sourceCode" id="cb673"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb673-1"><a href="regression.html#cb673-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>( regression<span class="fl">.1</span> )</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = dan.grump ~ dan.sleep, data = parenthood)
## 
## Coefficients:
## (Intercept)    dan.sleep  
##     125.956       -8.937</code></pre>
<p>This looks promising. There’s two separate pieces of information here. Firstly, R is politely reminding us what the command was that we used to specify the model in the first place, which can be helpful. More importantly from our perspective, however, is the second part, in which R gives us the intercept <span class="math inline">\(\hat{b}_0 = 125.96\)</span> and the slope <span class="math inline">\(\hat{b}_1 = -8.94\)</span>. In other words, the best-fitting regression line that I plotted in Figure <a href="regression.html#fig:regression1a">8.9</a> has this formula:
<span class="math display">\[
\hat{Y}_i = -8.94 \ X_i + 125.96
\]</span></p>
</div>
<div id="interpreting-the-estimated-model" class="section level3" number="8.6.2">
<h3><span class="header-section-number">8.6.2</span> Interpreting the estimated model</h3>
<p>The most important thing to be able to understand is how to interpret these coefficients. Let’s start with <span class="math inline">\(\hat{b}_1\)</span>, the slope. If we remember the definition of the slope, a regression coefficient of <span class="math inline">\(\hat{b}_1 = -8.94\)</span> means that if I increase <span class="math inline">\(X_i\)</span> by 1, then I’m decreasing <span class="math inline">\(Y_i\)</span> by 8.94. That is, each additional hour of sleep that I gain will improve my mood, reducing my grumpiness by 8.94 grumpiness points. What about the intercept? Well, since <span class="math inline">\(\hat{b}_0\)</span> corresponds to “the expected value of <span class="math inline">\(Y_i\)</span> when <span class="math inline">\(X_i\)</span> equals 0,” it’s pretty straightforward. It implies that if I get zero hours of sleep (<span class="math inline">\(X_i =0\)</span>) then my grumpiness will go off the scale, to an insane value of (<span class="math inline">\(Y_i = 125.96\)</span>). Best to be avoided, I think.</p>
</div>
</div>
<div id="multipleregression" class="section level2" number="8.7">
<h2><span class="header-section-number">8.7</span> Multiple linear regression</h2>
<p>The simple linear regression model that we’ve discussed up to this point assumes that there’s a single predictor variable that you’re interested in, in this case <code>dan.sleep</code>. In fact, up to this point, <em>every</em> statistical tool that we’ve talked about has assumed that your analysis uses one predictor variable and one outcome variable. However, in many (perhaps most) research projects you actually have multiple predictors that you want to examine. If so, it would be nice to be able to extend the linear regression framework to be able to include multiple predictors. Perhaps some kind of <strong><em>multiple regression</em></strong> model would be in order?</p>
<p>Multiple regression is conceptually very simple. All we do is add more terms to our regression equation. Let’s suppose that we’ve got two variables that we’re interested in; perhaps we want to use both <code>dan.sleep</code> and <code>baby.sleep</code> to predict the <code>dan.grump</code> variable. As before, we let <span class="math inline">\(Y_i\)</span> refer to my grumpiness on the <span class="math inline">\(i\)</span>-th day. But now we have two <span class="math inline">\(X\)</span> variables: the first corresponding to the amount of sleep I got and the second corresponding to the amount of sleep my son got. So we’ll let <span class="math inline">\(X_{i1}\)</span> refer to the hours I slept on the <span class="math inline">\(i\)</span>-th day, and <span class="math inline">\(X_{i2}\)</span> refers to the hours that the baby slept on that day. If so, then we can write our regression model like this:
<span class="math display">\[
Y_i = b_2 X_{i2} + b_1 X_{i1} + b_0 + \epsilon_i
\]</span>
As before, <span class="math inline">\(\epsilon_i\)</span> is the residual associated with the <span class="math inline">\(i\)</span>-th observation, <span class="math inline">\(\epsilon_i = {Y}_i - \hat{Y}_i\)</span>. In this model, we now have three coefficients that need to be estimated: <span class="math inline">\(b_0\)</span> is the intercept, <span class="math inline">\(b_1\)</span> is the coefficient associated with my sleep, and <span class="math inline">\(b_2\)</span> is the coefficient associated with my son’s sleep. However, although the number of coefficients that need to be estimated has changed, the basic idea of how the estimation works is unchanged: our estimated coefficients <span class="math inline">\(\hat{b}_0\)</span>, <span class="math inline">\(\hat{b}_1\)</span> and <span class="math inline">\(\hat{b}_2\)</span> are those that minimise the sum squared residuals.</p>
<div class="figure"><span style="display:block;" id="fig:multipleregression"></span>
<img src="img/regression2/scatter3d_full1.png" alt="A 3D visualisation of a multiple regression model. There are two predictors in the model, `dan.sleep` and `baby.sleep`; the outcome variable is `dan.grump`. Together, these three variables form a 3D space: each observation (blue dots) is a point in this space. In much the same way that a simple linear regression model forms a line in 2D space, this multiple regression model forms a plane in 3D space. When we estimate the regression coefficients, what we're trying to do is find a plane that is as close to all the blue dots as possible." width="760" />
<p class="caption">
Figure 8.13: A 3D visualisation of a multiple regression model. There are two predictors in the model, <code>dan.sleep</code> and <code>baby.sleep</code>; the outcome variable is <code>dan.grump</code>. Together, these three variables form a 3D space: each observation (blue dots) is a point in this space. In much the same way that a simple linear regression model forms a line in 2D space, this multiple regression model forms a plane in 3D space. When we estimate the regression coefficients, what we’re trying to do is find a plane that is as close to all the blue dots as possible.
</p>
</div>
<div id="doing-it-in-r" class="section level3" number="8.7.1">
<h3><span class="header-section-number">8.7.1</span> Doing it in R</h3>
<p>Multiple regression in R is no different to simple regression: all we have to do is specify a more complicated <code>formula</code> when using the <code>lm()</code> function. For example, if we want to use both <code>dan.sleep</code> and <code>baby.sleep</code> as predictors in our attempt to explain why I’m so grumpy, then the formula we need is this:</p>
<pre><code>                    dan.grump ~ dan.sleep + baby.sleep</code></pre>
<p>Notice that, just like last time, I haven’t explicitly included any reference to the intercept term in this formula; only the two predictor variables and the outcome. By default, the <code>lm()</code> function assumes that the model should include an intercept (though you can get rid of it if you want). In any case, I can create a new regression model – which I’ll call <code>regression.2</code> – using the following command:</p>
<div class="sourceCode" id="cb676"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb676-1"><a href="regression.html#cb676-1" aria-hidden="true" tabindex="-1"></a>regression<span class="fl">.2</span> <span class="ot">&lt;-</span> <span class="fu">lm</span>( <span class="at">formula =</span> dan.grump <span class="sc">~</span> dan.sleep <span class="sc">+</span> baby.sleep,  </span>
<span id="cb676-2"><a href="regression.html#cb676-2" aria-hidden="true" tabindex="-1"></a>                     <span class="at">data =</span> parenthood )</span></code></pre></div>
<p>And just like last time, if we <code>print()</code> out this regression model we can see what the estimated regression coefficients are:</p>
<div class="sourceCode" id="cb677"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb677-1"><a href="regression.html#cb677-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>( regression<span class="fl">.2</span> )</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = dan.grump ~ dan.sleep + baby.sleep, data = parenthood)
## 
## Coefficients:
## (Intercept)    dan.sleep   baby.sleep  
##   125.96557     -8.95025      0.01052</code></pre>
<p>The coefficient associated with <code>dan.sleep</code> is quite large, suggesting that every hour of sleep I lose makes me a lot grumpier. However, the coefficient for <code>baby.sleep</code> is very small, suggesting that it doesn’t really matter how much sleep my son gets; not really. What matters as far as my grumpiness goes is how much sleep <em>I</em> get. To get a sense of what this multiple regression model looks like, Figure <a href="regression.html#fig:multipleregression">8.13</a> shows a 3D plot that plots all three variables, along with the regression model itself.</p>
</div>
<div id="formula-for-the-general-case" class="section level3" number="8.7.2">
<h3><span class="header-section-number">8.7.2</span> Formula for the general case</h3>
<p>The equation that I gave above shows you what a multiple regression model looks like when you include two predictors. Not surprisingly, then, if you want more than two predictors all you have to do is add more <span class="math inline">\(X\)</span> terms and more <span class="math inline">\(b\)</span> coefficients. In other words, if you have <span class="math inline">\(K\)</span> predictor variables in the model then the regression equation looks like this:
<span class="math display">\[
Y_i = \left( \sum_{k=1}^K b_{k} X_{ik} \right) + b_0 + \epsilon_i
\]</span></p>
</div>
</div>
<div id="r2" class="section level2" number="8.8">
<h2><span class="header-section-number">8.8</span> Quantifying the fit of the regression model</h2>
<p>So we now know how to estimate the coefficients of a linear regression model. The problem is, we don’t yet know if this regression model is any good. For example, the <code>regression.1</code> model <em>claims</em> that every hour of sleep will improve my mood by quite a lot, but it might just be rubbish. Remember, the regression model only produces a prediction <span class="math inline">\(\hat{Y}_i\)</span> about what my mood is like: my actual mood is <span class="math inline">\(Y_i\)</span>. If these two are very close, then the regression model has done a good job. If they are very different, then it has done a bad job.</p>
<div id="the-r2-value" class="section level3" number="8.8.1">
<h3><span class="header-section-number">8.8.1</span> The <span class="math inline">\(R^2\)</span> value</h3>
<p>Once again, let’s wrap a little bit of mathematics around this. Firstly, we’ve got the sum of the squared residuals:
<span class="math display">\[
\mbox{SS}_{res} = \sum_i (Y_i - \hat{Y}_i)^2
\]</span>
which we would hope to be pretty small. Specifically, what we’d like is for it to be very small in comparison to the total variability in the outcome variable,
<span class="math display">\[
\mbox{SS}_{tot} = \sum_i (Y_i - \bar{Y})^2
\]</span>
While we’re here, let’s calculate these values in R. Firstly, in order to make my R commands look a bit more similar to the mathematical equations, I’ll create variables <code>X</code> and <code>Y</code>:</p>
<div class="sourceCode" id="cb679"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb679-1"><a href="regression.html#cb679-1" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> parenthood<span class="sc">$</span>dan.sleep  <span class="co"># the predictor</span></span>
<span id="cb679-2"><a href="regression.html#cb679-2" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> parenthood<span class="sc">$</span>dan.grump  <span class="co"># the outcome</span></span></code></pre></div>
<p>Now that we’ve done this, let’s calculate the <span class="math inline">\(\hat{Y}\)</span> values and store them in a variable called <code>Y.pred</code>. For the simple model that uses only a single predictor, <code>regression.1</code>, we would do the following:</p>
<div class="sourceCode" id="cb680"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb680-1"><a href="regression.html#cb680-1" aria-hidden="true" tabindex="-1"></a>Y.pred <span class="ot">&lt;-</span> <span class="sc">-</span><span class="fl">8.94</span> <span class="sc">*</span> X  <span class="sc">+</span>  <span class="fl">125.97</span></span></code></pre></div>
<p>Okay, now that we’ve got a variable which stores the regression model predictions for how grumpy I will be on any given day, let’s calculate our sum of squared residuals. We would do that using the following command:</p>
<div class="sourceCode" id="cb681"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb681-1"><a href="regression.html#cb681-1" aria-hidden="true" tabindex="-1"></a>SS.resid <span class="ot">&lt;-</span> <span class="fu">sum</span>( (Y <span class="sc">-</span> Y.pred)<span class="sc">^</span><span class="dv">2</span> )</span>
<span id="cb681-2"><a href="regression.html#cb681-2" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>( SS.resid )</span></code></pre></div>
<pre><code>## [1] 1838.722</code></pre>
<p>Wonderful. A big number that doesn’t mean very much. Still, let’s forge boldly onwards anyway, and calculate the total sum of squares as well. That’s also pretty simple:</p>
<div class="sourceCode" id="cb683"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb683-1"><a href="regression.html#cb683-1" aria-hidden="true" tabindex="-1"></a>SS.tot <span class="ot">&lt;-</span> <span class="fu">sum</span>( (Y <span class="sc">-</span> <span class="fu">mean</span>(Y))<span class="sc">^</span><span class="dv">2</span> )</span>
<span id="cb683-2"><a href="regression.html#cb683-2" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>( SS.tot )</span></code></pre></div>
<pre><code>## [1] 9998.59</code></pre>
<p>Hm. Well, it’s a much bigger number than the last one, so this does suggest that our regression model was making good predictions. But it’s not very interpretable.</p>
<p>Perhaps we can fix this. What we’d like to do is to convert these two fairly meaningless numbers into one number. A nice, interpretable number, which for no particular reason we’ll call <span class="math inline">\(R^2\)</span>. What we would like is for the value of <span class="math inline">\(R^2\)</span> to be equal to 1 if the regression model makes no errors in predicting the data. In other words, if it turns out that the residual errors are zero – that is, if <span class="math inline">\(\mbox{SS}_{res} = 0\)</span> – then we expect <span class="math inline">\(R^2 = 1\)</span>. Similarly, if the model is completely useless, we would like <span class="math inline">\(R^2\)</span> to be equal to 0. What do I mean by “useless?” Tempting as it is demand that the regression model move out of the house, cut its hair and get a real job, I’m probably going to have to pick a more practical definition: in this case, all I mean is that the residual sum of squares is no smaller than the total sum of squares, <span class="math inline">\(\mbox{SS}_{res} = \mbox{SS}_{tot}\)</span>. Wait, why don’t we do exactly that? The formula that provides us with out <span class="math inline">\(R^2\)</span> value is pretty simple to write down,
<span class="math display">\[
R^2 = 1 - \frac{\mbox{SS}_{res}}{\mbox{SS}_{tot}}
\]</span>
and equally simple to calculate in R:</p>
<div class="sourceCode" id="cb685"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb685-1"><a href="regression.html#cb685-1" aria-hidden="true" tabindex="-1"></a>R.squared <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> (SS.resid <span class="sc">/</span> SS.tot)</span>
<span id="cb685-2"><a href="regression.html#cb685-2" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>( R.squared )</span></code></pre></div>
<pre><code>## [1] 0.8161018</code></pre>
<p>The <span class="math inline">\(R^2\)</span> value, sometimes called the <strong><em>coefficient of determination</em></strong><a href="#fn120" class="footnote-ref" id="fnref120"><sup>120</sup></a> has a simple interpretation: it is the <em>proportion</em> of the variance in the outcome variable that can be accounted for by the predictor. So in this case, the fact that we have obtained <span class="math inline">\(R^2 = .816\)</span> means that the predictor (<code>my.sleep</code>) explains 81.6% of the variance in the outcome (<code>my.grump</code>).</p>
<p>Naturally, you don’t actually need to type in all these commands yourself if you want to obtain the <span class="math inline">\(R^2\)</span> value for your regression model. As we’ll see later on in Section <a href="regression.html#regressionsummary">8.9.3</a>, all you need to do is use the <code>summary()</code> function. However, let’s put that to one side for the moment. There’s another property of <span class="math inline">\(R^2\)</span> that I want to point out.</p>
</div>
<div id="the-relationship-between-regression-and-correlation" class="section level3" number="8.8.2">
<h3><span class="header-section-number">8.8.2</span> The relationship between regression and correlation</h3>
<p>At this point we can revisit my earlier claim that regression, in this very simple form that I’ve discussed so far, is basically the same thing as a correlation. Previously, we used the symbol <span class="math inline">\(r\)</span> to denote a Pearson correlation. Might there be some relationship between the value of the correlation coefficient <span class="math inline">\(r\)</span> and the <span class="math inline">\(R^2\)</span> value from linear regression? Of course there is: the squared correlation <span class="math inline">\(r^2\)</span> is identical to the <span class="math inline">\(R^2\)</span> value for a linear regression with only a single predictor. To illustrate this, here’s the squared correlation:</p>
<div class="sourceCode" id="cb687"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb687-1"><a href="regression.html#cb687-1" aria-hidden="true" tabindex="-1"></a>r <span class="ot">&lt;-</span> <span class="fu">cor</span>(X, Y)  <span class="co"># calculate the correlation</span></span>
<span id="cb687-2"><a href="regression.html#cb687-2" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>( r<span class="sc">^</span><span class="dv">2</span> )    <span class="co"># print the squared correlation</span></span></code></pre></div>
<pre><code>## [1] 0.8161027</code></pre>
<p>Yep, same number. In other words, running a Pearson correlation is more or less equivalent to running a linear regression model that uses only one predictor variable.</p>
</div>
<div id="the-adjusted-r2-value" class="section level3" number="8.8.3">
<h3><span class="header-section-number">8.8.3</span> The adjusted <span class="math inline">\(R^2\)</span> value</h3>
<p>One final thing to point out before moving on. It’s quite common for people to report a slightly different measure of model performance, known as “adjusted <span class="math inline">\(R^2\)</span>.” The motivation behind calculating the adjusted <span class="math inline">\(R^2\)</span> value is the observation that adding more predictors into the model will <em>always</em> call the <span class="math inline">\(R^2\)</span> value to increase (or at least not decrease). The adjusted <span class="math inline">\(R^2\)</span> value introduces a slight change to the calculation, as follows. For a regression model with <span class="math inline">\(K\)</span> predictors, fit to a data set containing <span class="math inline">\(N\)</span> observations, the adjusted <span class="math inline">\(R^2\)</span> is:
<span class="math display">\[
\mbox{adj. } R^2 = 1 - \left(\frac{\mbox{SS}_{res}}{\mbox{SS}_{tot}} \times \frac{N-1}{N-K-1} \right)
\]</span>
This adjustment is an attempt to take the degrees of freedom into account. The big advantage of the adjusted <span class="math inline">\(R^2\)</span> value is that when you add more predictors to the model, the adjusted <span class="math inline">\(R^2\)</span> value will only increase if the new variables improve the model performance more than you’d expect by chance. The big disadvantage is that the adjusted <span class="math inline">\(R^2\)</span> value <em>can’t</em> be interpreted in the elegant way that <span class="math inline">\(R^2\)</span> can. <span class="math inline">\(R^2\)</span> has a simple interpretation as the proportion of variance in the outcome variable that is explained by the regression model; to my knowledge, no equivalent interpretation exists for adjusted <span class="math inline">\(R^2\)</span>.</p>
<p>An obvious question then, is whether you should report <span class="math inline">\(R^2\)</span> or adjusted <span class="math inline">\(R^2\)</span>. This is probably a matter of personal preference. If you care more about interpretability, then <span class="math inline">\(R^2\)</span> is better. If you care more about correcting for bias, then adjusted <span class="math inline">\(R^2\)</span> is probably better. Speaking just for myself, I prefer <span class="math inline">\(R^2\)</span>: my feeling is that it’s more important to be able to interpret your measure of model performance. Besides, as we’ll see in Section <a href="regression.html#regressiontests">8.9</a>, if you’re worried that the improvement in <span class="math inline">\(R^2\)</span> that you get by adding a predictor is just due to chance and not because it’s a better model, well, we’ve got hypothesis tests for that.</p>
</div>
</div>
<div id="regressiontests" class="section level2" number="8.9">
<h2><span class="header-section-number">8.9</span> Hypothesis tests for regression models</h2>
<p>So far we’ve talked about what a regression model is, how the coefficients of a regression model are estimated, and how we quantify the performance of the model (the last of these, incidentally, is basically our measure of effect size). The next thing we need to talk about is hypothesis tests. There are two different (but related) kinds of hypothesis tests that we need to talk about: those in which we test whether the regression model as a whole is performing significantly better than a null model; and those in which we test whether a particular regression coefficient is significantly different from zero.</p>
<p>At this point, you’re probably groaning internally, thinking that I’m going to introduce a whole new collection of tests…Me too. I’m so sick of hypothesis tests that I’m going to shamelessly reuse the <span class="math inline">\(F\)</span>-test from ANOVA and the <span class="math inline">\(t\)</span>-test. In fact, all I’m going to do in this section is show you how those tests are imported wholesale into the regression framework.</p>
<div id="testing-the-model-as-a-whole-the-omnibus-test" class="section level3" number="8.9.1">
<h3><span class="header-section-number">8.9.1</span> Testing the model as a whole: The omnibus test</h3>
<p>Okay, suppose you’ve estimated your regression model. The first hypothesis test you might want to try is one in which the null hypothesis that there is <em>no relationship</em> between the predictors and the outcome, and the alternative hypothesis is that <em>the data are distributed in exactly the way that the regression model predicts</em>. Formally, our “null model” corresponds to the fairly trivial “regression” model in which we include 0 predictors, and only include the intercept term <span class="math inline">\(b_0\)</span>
<span class="math display">\[
H_0: Y_i = b_0 + \epsilon_i
\]</span>
If our regression model has <span class="math inline">\(K\)</span> predictors, the “alternative model” is described using the usual formula for a multiple regression model:
<span class="math display">\[
H_1: Y_i = \left( \sum_{k=1}^K b_{k} X_{ik} \right) + b_0 + \epsilon_i
\]</span></p>
<p>How can we test these two hypotheses against each other? The trick is to understand that just like we did with ANOVA, it’s possible to divide up the total variance <span class="math inline">\(\mbox{SS}_{tot}\)</span> into the sum of the residual variance <span class="math inline">\(\mbox{SS}_{res}\)</span> and the regression model variance <span class="math inline">\(\mbox{SS}_{mod}\)</span>. I’ll skip over the technicalities, since we will cover most of them in a future ANOVA chapter, and just note that the sum of squares for the model is equal to the total sum of squares minus sum of squares for the residual:
<span class="math display">\[
\mbox{SS}_{mod} = \mbox{SS}_{tot} - \mbox{SS}_{res}
\]</span>
And, just like we will do with the ANOVA, we can convert the sums of squares in to mean squares by dividing by the degrees of freedom.
<span class="math display">\[
\begin{array}{rcl}
\mbox{MS}_{mod} &amp;=&amp; \displaystyle\frac{\mbox{SS}_{mod} }{df_{mod}} \\ \\
\mbox{MS}_{res} &amp;=&amp; \displaystyle\frac{\mbox{SS}_{res} }{df_{res} }
\end{array}
\]</span></p>
<p>So, how many degrees of freedom do we have? As you might expect, the <span class="math inline">\(df\)</span> associated with the model is closely tied to the number of predictors that we’ve included. In fact, it turns out that <span class="math inline">\(df_{mod} = K\)</span>. For the residuals, the total degrees of freedom is <span class="math inline">\(df_{res} = N -K - 1\)</span>.</p>
<p>Now that we’ve got our mean square values, you’re probably going to be entirely unsurprised to discover that we can calculate an <span class="math inline">\(F\)</span>-statistic like this:
<span class="math display">\[
F =  \frac{\mbox{MS}_{mod}}{\mbox{MS}_{res}}
\]</span>
and the degrees of freedom associated with this are <span class="math inline">\(K\)</span> and <span class="math inline">\(N-K-1\)</span>. This <span class="math inline">\(F\)</span> statistic has exactly the same interpretation as for ANOVA. Large <span class="math inline">\(F\)</span> values indicate that the null hypothesis is performing poorly in comparison to the alternative hypothesis. And since we already did some tedious “do it the long way” calculations back then, I won’t waste your time repeating them. In a moment I’ll show you how to do the test in R the easy way, but first, let’s have a look at the tests for the individual regression coefficients.</p>
</div>
<div id="tests-for-individual-coefficients" class="section level3" number="8.9.2">
<h3><span class="header-section-number">8.9.2</span> Tests for individual coefficients</h3>
<p>The <span class="math inline">\(F\)</span>-test that we’ve just introduced is useful for checking that the model as a whole is performing better than chance. This is important: if your regression model doesn’t produce a significant result for the <span class="math inline">\(F\)</span>-test then you probably don’t have a very good regression model (or, quite possibly, you don’t have very good data). However, while failing this test is a pretty strong indicator that the model has problems, <em>passing</em> the test (i.e., rejecting the null) doesn’t imply that the model is good! Why is that, you might be wondering? The answer to that can be found by looking at the coefficients for the <code>regression.2</code> model:</p>
<div class="sourceCode" id="cb689"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb689-1"><a href="regression.html#cb689-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>( regression<span class="fl">.2</span> ) </span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = dan.grump ~ dan.sleep + baby.sleep, data = parenthood)
## 
## Coefficients:
## (Intercept)    dan.sleep   baby.sleep  
##   125.96557     -8.95025      0.01052</code></pre>
<p>I can’t help but notice that the estimated regression coefficient for the <code>baby.sleep</code> variable is tiny (0.01), relative to the value that we get for <code>dan.sleep</code> (-8.95). Given that these two variables are absolutely on the same scale (they’re both measured in “hours slept”), I find this suspicious. In fact, I’m beginning to suspect that it’s really only the amount of sleep that <em>I</em> get that matters in order to predict my grumpiness.</p>
<p>Once again, we can reuse a hypothesis test that we discussed earlier, this time the <span class="math inline">\(t\)</span>-test. The test that we’re interested has a null hypothesis that the true regression coefficient is zero (<span class="math inline">\(b = 0\)</span>), which is to be tested against the alternative hypothesis that it isn’t (<span class="math inline">\(b \neq 0\)</span>). That is:
<span class="math display">\[
\begin{array}{rl}
H_0: &amp; b = 0 \\
H_1: &amp; b \neq 0 
\end{array}
\]</span>
How can we test this? Well, if the central limit theorem is kind to us, we might be able to guess that the sampling distribution of <span class="math inline">\(\hat{b}\)</span>, the estimated regression coefficient, is a normal distribution with mean centred on <span class="math inline">\(b\)</span>. What that would mean is that if the null hypothesis were true, then the sampling distribution of <span class="math inline">\(\hat{b}\)</span> has mean zero and unknown standard deviation. Assuming that we can come up with a good estimate for the standard error of the regression coefficient, <span class="math inline">\(\mbox{SE}({\hat{b}})\)</span>, then we’re in luck. That’s <em>exactly</em> the situation for which we introduced the one-sample <span class="math inline">\(t\)</span> way back in Chapter <a href="ttest.html#ttest">9</a>. So let’s define a <span class="math inline">\(t\)</span>-statistic like this,
<span class="math display">\[
t = \frac{\hat{b}}{\mbox{SE}({\hat{b})}}
\]</span>
I’ll skip over the reasons why, but our degrees of freedom in this case are <span class="math inline">\(df = N- K- 1\)</span>. Irritatingly, the estimate of the standard error of the regression coefficient, <span class="math inline">\(\mbox{SE}({\hat{b}})\)</span>, is not as easy to calculate as the standard error of the mean that we used for the simpler <span class="math inline">\(t\)</span>-tests and <span class="math inline">\(z\)</span>-tests. In fact, the formula is somewhat ugly, and not terribly helpful to look at.
For our purposes it’s sufficient to point out that the standard error of the estimated regression coefficient depends on both the predictor and outcome variables, and is somewhat sensitive to violations of the homogeneity of variance assumption (discussed shortly).</p>
<p>In any case, this <span class="math inline">\(t\)</span>-statistic can be interpreted in the same way as any <span class="math inline">\(t\)</span>-statistic. Assuming that you have a two-sided alternative (i.e., you don’t really care if <span class="math inline">\(b &gt;0\)</span> or <span class="math inline">\(b &lt; 0\)</span>), then it’s the extreme values of <span class="math inline">\(t\)</span> (i.e., a lot less than zero or a lot greater than zero) that suggest that you should reject the null hypothesis.</p>
</div>
<div id="regressionsummary" class="section level3" number="8.9.3">
<h3><span class="header-section-number">8.9.3</span> Running the hypothesis tests in R</h3>
<p>To compute all of the quantities that we have talked about so far, all you need to do is ask for a <code>summary()</code> of your regression model. Since I’ve been using <code>regression.2</code> as my example, let’s do that:</p>
<div class="sourceCode" id="cb691"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb691-1"><a href="regression.html#cb691-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>( regression<span class="fl">.2</span> )</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = dan.grump ~ dan.sleep + baby.sleep, data = parenthood)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -11.0345  -2.2198  -0.4016   2.6775  11.7496 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 125.96557    3.04095  41.423   &lt;2e-16 ***
## dan.sleep    -8.95025    0.55346 -16.172   &lt;2e-16 ***
## baby.sleep    0.01052    0.27106   0.039    0.969    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 4.354 on 97 degrees of freedom
## Multiple R-squared:  0.8161, Adjusted R-squared:  0.8123 
## F-statistic: 215.2 on 2 and 97 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>The output that this command produces is pretty dense, but we’ve already discussed everything of interest in it, so what I’ll do is go through it line by line. The first line reminds us of what the actual regression model is:</p>
<pre><code>Call:
lm(formula = dan.grump ~ dan.sleep + baby.sleep, data = parenthood)</code></pre>
<p>You can see why this is handy, since it was a little while back when we actually created the <code>regression.2</code> model, and so it’s nice to be reminded of what it was we were doing. The next part provides a quick summary of the residuals (i.e., the <span class="math inline">\(\epsilon_i\)</span> values),</p>
<pre><code>Residuals:
     Min       1Q   Median       3Q      Max 
-11.0345  -2.2198  -0.4016   2.6775  11.7496 </code></pre>
<p>which can be convenient as a quick and dirty check that the model is okay. Remember, we did assume that these residuals were normally distributed, with mean 0. In particular it’s worth quickly checking to see if the median is close to zero, and to see if the first quartile is about the same size as the third quartile. If they look badly off, there’s a good chance that the assumptions of regression are violated. These ones look pretty nice to me, so let’s move on to the interesting stuff. The next part of the R output looks at the coefficients of the regression model:</p>
<pre><code>Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 125.96557    3.04095  41.423   &lt;2e-16 ***
dan.sleep    -8.95025    0.55346 -16.172   &lt;2e-16 ***
baby.sleep    0.01052    0.27106   0.039    0.969 
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1</code></pre>
<p>Each row in this table refers to one of the coefficients in the regression model. The first row is the intercept term, and the later ones look at each of the predictors. The columns give you all of the relevant information. The first column is the actual estimate of <span class="math inline">\(b\)</span> (e.g., 125.96 for the intercept, and -8.9 for the <code>dan.sleep</code> predictor). The second column is the standard error estimate <span class="math inline">\(\hat\sigma_b\)</span>. The third column gives you the <span class="math inline">\(t\)</span>-statistic, and it’s worth noticing that in this table <span class="math inline">\(t= \hat{b}/\mbox{SE}({\hat{b}})\)</span> every time. Finally, the fourth column gives you the actual <span class="math inline">\(p\)</span> value for each of these tests.<a href="#fn121" class="footnote-ref" id="fnref121"><sup>121</sup></a> The only thing that the table itself doesn’t list is the degrees of freedom used in the <span class="math inline">\(t\)</span>-test, which is always <span class="math inline">\(N-K-1\)</span> and is listed immediately below, in this line:</p>
<pre><code>Residual standard error: 4.354 on 97 degrees of freedom</code></pre>
<p>The value of <span class="math inline">\(df = 97\)</span> is equal to <span class="math inline">\(N-K-1\)</span>, so that’s what we use for our <span class="math inline">\(t\)</span>-tests. In the final part of the output we have the <span class="math inline">\(F\)</span>-test and the <span class="math inline">\(R^2\)</span> values which assess the performance of the model as a whole</p>
<pre><code>Residual standard error: 4.354 on 97 degrees of freedom
Multiple R-squared: 0.8161, Adjusted R-squared: 0.8123 
F-statistic: 215.2 on 2 and 97 DF,  p-value: &lt; 2.2e-16 </code></pre>
<p>So in this case, the model performs significantly better than you’d expect by chance (<span class="math inline">\(F(2,97) = 215.2\)</span>, <span class="math inline">\(p&lt;.001\)</span>), which isn’t all that surprising: the <span class="math inline">\(R^2 = .812\)</span> value indicate that the regression model accounts for 81.2% of the variability in the outcome measure. However, when we look back up at the <span class="math inline">\(t\)</span>-tests for each of the individual coefficients, we lack evidence that the <code>baby.sleep</code> variable has a significant effect; all the work could be done by the <code>dan.sleep</code> variable. Taken together, these results suggest that <code>regression.2</code> is actually the wrong model for the data: you’d probably be better off dropping the <code>baby.sleep</code> predictor entirely. In other words, the <code>regression.1</code> model that we started with is the better model.</p>
</div>
</div>
<div id="corrhyp" class="section level2" number="8.10">
<h2><span class="header-section-number">8.10</span> Testing the significance of a correlation</h2>
<div id="hypothesis-tests-for-a-single-correlation" class="section level3" number="8.10.1">
<h3><span class="header-section-number">8.10.1</span> Hypothesis tests for a single correlation</h3>
<p>I don’t want to spend too much time on this, but it’s worth very briefly returning to the point I made earlier, that Pearson correlations are basically the same thing as linear regressions with only a single predictor added to the model. What this means is that the hypothesis tests that I just described in a regression context can also be applied to correlation coefficients. To see this, let’s take a <code>summary()</code> of the <code>regression.1</code> model:</p>
<div class="sourceCode" id="cb698"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb698-1"><a href="regression.html#cb698-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>( regression<span class="fl">.1</span> )</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = dan.grump ~ dan.sleep, data = parenthood)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -11.025  -2.213  -0.399   2.681  11.750 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 125.9563     3.0161   41.76   &lt;2e-16 ***
## dan.sleep    -8.9368     0.4285  -20.85   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 4.332 on 98 degrees of freedom
## Multiple R-squared:  0.8161, Adjusted R-squared:  0.8142 
## F-statistic: 434.9 on 1 and 98 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>The important thing to note here is the <span class="math inline">\(t\)</span> test associated with the predictor, in which we get a result of <span class="math inline">\(t(98) = -20.85\)</span>, <span class="math inline">\(p&lt;.001\)</span>. Now let’s compare this to the output of a different function, which goes by the name of <code>cor.test()</code>. As you might expect, this function runs a hypothesis test to see if the observed correlation between two variables is significantly different from 0. Let’s have a look:</p>
<div class="sourceCode" id="cb700"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb700-1"><a href="regression.html#cb700-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cor.test</span>( <span class="at">x =</span> parenthood<span class="sc">$</span>dan.sleep, <span class="at">y =</span> parenthood<span class="sc">$</span>dan.grump )</span></code></pre></div>
<pre><code>## 
##  Pearson&#39;s product-moment correlation
## 
## data:  parenthood$dan.sleep and parenthood$dan.grump
## t = -20.854, df = 98, p-value &lt; 2.2e-16
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  -0.9340614 -0.8594714
## sample estimates:
##       cor 
## -0.903384</code></pre>
<p>Again, the key thing to note is the line that reports the hypothesis test itself, which seems to be saying that <span class="math inline">\(t(98) = -20.85\)</span>, <span class="math inline">\(p&lt;.001\)</span>. Hm. Looks like it’s exactly the same test, doesn’t it? And that’s exactly what it is. The test for the significance of a correlation is identical to the <span class="math inline">\(t\)</span> test that we run on a coefficient in a regression model.</p>
</div>
<div id="corrhyp2" class="section level3" number="8.10.2">
<h3><span class="header-section-number">8.10.2</span> Hypothesis tests for all pairwise correlations</h3>
<p>Okay, one more digression before I return to regression properly. In the previous section I talked about the <code>cor.test()</code> function, which lets you run a hypothesis test on a single correlation. The <code>cor.test()</code> function is (obviously) an extension of the <code>cor()</code> function, which we talked about in Section <a href="regression.html#correl">8.4</a>. However, the <code>cor()</code> function isn’t restricted to computing a single correlation: you can use it to compute <em>all</em> pairwise correlations among the variables in your data set. This leads people to the natural question: can the <code>cor.test()</code> function do the same thing? Can we use <code>cor.test()</code> to run hypothesis tests for all possible parwise correlations among the variables in a data frame?</p>
<p>The answer is no, and there’s a very good reason for this. Testing a single correlation is fine: if you’ve got some reason to be asking “is A related to B?” then you should absolutely run a test to see if there’s a significant correlation. But if you’ve got variables A, B, C, D and E and you’re thinking about testing the correlations among all possible pairs of these, a statistician would want to ask: what’s your hypothesis? If you’re in the position of wanting to test all possible pairs of variables, then you’re pretty clearly on a fishing expedition, hunting around in search of significant effects when you don’t actually have a clear research hypothesis in mind. This is <em>dangerous</em>, and the authors of <code>cor.test()</code> obviously felt that they didn’t want to support that kind of behaviour.</p>
<p>On the other hand… a somewhat less hardline view might be to argue we encounter this situation when we talk about <em>post hoc tests</em> in ANOVA. When running post hoc tests, we didn’t have any specific comparisons in mind, so what we do is apply a correction (e.g., Bonferroni, Holm, etc) in order to avoid the possibility of an inflated Type I error rate. From this perspective, it’s okay to run hypothesis tests on all your pairwise correlations, but you must treat them as post hoc analyses, and if so you need to apply a correction for multiple comparisons. That’s what the <code>correlate()</code> function in the <code>lsr</code> package does. When we use the <code>correlate()</code> function in Section <a href="regression.html#correl">8.4</a> all it did was print out the correlation matrix. But you can get it to output the results of all the pairwise tests as well by specifying <code>test=TRUE</code>. Here’s what happens with the <code>parenthood</code> data:</p>
<div class="sourceCode" id="cb702"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb702-1"><a href="regression.html#cb702-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(lsr)</span>
<span id="cb702-2"><a href="regression.html#cb702-2" aria-hidden="true" tabindex="-1"></a><span class="fu">correlate</span>(parenthood, <span class="at">test=</span><span class="cn">TRUE</span>)</span></code></pre></div>
<pre><code>## 
## CORRELATIONS
## ============
## - correlation type:  pearson 
## - correlations shown only when both variables are numeric
## 
##            dan.sleep    baby.sleep    dan.grump       day   
## dan.sleep          .         0.628***    -0.903*** -0.098   
## baby.sleep     0.628***          .       -0.566*** -0.010   
## dan.grump     -0.903***     -0.566***         .     0.076   
## day           -0.098        -0.010        0.076         .   
## 
## ---
## Signif. codes: . = p &lt; .1, * = p&lt;.05, ** = p&lt;.01, *** = p&lt;.001
## 
## 
## p-VALUES
## ========
## - total number of tests run:  6 
## - correction for multiple testing:  holm 
## 
##            dan.sleep baby.sleep dan.grump   day
## dan.sleep          .      0.000     0.000 0.990
## baby.sleep     0.000          .     0.000 0.990
## dan.grump      0.000      0.000         . 0.990
## day            0.990      0.990     0.990     .
## 
## 
## SAMPLE SIZES
## ============
## 
##            dan.sleep baby.sleep dan.grump day
## dan.sleep        100        100       100 100
## baby.sleep       100        100       100 100
## dan.grump        100        100       100 100
## day              100        100       100 100</code></pre>
<p>The output here contains three matrices. First it prints out the correlation matrix. Second it prints out a matrix of <span class="math inline">\(p\)</span>-values, using the Holm method<a href="#fn122" class="footnote-ref" id="fnref122"><sup>122</sup></a> to correct for multiple comparisons. Finally, it prints out a matrix indicating the sample size (number of pairwise complete cases) that contributed to each correlation.</p>
<p>So there you have it. If you really desperately want to do pairwise hypothesis tests on your correlations, the <code>correlate()</code> function will let you do it. But please, <strong>please</strong> be careful. I can’t count the number of times I’ve had a student panicking in my office because they’ve run these pairwise correlation tests, and they get one or two significant results that don’t make any sense. For some reason, the moment people see those little significance stars appear, they feel compelled to throw away all common sense and assume that the results must correspond to something real that requires an explanation. In most such cases, my experience has been that the right answer is “it’s a Type I error.”</p>
</div>
</div>
<div id="regressioncoefs" class="section level2" number="8.11">
<h2><span class="header-section-number">8.11</span> Regarding regression coefficients</h2>
<p>Before moving on to discuss the assumptions underlying linear regression and what you can do to check if they’re being met, there’s two more topics I want to briefly discuss, both of which relate to the regression coefficients. The first thing to talk about is calculating confidence intervals for the coefficients; after that, I’ll discuss the somewhat murky question of how to determine which of predictor is most important.</p>
<div id="confidence-intervals-for-the-coefficients" class="section level3" number="8.11.1">
<h3><span class="header-section-number">8.11.1</span> Confidence intervals for the coefficients</h3>
<p>Like any population parameter, the regression coefficients <span class="math inline">\(b\)</span> cannot be estimated with complete precision from a sample of data; that’s part of why we need hypothesis tests. Given this, it’s quite useful to be able to report confidence intervals that capture our uncertainty about the true value of <span class="math inline">\(b\)</span>. This is especially useful when the research question focuses heavily on an attempt to find out <em>how</em> strongly variable <span class="math inline">\(X\)</span> is related to variable <span class="math inline">\(Y\)</span>, since in those situations the interest is primarily in the regression weight <span class="math inline">\(b\)</span>. Fortunately, confidence intervals for the regression weights can be constructed in the usual fashion,
<span class="math display">\[
\mbox{CI}(b) = \hat{b} \pm \left( t_{crit} \times \mbox{SE}({\hat{b})}  \right)
\]</span>
where <span class="math inline">\(\mbox{SE}({\hat{b}})\)</span> is the standard error of the regression coefficient, and <span class="math inline">\(t_{crit}\)</span> is the relevant critical value of the appropriate <span class="math inline">\(t\)</span> distribution. For instance, if it’s a 95% confidence interval that we want, then the critical value is the 97.5th quantile of a <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(N-K-1\)</span> degrees of freedom. In other words, this is basically the same approach to calculating confidence intervals that we’ve used throughout. To do this in R we can use the <code>confint()</code> function. There arguments to this function are</p>
<ul>
<li><code>object</code>. The regression model (<code>lm</code> object) for which confidence intervals are required.</li>
<li><code>parm</code>. A vector indicating which coefficients we should calculate intervals for. This can be either a vector of numbers or (more usefully) a character vector containing variable names. By default, all coefficients are included, so usually you don’t bother specifying this argument.</li>
<li><code>level</code>. A number indicating the confidence level that should be used. As is usually the case, the default value is 0.95, so you wouldn’t usually need to specify this argument.</li>
</ul>
<p>So, suppose I want 99% confidence intervals for the coefficients in the <code>regression.2</code> model. I could do this using the following command:</p>
<div class="sourceCode" id="cb704"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb704-1"><a href="regression.html#cb704-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>( <span class="at">object =</span> regression<span class="fl">.2</span>,</span>
<span id="cb704-2"><a href="regression.html#cb704-2" aria-hidden="true" tabindex="-1"></a>        <span class="at">level =</span> .<span class="dv">99</span>)</span></code></pre></div>
<pre><code>##                   0.5 %      99.5 %
## (Intercept) 117.9755724 133.9555593
## dan.sleep   -10.4044419  -7.4960575
## baby.sleep   -0.7016868   0.7227357</code></pre>
<p>Simple enough.</p>
</div>
<div id="calculating-standardised-regression-coefficients" class="section level3" number="8.11.2">
<h3><span class="header-section-number">8.11.2</span> Calculating standardised regression coefficients</h3>
<p>One more thing that you might want to do is to calculate “standardised” regression coefficients, often denoted <span class="math inline">\(\beta\)</span>. The rationale behind standardised coefficients goes like this. In a lot of situations, your variables are on fundamentally different scales. Suppose, for example, my regression model aims to predict people’s IQ scores, using their educational attainment (number of years of education) and their income as predictors. Obviously, educational attainment and income are not on the same scales: the number of years of schooling can only vary by 10s of years, whereas income would vary by 10,000s of dollars (or more). The units of measurement have a big influence on the regression coefficients: the <span class="math inline">\(b\)</span> coefficients only make sense when interpreted in light of the units, both of the predictor variables and the outcome variable. This makes it very difficult to compare the coefficients of different predictors. Yet there are situations where you really do want to make comparisons between different coefficients. Specifically, you might want some kind of standard measure of which predictors have the strongest relationship to the outcome. This is what <strong><em>standardised coefficients</em></strong> aim to do.</p>
<p>The basic idea is quite simple: the standardised coefficients are the coefficients that you would have obtained if you’d converted all the variables to <span class="math inline">\(z\)</span>-scores before running the regression.<a href="#fn123" class="footnote-ref" id="fnref123"><sup>123</sup></a> The idea here is that, by converting all the predictors to <span class="math inline">\(z\)</span>-scores, they all go into the regression on the same scale, thereby removing the problem of having variables on different scales. Regardless of what the original variables were, a <span class="math inline">\(\beta\)</span> value of 1 means that an increase in the predictor of 1 standard deviation will produce a corresponding 1 standard deviation increase in the outcome variable. Therefore, if variable A has a larger absolute value of <span class="math inline">\(\beta\)</span> than variable B, it is deemed to have a stronger relationship with the outcome. Or at least that’s the idea: it’s worth being a little cautious here, since this does rely very heavily on the assumption that “a 1 standard deviation change” is fundamentally the same kind of thing for all variables. It’s not always obvious that this is true.</p>
<p>Leaving aside the interpretation issues, let’s look at how it’s calculated. What you could do is standardise all the variables yourself and then run a regression, but there’s a much simpler way to do it. As it turns out, the <span class="math inline">\(\beta\)</span> coefficient for a predictor <span class="math inline">\(X\)</span> and outcome <span class="math inline">\(Y\)</span> has a very simple formula, namely
<span class="math display">\[
\beta_X = b_X \times \frac{\sigma_X}{\sigma_Y} 
\]</span>
where <span class="math inline">\(\sigma_X\)</span> is the standard deviation of the predictor, and <span class="math inline">\(\sigma_Y\)</span> is the standard deviation of the outcome variable <span class="math inline">\(Y\)</span>. This makes matters a lot simpler. To make things even simpler, the <code>lsr</code> package includes a function <code>standardCoefs()</code> that computes the <span class="math inline">\(\beta\)</span> coefficients.</p>
<div class="sourceCode" id="cb706"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb706-1"><a href="regression.html#cb706-1" aria-hidden="true" tabindex="-1"></a><span class="fu">standardCoefs</span>( regression<span class="fl">.2</span> )</span></code></pre></div>
<pre><code>##                      b        beta
## dan.sleep  -8.95024973 -0.90474809
## baby.sleep  0.01052447  0.00217223</code></pre>
<p>This clearly shows that the <code>dan.sleep</code> variable has a much stronger effect than the <code>baby.sleep</code> variable. However, this is a perfect example of a situation where it would probably make sense to use the original coefficients <span class="math inline">\(b\)</span> rather than the standardised coefficients <span class="math inline">\(\beta\)</span>. After all, my sleep and the baby’s sleep are <em>already</em> on the same scale: number of hours slept. Why complicate matters by converting these to <span class="math inline">\(z\)</span>-scores?</p>
</div>
</div>
<div id="regressionassumptions" class="section level2" number="8.12">
<h2><span class="header-section-number">8.12</span> Assumptions of regression</h2>
<p>The linear regression model that I’ve been discussing relies on several assumptions. In Section <a href="regression.html#regressiondiagnostics">8.13</a> we’ll talk a lot more about how to check that these assumptions are being met, but first, let’s have a look at each of them.</p>
<ul>
<li><em>Normality</em>. Like half the models in statistics, standard linear regression relies on an assumption of normality. Specifically, it assumes that the <em>residuals</em> are normally distributed. It’s actually okay if the predictors <span class="math inline">\(X\)</span> and the outcome <span class="math inline">\(Y\)</span> are non-normal, so long as the residuals <span class="math inline">\(\epsilon\)</span> are normal. See Section <a href="regression.html#regressionnormality">8.13.3</a>.</li>
<li><em>Linearity</em>. A pretty fundamental assumption of the linear regression model is that relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> actually be linear! Regardless of whether it’s a simple regression or a multiple regression, we assume that the relatiships involved are linear. See Section <a href="regression.html#regressionlinearity">8.13.4</a>.</li>
<li><em>Homogeneity of variance</em>. Strictly speaking, the regression model assumes that each residual <span class="math inline">\(\epsilon_i\)</span> is generated from a normal distribution with mean 0, and (more importantly for the current purposes) with a standard deviation <span class="math inline">\(\sigma\)</span> that is the same for every single residual. In practice, it’s impossible to test the assumption that every residual is identically distributed. Instead, what we care about is that the standard deviation of the residual is the same for all values of <span class="math inline">\(\hat{Y}\)</span>, and (if we’re being especially paranoid) all values of every predictor <span class="math inline">\(X\)</span> in the model. See Section <a href="regression.html#regressionhomogeneity">8.13.5</a>.</li>
<li><em>Uncorrelated predictors</em>. The idea here is that, is a multiple regression model, you don’t want your predictors to be too strongly correlated with each other. This isn’t “technically” an assumption of the regression model, but in practice it’s required. Predictors that are too strongly correlated with each other (referred to as “collinearity”) can cause problems when evaluating the model. See Section <a href="regression.html#regressioncollinearity">8.13.6</a></li>
<li><em>Residuals are independent of each other</em>. This is really just a “catch all” assumption, to the effect that “there’s nothing else funny going on in the residuals.” If there is something weird (e.g., the residuals all depend heavily on some other unmeasured variable) going on, it might screw things up.</li>
<li><em>No “bad” outliers</em>. Again, not actually a technical assumption of the model (or rather, it’s sort of implied by all the others), but there is an implicit assumption that your regression model isn’t being too strongly influenced by one or two anomalous data points; since this raises questions about the adequacy of the model, and the trustworthiness of the data in some cases. See Section <a href="regression.html#regressionoutliers">8.13.2</a>.</li>
</ul>
</div>
<div id="regressiondiagnostics" class="section level2" number="8.13">
<h2><span class="header-section-number">8.13</span> Model checking</h2>
<p>The main focus of this section is <strong><em>regression diagnostics</em></strong>, a term that refers to the art of checking that the assumptions of your regression model have been met, figuring out how to fix the model if the assumptions are violated, and generally to check that nothing “funny” is going on. I refer to this as the “art” of model checking with good reason: it’s not easy, and while there are a lot of fairly standardised tools that you can use to diagnose and maybe even cure the problems that ail your model (if there are any, that is!), you really do need to exercise a certain amount of judgment when doing this. It’s easy to get lost in all the details of checking this thing or that thing, and it’s quite exhausting to try to remember what all the different things are. This has the very nasty side effect that a lot of people get frustrated when trying to learn <em>all</em> the tools, so instead they decide not to do <em>any</em> model checking. This is a bit of a worry!</p>
<p>In this section, I describe several different things you can do to check that your regression model is doing what it’s supposed to. It doesn’t cover the full space of things you could do, but it’s still much more detailed than what I see a lot of people doing in practice; and I don’t usually cover all of this in my intro stats class myself. However, I do think it’s important that you get a sense of what tools are at your disposal, so I’ll try to introduce a bunch of them here. Finally, I should note that this section draws quite heavily from the <span class="citation"><a href="#ref-Fox2011" role="doc-biblioref">Fox and Weisberg</a> (<a href="#ref-Fox2011" role="doc-biblioref">2011</a>)</span> text, the book associated with the <code>car</code> package. The <code>car</code> package is notable for providing some excellent tools for regression diagnostics, and the book itself talks about them in an admirably clear fashion. I don’t want to sound too gushy about it, but I do think that <span class="citation"><a href="#ref-Fox2011" role="doc-biblioref">Fox and Weisberg</a> (<a href="#ref-Fox2011" role="doc-biblioref">2011</a>)</span> is well worth reading.</p>
<div id="three-kinds-of-residuals" class="section level3" number="8.13.1">
<h3><span class="header-section-number">8.13.1</span> Three kinds of residuals</h3>
<p>The majority of regression diagnostics revolve around looking at the residuals, and by now you’ve probably formed a sufficiently pessimistic theory of statistics to be able to guess that – precisely <em>because</em> of the fact that we care a lot about the residuals – there are several different kinds of residual that we might consider. In particular, the following three kinds of residual are referred to in this section: “ordinary residuals,” “standardised residuals,” and “Studentised residuals.” There is a fourth kind that you’ll see referred to in some of the Figures, and that’s the “Pearson residual”: however, for the models that we’re talking about in this chapter, the Pearson residual is identical to the ordinary residual.</p>
<p>The first and simplest kind of residuals that we care about are <strong><em>ordinary residuals</em></strong>. These are the actual, raw residuals that I’ve been talking about throughout this chapter. The ordinary residual is just the difference between the fitted value <span class="math inline">\(\hat{Y}_i\)</span> and the observed value <span class="math inline">\(Y_i\)</span>. I’ve been using the notation <span class="math inline">\(\epsilon_i\)</span> to refer to the <span class="math inline">\(i\)</span>-th ordinary residual, and by gum I’m going to stick to it. With this in mind, we have the very simple equation
<span class="math display">\[
\epsilon_i = Y_i - \hat{Y}_i
\]</span>
This is of course what we saw earlier, and unless I specifically refer to some other kind of residual, this is the one I’m talking about. So there’s nothing new here: I just wanted to repeat myself. In any case, you can get R to output a vector of ordinary residuals, you can use a command like this:</p>
<div class="sourceCode" id="cb708"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb708-1"><a href="regression.html#cb708-1" aria-hidden="true" tabindex="-1"></a><span class="fu">residuals</span>( <span class="at">object =</span> regression<span class="fl">.2</span> )</span></code></pre></div>
<pre><code>##           1           2           3           4           5           6 
##  -2.1403095   4.7081942   1.9553640  -2.0602806   0.7194888  -0.4066133 
##           7           8           9          10          11          12 
##   0.2269987  -1.7003077   0.2025039   3.8524589   3.9986291  -4.9120150 
##          13          14          15          16          17          18 
##   1.2060134   0.4946578  -2.6579276  -0.3966805   3.3538613   1.7261225 
##          19          20          21          22          23          24 
##  -0.4922551  -5.6405941  -0.4660764   2.7238389   9.3653697   0.2841513 
##          25          26          27          28          29          30 
##  -0.5037668  -1.4941146   8.1328623   1.9787316  -1.5126726   3.5171148 
##          31          32          33          34          35          36 
##  -8.9256951  -2.8282946   6.1030349  -7.5460717   4.5572128 -10.6510836 
##          37          38          39          40          41          42 
##  -5.6931846   6.3096506  -2.1082466  -0.5044253   0.1875576   4.8094841 
##          43          44          45          46          47          48 
##  -5.4135163  -6.2292842  -4.5725232  -5.3354601   3.9950111   2.1718745 
##          49          50          51          52          53          54 
##  -3.4766440   0.4834367   6.2839790   2.0109396  -1.5846631  -2.2166613 
##          55          56          57          58          59          60 
##   2.2033140   1.9328736  -1.8301204  -1.5401430   2.5298509  -3.3705782 
##          61          62          63          64          65          66 
##  -2.9380806   0.6590736  -0.5917559  -8.6131971   5.9781035   5.9332979 
##          67          68          69          70          71          72 
##  -1.2341956   3.0047669  -1.0802468   6.5174672  -3.0155469   2.1176720 
##          73          74          75          76          77          78 
##   0.6058757  -2.7237421  -2.2291472  -1.4053822   4.7461491  11.7495569 
##          79          80          81          82          83          84 
##   4.7634141   2.6620908 -11.0345292  -0.7588667   1.4558227  -0.4745727 
##          85          86          87          88          89          90 
##   8.9091201  -1.1409777   0.7555223  -0.4107130   0.8797237  -1.4095586 
##          91          92          93          94          95          96 
##   3.1571385  -3.4205757  -5.7228699  -2.2033958  -3.8647891   0.4982711 
##          97          98          99         100 
##  -5.5249495   4.1134221  -8.2038533   5.6800859</code></pre>
<p>One drawback to using ordinary residuals is that they’re always on a different scale, depending on what the outcome variable is and how good the regression model is. That is, Unless you’ve decided to run a regression model without an intercept term, the ordinary residuals will have mean 0; but the variance is different for every regression. In a lot of contexts, especially where you’re only interested in the <em>pattern</em> of the residuals and not their actual values, it’s convenient to estimate the <strong><em>standardised residuals</em></strong>, which are normalised in such a way as to have standard deviation 1. The way we calculate these is to divide the ordinary residual by an estimate of the (population) standard deviation of these residuals. For technical reasons, mumble mumble, the formula for this is:
<span class="math display">\[
\epsilon_{i}^\prime = \frac{\epsilon_i}{\hat{\sigma} \sqrt{1-h_i}}
\]</span>
where <span class="math inline">\(\hat\sigma\)</span> in this context is the estimated population standard deviation of the ordinary residuals, and <span class="math inline">\(h_i\)</span> is the “hat value” of the <span class="math inline">\(i\)</span>th observation. I haven’t explained hat values to you yet (but have no fear,<a href="#fn124" class="footnote-ref" id="fnref124"><sup>124</sup></a> it’s coming shortly), so this won’t make a lot of sense. For now, it’s enough to interpret the standardised residuals as if we’d converted the ordinary residuals to <span class="math inline">\(z\)</span>-scores. In fact, that is more or less the truth, it’s just that we’re being a bit fancier. To get the standardised residuals, the command you want is this:</p>
<div class="sourceCode" id="cb710"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb710-1"><a href="regression.html#cb710-1" aria-hidden="true" tabindex="-1"></a><span class="fu">rstandard</span>( <span class="at">model =</span> regression<span class="fl">.2</span> )</span></code></pre></div>
<pre><code>##           1           2           3           4           5           6 
## -0.49675845  1.10430571  0.46361264 -0.47725357  0.16756281 -0.09488969 
##           7           8           9          10          11          12 
##  0.05286626 -0.39260381  0.04739691  0.89033990  0.95851248 -1.13898701 
##          13          14          15          16          17          18 
##  0.28047841  0.11519184 -0.61657092 -0.09191865  0.77692937  0.40403495 
##          19          20          21          22          23          24 
## -0.11552373 -1.31540412 -0.10819238  0.62951824  2.17129803  0.06586227 
##          25          26          27          28          29          30 
## -0.11980449 -0.34704024  1.91121833  0.45686516 -0.34986350  0.81233165 
##          31          32          33          34          35          36 
## -2.08659993 -0.66317843  1.42930082 -1.77763064  1.07452436 -2.47385780 
##          37          38          39          40          41          42 
## -1.32715114  1.49419658 -0.49115639 -0.11674947  0.04401233  1.11881912 
##          43          44          45          46          47          48 
## -1.27081641 -1.46422595 -1.06943700 -1.24659673  0.94152881  0.51069809 
##          49          50          51          52          53          54 
## -0.81373349  0.11412178  1.47938594  0.46437962 -0.37157009 -0.51609949 
##          55          56          57          58          59          60 
##  0.51800753  0.44813204 -0.42662358 -0.35575611  0.58403297 -0.78022677 
##          61          62          63          64          65          66 
## -0.67833325  0.15484699 -0.13760574 -2.05662232  1.40238029  1.37505125 
##          67          68          69          70          71          72 
## -0.28964989  0.69497632 -0.24945316  1.50709623 -0.69864682  0.49071427 
##          73          74          75          76          77          78 
##  0.14267297 -0.63246560 -0.51972828 -0.32509811  1.10842574  2.72171671 
##          79          80          81          82          83          84 
##  1.09975101  0.62057080 -2.55172097 -0.17584803  0.34340064 -0.11158952 
##          85          86          87          88          89          90 
##  2.10863391 -0.26386516  0.17624445 -0.09504416  0.20450884 -0.32730740 
##          91          92          93          94          95          96 
##  0.73475640 -0.79400855 -1.32768248 -0.51940736 -0.91512580  0.11661226 
##          97          98          99         100 
## -1.28069115  0.96332849 -1.90290258  1.31368144</code></pre>
<p>Note that this function uses a different name for the input argument, but it’s still just a linear regression object that the function wants to take as its input here.</p>
<p>The third kind of residuals are <strong><em>Studentised residuals</em></strong> (also called “jackknifed residuals”) and they’re even fancier than standardised residuals. Again, the idea is to take the ordinary residual and divide it by some quantity in order to estimate some standardised notion of the residual, but the formula for doing the calculations this time is subtly different:
<span class="math display">\[
\epsilon_{i}^* = \frac{\epsilon_i}{\hat{\sigma}_{(-i)} \sqrt{1-h_i}}
\]</span>
Notice that our estimate of the standard deviation here is written <span class="math inline">\(\hat{\sigma}_{(-i)}\)</span>. What this corresponds to is the estimate of the residual standard deviation that you <em>would have obtained</em>, if you just deleted the <span class="math inline">\(i\)</span>th observation from the data set. This sounds like the sort of thing that would be a nightmare to calculate, since it seems to be saying that you have to run <span class="math inline">\(N\)</span> new regression models (even a modern computer might grumble a bit at that, especially if you’ve got a large data set). Fortunately, some terribly clever person has shown that this standard deviation estimate is actually given by the following equation:
<span class="math display">\[
\hat\sigma_{(-i)} = \hat{\sigma} \ \sqrt{\frac{N-K-1 - {\epsilon_{i}^\prime}^2}{N-K-2}}
\]</span>
Isn’t that a pip? Anyway, the command that you would use if you wanted to pull out the Studentised residuals for our regression model is</p>
<div class="sourceCode" id="cb712"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb712-1"><a href="regression.html#cb712-1" aria-hidden="true" tabindex="-1"></a><span class="fu">rstudent</span>( <span class="at">model =</span> regression<span class="fl">.2</span> )</span></code></pre></div>
<pre><code>##           1           2           3           4           5           6 
## -0.49482102  1.10557030  0.46172854 -0.47534555  0.16672097 -0.09440368 
##           7           8           9          10          11          12 
##  0.05259381 -0.39088553  0.04715251  0.88938019  0.95810710 -1.14075472 
##          13          14          15          16          17          18 
##  0.27914212  0.11460437 -0.61459001 -0.09144760  0.77533036  0.40228555 
##          19          20          21          22          23          24 
## -0.11493461 -1.32043609 -0.10763974  0.62754813  2.21456485  0.06552336 
##          25          26          27          28          29          30 
## -0.11919416 -0.34546127  1.93818473  0.45499388 -0.34827522  0.81089646 
##          31          32          33          34          35          36 
## -2.12403286 -0.66125192  1.43712830 -1.79797263  1.07539064 -2.54258876 
##          37          38          39          40          41          42 
## -1.33244515  1.50388257 -0.48922682 -0.11615428  0.04378531  1.12028904 
##          43          44          45          46          47          48 
## -1.27490649 -1.47302872 -1.07023828 -1.25020935  0.94097261  0.50874322 
##          49          50          51          52          53          54 
## -0.81230544  0.11353962  1.48863006  0.46249410 -0.36991317 -0.51413868 
##          55          56          57          58          59          60 
##  0.51604474  0.44627831 -0.42481754 -0.35414868  0.58203894 -0.77864171 
##          61          62          63          64          65          66 
## -0.67643392  0.15406579 -0.13690795 -2.09211556  1.40949469  1.38147541 
##          67          68          69          70          71          72 
## -0.28827768  0.69311245 -0.24824363  1.51717578 -0.69679156  0.48878534 
##          73          74          75          76          77          78 
##  0.14195054 -0.63049841 -0.51776374 -0.32359434  1.10974786  2.81736616 
##          79          80          81          82          83          84 
##  1.10095270  0.61859288 -2.62827967 -0.17496714  0.34183379 -0.11101996 
##          85          86          87          88          89          90 
##  2.14753375 -0.26259576  0.17536170 -0.09455738  0.20349582 -0.32579584 
##          91          92          93          94          95          96 
##  0.73300184 -0.79248469 -1.33298848 -0.51744314 -0.91435205  0.11601774 
##          97          98          99         100 
## -1.28498273  0.96296745 -1.92942389  1.31867548</code></pre>
<p>Before moving on, I should point out that you don’t often need to manually extract these residuals yourself, even though they are at the heart of almost all regression diagnostics. That is, the <code>residuals()</code>, <code>rstandard()</code> and <code>rstudent()</code> functions are all useful to <em>know</em> about, but most of the time the various functions that run the diagnostics will take care of these calculations for you. Even so, it’s always nice to know how to actually get hold of these things yourself in case you ever need to do something non-standard.</p>
</div>
<div id="regressionoutliers" class="section level3" number="8.13.2">
<h3><span class="header-section-number">8.13.2</span> Three kinds of anomalous data</h3>
<p>One danger that you can run into with linear regression models is that your analysis might be disproportionately sensitive to a smallish number of “unusual” or “anomalous” observations. I discussed this idea previously in Section <a href="descriptives.html#boxplotoutliers">3.9.3.2</a> in the context of discussing the outliers that get automatically identified by the <code>boxplot()</code> function, but this time we need to be much more precise. In the context of linear regression, there are three conceptually distinct ways in which an observation might be called “anomalous.” All three are interesting, but they have rather different implications for your analysis.</p>
The first kind of unusual observation is an <strong><em>outlier</em></strong>. The definition of an outlier (in this context) is an observation that is very different from what the regression model predicts. An example is shown in Figure <a href="regression.html#fig:outlier">8.14</a>. In practice, we operationalise this concept by saying that an outlier is an observation that has a very large Studentised residual, <span class="math inline">\(\epsilon_i^*\)</span>. Outliers are interesting: a big outlier <em>might</em> correspond to junk data – e.g., the variables might have been entered incorrectly, or some other defect may be detectable. Note that you shouldn’t throw an observation away just because it’s an outlier. But the fact that it’s an outlier is often a cue to look more closely at that case, and try to find out why it’s so different.
<div class="figure"><span style="display:block;" id="fig:outlier"></span>
<img src="img/regression2/unusual_outlier-eps-converted-to.pdf" alt="An illustration of outliers. The dotted lines plot the regression line that would have been estimated without the anomalous observation included, and the corresponding residual (i.e., the Studentised residual). The solid line shows the regression line with the anomalous observation included. The outlier has an unusual value on the outcome (y axis location) but not the predictor (x axis location), and lies a long way from the regression line."  />
<p class="caption">
Figure 8.14: An illustration of outliers. The dotted lines plot the regression line that would have been estimated without the anomalous observation included, and the corresponding residual (i.e., the Studentised residual). The solid line shows the regression line with the anomalous observation included. The outlier has an unusual value on the outcome (y axis location) but not the predictor (x axis location), and lies a long way from the regression line.
</p>
</div>
<p>The second way in which an observation can be unusual is if it has high <strong><em>leverage</em></strong>: this happens when the observation is very different from all the other observations. This doesn’t necessarily have to correspond to a large residual: if the observation happens to be unusual on all variables in precisely the same way, it can actually lie very close to the regression line. An example of this is shown in Figure <a href="regression.html#fig:leverage">8.15</a>. The leverage of an observation is operationalised in terms of its <em>hat value</em>, usually written <span class="math inline">\(h_i\)</span>. The formula for the hat value is rather complicated<a href="#fn125" class="footnote-ref" id="fnref125"><sup>125</sup></a> but its interpretation is not: <span class="math inline">\(h_i\)</span> is a measure of the extent to which the <span class="math inline">\(i\)</span>-th observation is “in control” of where the regression line ends up going. You can extract the hat values using the following command:</p>
<div class="sourceCode" id="cb714"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb714-1"><a href="regression.html#cb714-1" aria-hidden="true" tabindex="-1"></a><span class="fu">hatvalues</span>( <span class="at">model =</span> regression<span class="fl">.2</span> )</span></code></pre></div>
<pre><code>##          1          2          3          4          5          6 
## 0.02067452 0.04105320 0.06155445 0.01685226 0.02734865 0.03129943 
##          7          8          9         10         11         12 
## 0.02735579 0.01051224 0.03698976 0.01229155 0.08189763 0.01882551 
##         13         14         15         16         17         18 
## 0.02462902 0.02718388 0.01964210 0.01748592 0.01691392 0.03712530 
##         19         20         21         22         23         24 
## 0.04213891 0.02994643 0.02099435 0.01233280 0.01853370 0.01804801 
##         25         26         27         28         29         30 
## 0.06722392 0.02214927 0.04472007 0.01039447 0.01381812 0.01105817 
##         31         32         33         34         35         36 
## 0.03468260 0.04048248 0.03814670 0.04934440 0.05107803 0.02208177 
##         37         38         39         40         41         42 
## 0.02919013 0.05928178 0.02799695 0.01519967 0.04195751 0.02514137 
##         43         44         45         46         47         48 
## 0.04267879 0.04517340 0.03558080 0.03360160 0.05019778 0.04587468 
##         49         50         51         52         53         54 
## 0.03701290 0.05331282 0.04814477 0.01072699 0.04047386 0.02681315 
##         55         56         57         58         59         60 
## 0.04556787 0.01856997 0.02919045 0.01126069 0.01012683 0.01546412 
##         61         62         63         64         65         66 
## 0.01029534 0.04428870 0.02438944 0.07469673 0.04135090 0.01775697 
##         67         68         69         70         71         72 
## 0.04217616 0.01384321 0.01069005 0.01340216 0.01716361 0.01751844 
##         73         74         75         76         77         78 
## 0.04863314 0.02158623 0.02951418 0.01411915 0.03276064 0.01684599 
##         79         80         81         82         83         84 
## 0.01028001 0.02920514 0.01348051 0.01752758 0.05184527 0.04583604 
##         85         86         87         88         89         90 
## 0.05825858 0.01359644 0.03054414 0.01487724 0.02381348 0.02159418 
##         91         92         93         94         95         96 
## 0.02598661 0.02093288 0.01982480 0.05063492 0.05907629 0.03682026 
##         97         98         99        100 
## 0.01817919 0.03811718 0.01945603 0.01373394</code></pre>
<div class="figure"><span style="display:block;" id="fig:leverage"></span>
<img src="img/regression2/unusual_leverage-eps-converted-to.pdf" alt="An illustration of high leverage points. The anomalous observation in this case is unusual both in terms of the predictor (x axis) and the outcome (y axis), but this unusualness is highly consistent with the pattern of correlations that exists among the other observations; as a consequence, the observation falls very close to the regression line and does not distort it."  />
<p class="caption">
Figure 8.15: An illustration of high leverage points. The anomalous observation in this case is unusual both in terms of the predictor (x axis) and the outcome (y axis), but this unusualness is highly consistent with the pattern of correlations that exists among the other observations; as a consequence, the observation falls very close to the regression line and does not distort it.
</p>
</div>
<p>In general, if an observation lies far away from the other ones in terms of the predictor variables, it will have a large hat value (as a rough guide, high leverage is when the hat value is more than 2-3 times the average; and note that the sum of the hat values is constrained to be equal to <span class="math inline">\(K+1\)</span>). High leverage points are also worth looking at in more detail, but they’re much less likely to be a cause for concern unless they are also outliers. % guide from Venables and Ripley.</p>
<p>This brings us to our third measure of unusualness, the <strong><em>influence</em></strong> of an observation. A high influence observation is an outlier that has high leverage. That is, it is an observation that is very different to all the other ones in some respect, and also lies a long way from the regression line. This is illustrated in Figure <a href="regression.html#fig:influence">8.16</a>. Notice the contrast to the previous two figures: outliers don’t move the regression line much, and neither do high leverage points. But something that is an outlier and has high leverage… that has a big effect on the regression line.</p>
<div class="figure"><span style="display:block;" id="fig:influence"></span>
<img src="img/regression2/unusual_influence-eps-converted-to.pdf" alt="An illustration of high influence points. In this case, the anomalous observation is highly unusual on the predictor variable (x axis), and falls a long way from the regression line. As a consequence, the regression line is highly distorted, even though (in this case) the anomalous observation is entirely typical in terms of the outcome variable (y axis)."  />
<p class="caption">
Figure 8.16: An illustration of high influence points. In this case, the anomalous observation is highly unusual on the predictor variable (x axis), and falls a long way from the regression line. As a consequence, the regression line is highly distorted, even though (in this case) the anomalous observation is entirely typical in terms of the outcome variable (y axis).
</p>
</div>
<p>That’s why we call these points high influence; and it’s why they’re the biggest worry. We operationalise influence in terms of a measure known as <strong><em>Cook’s distance</em></strong>,
<span class="math display">\[
D_i = \frac{{\epsilon_i^*}^2 }{K+1} \times \frac{h_i}{1-h_i}
\]</span>
Notice that this is a multiplication of something that measures the outlier-ness of the observation (the bit on the left), and something that measures the leverage of the observation (the bit on the right). In other words, in order to have a large Cook’s distance, an observation must be a fairly substantial outlier <em>and</em> have high leverage. In a stunning turn of events, you can obtain these values using the following command:</p>
<div class="sourceCode" id="cb716"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb716-1"><a href="regression.html#cb716-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cooks.distance</span>( <span class="at">model =</span> regression<span class="fl">.2</span> )</span></code></pre></div>
<pre><code>##            1            2            3            4            5 
## 1.736512e-03 1.740243e-02 4.699370e-03 1.301417e-03 2.631557e-04 
##            6            7            8            9           10 
## 9.697585e-05 2.620181e-05 5.458491e-04 2.876269e-05 3.288277e-03 
##           11           12           13           14           15 
## 2.731835e-02 8.296919e-03 6.621479e-04 1.235956e-04 2.538915e-03 
##           16           17           18           19           20 
## 5.012283e-05 3.461742e-03 2.098055e-03 1.957050e-04 1.780519e-02 
##           21           22           23           24           25 
## 8.367377e-05 1.649478e-03 2.967594e-02 2.657610e-05 3.448032e-04 
##           26           27           28           29           30 
## 9.093379e-04 5.699951e-02 7.307943e-04 5.716998e-04 2.459564e-03 
##           31           32           33           34           35 
## 5.214331e-02 6.185200e-03 2.700686e-02 5.467345e-02 2.071643e-02 
##           36           37           38           39           40 
## 4.606378e-02 1.765312e-02 4.689817e-02 2.316122e-03 7.012530e-05 
##           41           42           43           44           45 
## 2.827824e-05 1.076083e-02 2.399931e-02 3.381062e-02 1.406498e-02 
##           46           47           48           49           50 
## 1.801086e-02 1.561699e-02 4.179986e-03 8.483514e-03 2.444787e-04 
##           51           52           53           54           55 
## 3.689946e-02 7.794472e-04 1.941235e-03 2.446230e-03 4.270361e-03 
##           56           57           58           59           60 
## 1.266609e-03 1.824212e-03 4.804705e-04 1.163181e-03 3.187235e-03 
##           61           62           63           64           65 
## 1.595512e-03 3.703826e-04 1.577892e-04 1.138165e-01 2.827715e-02 
##           66           67           68           69           70 
## 1.139374e-02 1.231422e-03 2.260006e-03 2.241322e-04 1.028479e-02 
##           71           72           73           74           75 
## 2.841329e-03 1.431223e-03 3.468538e-04 2.941757e-03 2.738249e-03 
##           76           77           78           79           80 
## 5.045357e-04 1.387108e-02 4.230966e-02 4.187440e-03 3.861831e-03 
##           81           82           83           84           85 
## 2.965826e-02 1.838888e-04 2.149369e-03 1.993929e-04 9.168733e-02 
##           86           87           88           89           90 
## 3.198994e-04 3.262192e-04 4.547383e-05 3.400893e-04 7.881487e-04 
##           91           92           93           94           95 
## 4.801204e-03 4.493095e-03 1.188427e-02 4.796360e-03 1.752666e-02 
##           96           97           98           99          100 
## 1.732793e-04 1.012302e-02 1.225818e-02 2.394964e-02 8.010508e-03</code></pre>
<p>As a rough guide, Cook’s distance greater than 1 is often considered large (that’s what I typically use as a quick and dirty rule), though a quick scan of the internet and a few papers suggests that <span class="math inline">\(4/N\)</span> has also been suggested as a possible rule of thumb.</p>
<p>As hinted above, you don’t usually need to make use of these functions, since you can have R automatically draw the critical plots.<a href="#fn126" class="footnote-ref" id="fnref126"><sup>126</sup></a> For the <code>regression.2</code> model, these are the plots showing Cook’s distance (Figure <a href="regression.html#fig:regressionplot4">8.17</a>) and the more detailed breakdown showing the scatter plot of the Studentised residual against leverage (Figure <a href="regression.html#fig:regressionplot5">8.18</a>). To draw these, we can use the <code>plot()</code> function. When the main argument <code>x</code> to this function is a linear model object, it will draw one of six different plots, each of which is quite useful for doing regression diagnostics. You specify which one you want using the <code>which</code> argument (a number between 1 and 6). If you don’t do this then R will draw all six. The two plots of interest to us in this context are generated using the following commands:</p>
<div class="sourceCode" id="cb718"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb718-1"><a href="regression.html#cb718-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> regression<span class="fl">.2</span>, <span class="at">which =</span> <span class="dv">4</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:regressionplot4"></span>
<img src="schuster-statistics-remix_files/figure-html/regressionplot4-1.png" alt="Cook's distance for every observation. This is one of the standard regression plots produced by the `plot()` function when the input is a linear regression object. It is obtained by setting `which=4`" width="672" />
<p class="caption">
Figure 8.17: Cook’s distance for every observation. This is one of the standard regression plots produced by the <code>plot()</code> function when the input is a linear regression object. It is obtained by setting <code>which=4</code>
</p>
</div>
<div class="sourceCode" id="cb719"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb719-1"><a href="regression.html#cb719-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> regression<span class="fl">.2</span>, <span class="at">which =</span> <span class="dv">5</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:regressionplot5"></span>
<img src="schuster-statistics-remix_files/figure-html/regressionplot5-1.png" alt="Residuals versus leverage. This is one of the standard regression plots produced by the `plot()` function when the input is a linear regression object. It is obtained by setting `which=5`." width="672" />
<p class="caption">
Figure 8.18: Residuals versus leverage. This is one of the standard regression plots produced by the <code>plot()</code> function when the input is a linear regression object. It is obtained by setting <code>which=5</code>.
</p>
</div>
<p>An obvious question to ask next is, if you do have large values of Cook’s distance, what should you do? As always, there’s no hard and fast rules. Probably the first thing to do is to try running the regression with that point excluded and see what happens to the model performance and to the regression coefficients. If they really are substantially different, it’s time to start digging into your data set and your notes that you no doubt were scribbling as your ran your study; try to figure out <em>why</em> the point is so different. If you start to become convinced that this one data point is badly distorting your results, you might consider excluding it, but that’s less than ideal unless you have a solid explanation for why this particular case is qualitatively different from the others and therefore deserves to be handled separately.<a href="#fn127" class="footnote-ref" id="fnref127"><sup>127</sup></a> To give an example, let’s delete the observation from day 64, the observation with the largest Cook’s distance for the <code>regression.2</code> model. We can do this using the <code>subset</code> argument:</p>
<div class="sourceCode" id="cb720"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb720-1"><a href="regression.html#cb720-1" aria-hidden="true" tabindex="-1"></a><span class="fu">lm</span>( <span class="at">formula =</span> dan.grump <span class="sc">~</span> dan.sleep <span class="sc">+</span> baby.sleep,  <span class="co"># same formula</span></span>
<span id="cb720-2"><a href="regression.html#cb720-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">data =</span> parenthood,       <span class="co"># same data frame...</span></span>
<span id="cb720-3"><a href="regression.html#cb720-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">subset =</span> <span class="sc">-</span><span class="dv">64</span>             <span class="co"># ...but observation 64 is deleted</span></span>
<span id="cb720-4"><a href="regression.html#cb720-4" aria-hidden="true" tabindex="-1"></a> )</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = dan.grump ~ dan.sleep + baby.sleep, data = parenthood, 
##     subset = -64)
## 
## Coefficients:
## (Intercept)    dan.sleep   baby.sleep  
##    126.3553      -8.8283      -0.1319</code></pre>
<p>As you can see, those regression coefficients have barely changed in comparison to the values we got earlier. In other words, we really don’t have any problem as far as anomalous data are concerned.</p>
</div>
<div id="regressionnormality" class="section level3" number="8.13.3">
<h3><span class="header-section-number">8.13.3</span> Checking the normality of the residuals</h3>
<p>Like many of the statistical tools we’ve discussed in this book, regression models rely on a normality assumption. In this case, we assume that the residuals are normally distributed. Firstly, I firmly believe that it never hurts to draw an old fashioned histogram. The command I use might be something like this:</p>
<div class="sourceCode" id="cb722"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb722-1"><a href="regression.html#cb722-1" aria-hidden="true" tabindex="-1"></a> <span class="fu">hist</span>( <span class="at">x =</span> <span class="fu">residuals</span>( regression<span class="fl">.2</span> ),   <span class="co"># data are the residuals</span></span>
<span id="cb722-2"><a href="regression.html#cb722-2" aria-hidden="true" tabindex="-1"></a>       <span class="at">xlab =</span> <span class="st">&quot;Value of residual&quot;</span>,      <span class="co"># x-axis label</span></span>
<span id="cb722-3"><a href="regression.html#cb722-3" aria-hidden="true" tabindex="-1"></a>       <span class="at">main =</span> <span class="st">&quot;&quot;</span>,                       <span class="co"># no title </span></span>
<span id="cb722-4"><a href="regression.html#cb722-4" aria-hidden="true" tabindex="-1"></a>       <span class="at">breaks =</span> <span class="dv">20</span>                      <span class="co"># lots of breaks</span></span>
<span id="cb722-5"><a href="regression.html#cb722-5" aria-hidden="true" tabindex="-1"></a> )</span></code></pre></div>
<p>The resulting plot is shown in Figure <a href="regression.html#fig:residhist">8.19</a>, and as you can see the plot looks pretty damn close to normal, almost unnaturally so.</p>
<div class="figure"><span style="display:block;" id="fig:residhist"></span>
<img src="img/regression2/residhist-eps-converted-to.pdf" alt="A histogram of the (ordinary) residuals in the `regression.2` model. These residuals look very close to being normally distributed, much moreso than is typically seen with real data. This shouldn't surprise you... they aren't real data, and they aren't real residuals!"  />
<p class="caption">
Figure 8.19: A histogram of the (ordinary) residuals in the <code>regression.2</code> model. These residuals look very close to being normally distributed, much moreso than is typically seen with real data. This shouldn’t surprise you… they aren’t real data, and they aren’t real residuals!
</p>
</div>
<p>I could also run a Shapiro-Wilk test to check, using the <code>shapiro.test()</code> function; the <span class="math inline">\(W\)</span> value of .99, at this sample size, is non-significant (<span class="math inline">\(p=.84\)</span>), again suggesting that the normality assumption isn’t in any danger here. As a third measure, we might also want to draw a QQ-plot using the <code>qqnorm()</code> function. The QQ plot is an excellent one to draw, and so you might not be surprised to discover that it’s one of the regression plots that we can produce using the <code>plot()</code> function:</p>
<div class="sourceCode" id="cb723"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb723-1"><a href="regression.html#cb723-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>( <span class="at">x =</span> regression<span class="fl">.2</span>, <span class="at">which =</span> <span class="dv">2</span> )   <span class="co"># Figure @ref{fig:regressionplot2}</span></span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:regressionplot2"></span>
<img src="schuster-statistics-remix_files/figure-html/regressionplot2-1.png" alt="Plot of the theoretical quantiles according to the model, against the quantiles of the standardised residuals. This is one of the standard regression plots produced by the `plot()` function when the input is a linear regression object. It is obtained by setting `which=2`." width="672" />
<p class="caption">
Figure 8.20: Plot of the theoretical quantiles according to the model, against the quantiles of the standardised residuals. This is one of the standard regression plots produced by the <code>plot()</code> function when the input is a linear regression object. It is obtained by setting <code>which=2</code>.
</p>
</div>
<p>The output is shown in Figure <a href="regression.html#fig:regressionplot2">8.20</a>, showing the standardised residuals plotted as a function of their theoretical quantiles according to the regression model. The fact that the output appends the model specification to the picture is nice.</p>
</div>
<div id="regressionlinearity" class="section level3" number="8.13.4">
<h3><span class="header-section-number">8.13.4</span> Checking the linearity of the relationship</h3>
<div class="figure"><span style="display:block;" id="fig:regressionlinearity"></span>
<img src="schuster-statistics-remix_files/figure-html/regressionlinearity-1.png" alt="Plot of the fitted values against the observed values of the outcome variable. A straight line is what we're hoping to see here. This looks pretty good, suggesting that there's nothing grossly wrong, but there could be hidden subtle issues." width="672" />
<p class="caption">
Figure 8.21: Plot of the fitted values against the observed values of the outcome variable. A straight line is what we’re hoping to see here. This looks pretty good, suggesting that there’s nothing grossly wrong, but there could be hidden subtle issues.
</p>
</div>
<p>The third thing we might want to test is the linearity of the relationships between the predictors and the outcomes. There’s a few different things that you might want to do in order to check this. Firstly, it never hurts to just plot the relationship between the fitted values <span class="math inline">\(\hat{Y}_i\)</span> and the observed values <span class="math inline">\(Y_i\)</span> for the outcome variable, as illustrated in Figure <a href="regression.html#fig:regressionlinearity">8.21</a>. To draw this we could use the <code>fitted.values()</code> function to extract the <span class="math inline">\(\hat{Y_i}\)</span> values in much the same way that we used the <code>residuals()</code> function to extract the <span class="math inline">\(\epsilon_i\)</span> values. So the commands to draw this figure might look like this:</p>
<div class="sourceCode" id="cb724"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb724-1"><a href="regression.html#cb724-1" aria-hidden="true" tabindex="-1"></a> yhat<span class="fl">.2</span> <span class="ot">&lt;-</span> <span class="fu">fitted.values</span>( <span class="at">object =</span> regression<span class="fl">.2</span> )</span>
<span id="cb724-2"><a href="regression.html#cb724-2" aria-hidden="true" tabindex="-1"></a> <span class="fu">plot</span>( <span class="at">x =</span> yhat<span class="fl">.2</span>, </span>
<span id="cb724-3"><a href="regression.html#cb724-3" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> parenthood<span class="sc">$</span>dan.grump,</span>
<span id="cb724-4"><a href="regression.html#cb724-4" aria-hidden="true" tabindex="-1"></a>       <span class="at">xlab =</span> <span class="st">&quot;Fitted Values&quot;</span>,</span>
<span id="cb724-5"><a href="regression.html#cb724-5" aria-hidden="true" tabindex="-1"></a>       <span class="at">ylab =</span> <span class="st">&quot;Observed Values&quot;</span> </span>
<span id="cb724-6"><a href="regression.html#cb724-6" aria-hidden="true" tabindex="-1"></a> )</span></code></pre></div>
<p>One of the reasons I like to draw these plots is that they give you a kind of “big picture view.” If this plot looks approximately linear, then we’re probably not doing too badly (though that’s not to say that there aren’t problems). However, if you can see big departures from linearity here, then it strongly suggests that you need to make some changes.</p>
<p>In any case, in order to get a more detailed picture it’s often more informative to look at the relationship between the fitted values and the residuals themselves. Again, we could draw this plot using low level commands, but there’s an easier way. Just <code>plot()</code> the regression model, and select <code>which = 1</code>:</p>
<div class="sourceCode" id="cb725"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb725-1"><a href="regression.html#cb725-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> regression<span class="fl">.2</span>, <span class="at">which =</span> <span class="dv">1</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:regressionplot1"></span>
<img src="schuster-statistics-remix_files/figure-html/regressionplot1-1.png" alt="Plot of the fitted values against the residuals for `regression.2`, with a line showing the relationship between the two. If this is horizontal and straight, then we can feel reasonably confident that the &quot;average residual&quot; for all &quot;fitted values&quot; is more or less the same. This is one of the standard regression plots produced by the `plot()` function when the input is a linear regression object. It is obtained by setting `which=1`." width="672" />
<p class="caption">
Figure 8.22: Plot of the fitted values against the residuals for <code>regression.2</code>, with a line showing the relationship between the two. If this is horizontal and straight, then we can feel reasonably confident that the “average residual” for all “fitted values” is more or less the same. This is one of the standard regression plots produced by the <code>plot()</code> function when the input is a linear regression object. It is obtained by setting <code>which=1</code>.
</p>
</div>
<p>The output is shown in Figure <a href="regression.html#fig:regressionplot1">8.22</a>. As you can see, not only does it draw the scatterplot showing the fitted value against the residuals, it also plots a line through the data that shows the relationship between the two. Ideally, this should be a straight, perfectly horizontal line. There’s some hint of curvature here, but it’s not clear whether or not we be concerned.</p>
<p>A somewhat more advanced version of the same plot is produced by the <code>residualPlots()</code> function in the <code>car</code> package. This function not only draws plots comparing the fitted values to the residuals, it does so for each individual predictor. The command is and the resulting plots are shown in Figure <a href="regression.html#fig:residualPlots">8.23</a>.</p>
<div class="sourceCode" id="cb726"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb726-1"><a href="regression.html#cb726-1" aria-hidden="true" tabindex="-1"></a><span class="fu">residualPlots</span>( <span class="at">model =</span> regression<span class="fl">.2</span> ) </span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:residualPlots"></span>
<img src="schuster-statistics-remix_files/figure-html/residualPlots-1.png" alt="Plot of the fitted values against the residuals for `regression.2`, along with similar plots for the two predictors individually. This plot is produced by the `residualPlots()` function in the `car` package. Note that it refers to the residuals as &quot;Pearson residuals&quot;, but in this context these are the same as ordinary residuals." width="672" />
<p class="caption">
Figure 8.23: Plot of the fitted values against the residuals for <code>regression.2</code>, along with similar plots for the two predictors individually. This plot is produced by the <code>residualPlots()</code> function in the <code>car</code> package. Note that it refers to the residuals as “Pearson residuals,” but in this context these are the same as ordinary residuals.
</p>
</div>
<pre><code>##            Test stat Pr(&gt;|Test stat|)  
## dan.sleep     2.1604          0.03323 *
## baby.sleep   -0.5445          0.58733  
## Tukey test    2.1615          0.03066 *
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Note that this function also reports the results of a bunch of <strong><em>curvature tests</em></strong>. For a predictor variable <span class="math inline">\(X\)</span> in some regression model, this test is equivalent to adding a new predictor to the model corresponding to <span class="math inline">\(X^2\)</span>, and running the <span class="math inline">\(t\)</span>-test on the <span class="math inline">\(b\)</span> coefficient associated with this new predictor. If it comes up significant, it implies that there is some nonlinear relationship between the variable and the residuals.</p>
<p>The third line here is the <strong><em>Tukey test</em></strong>, which is basically the same test, except that instead of squaring one of the predictors and adding it to the model, you square the fitted-value. In any case, the fact that the curvature tests have come up significant is hinting that the curvature that we can see in Figures <a href="regression.html#fig:regressionplot1">8.22</a> and <a href="regression.html#fig:residualPlots">8.23</a> is genuine;<a href="#fn128" class="footnote-ref" id="fnref128"><sup>128</sup></a> although it still bears remembering that the pattern in Figure <a href="regression.html#fig:regressionlinearity">8.21</a> is pretty damn straight: in other words the deviations from linearity are pretty small, and probably not worth worrying about.</p>
<p>In a lot of cases, the solution to this problem (and many others) is to transform one or more of the variables.</p>
<p><strong>Dave note</strong>: In my graduate training, I was taught to be conservative with variable transformations because they improve the statistical performance at the cost of interpretability. A significant relationship between a log-transformed Y and and X needs to be interpreted that way; you can no longer say you found a relationship between Y and X. For this reason, it may be preferable to suffer a decrease in statistical performance (possibly increasing the Type II error probability) rather than end up with a significant model that is challenging to interpret. In case you need it (and because <span class="citation"><a href="#ref-Navarro2018" role="doc-biblioref">Navarro</a> (<a href="#ref-Navarro2018" role="doc-biblioref">2018</a>)</span> said it was common), I will leave the following mention of the Box-Cox transform.</p>
<p>We discussed the basics of variable transformation in Sections <a href="#transform"><strong>??</strong></a> and (mathfunc), but I do want to make special note of one additional possibility that I didn’t mention earlier: the Box-Cox transform. The Box-Cox function is a fairly simple one, but it’s very widely used
<span class="math display">\[
f(x,\lambda) = \frac{x^\lambda - 1}{\lambda}
\]</span>
for all values of <span class="math inline">\(\lambda\)</span> except <span class="math inline">\(\lambda = 0\)</span>. When <span class="math inline">\(\lambda = 0\)</span> we just take the natural logarithm (i.e., <span class="math inline">\(\ln(x)\)</span>). You can calculate it using the <code>boxCox()</code> function in the <code>car</code> package. Better yet, if what you’re trying to do is convert a data to normal, or as normal as possible, there’s the <code>powerTransform()</code> function in the <code>car</code> package that can estimate the best value of <span class="math inline">\(\lambda\)</span>. Variable transformation is another topic that deserves a fairly detailed treatment, but (again) due to deadline constraints, it will have to wait until a future version of this book.</p>
</div>
<div id="regressionhomogeneity" class="section level3" number="8.13.5">
<h3><span class="header-section-number">8.13.5</span> Checking the homogeneity of variance</h3>
<p>The regression models that we’ve talked about all make a homogeneity of variance assumption: the variance of the residuals is assumed to be constant. The “default” plot that R provides to help with doing this (<code>which = 3</code> when using <code>plot()</code>) shows a plot of the square root of the size of the residual <span class="math inline">\(\sqrt{|\epsilon_i|}\)</span>, as a function of the fitted value <span class="math inline">\(\hat{Y}_i\)</span>. We can produce the plot using the following command,</p>
<div class="sourceCode" id="cb728"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb728-1"><a href="regression.html#cb728-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> regression<span class="fl">.2</span>, <span class="at">which =</span> <span class="dv">3</span>)</span></code></pre></div>
<p>and the resulting plot is shown in Figure <a href="regression.html#fig:regressionplot3">8.24</a>. Note that this plot actually uses the standardised residuals (i.e., converted to <span class="math inline">\(z\)</span> scores) rather than the raw ones, but it’s immaterial from our point of view. What we’re looking to see here is a straight, horizontal line running through the middle of the plot.</p>
<div class="figure"><span style="display:block;" id="fig:regressionplot3"></span>
<img src="schuster-statistics-remix_files/figure-html/regressionplot3-1.png" alt="Plot of the fitted values (model predictions) against the square root of the abs standardised residuals. This plot is used to diagnose violations of homogeneity of variance. If the variance is really constant, then the line through the middle should be horizontal and flat. This is one of the standard regression plots produced by the `plot()` function when the input is a linear regression object. It is obtained by setting `which=3`." width="672" />
<p class="caption">
Figure 8.24: Plot of the fitted values (model predictions) against the square root of the abs standardised residuals. This plot is used to diagnose violations of homogeneity of variance. If the variance is really constant, then the line through the middle should be horizontal and flat. This is one of the standard regression plots produced by the <code>plot()</code> function when the input is a linear regression object. It is obtained by setting <code>which=3</code>.
</p>
</div>
<p>A slightly more formal approach is to run hypothesis tests. The <code>car</code> package provides a function called <code>ncvTest()</code> (<strong><em>non-constant variance test</em></strong>) that can be used for this purpose <span class="citation">(<a href="#ref-Cook1983" role="doc-biblioref">R. D. Cook and Weisberg 1983</a>)</span>. I won’t explain the details of how it works, other than to say that the idea is that what you do is run a regression to see if there is a relationship between the squared residuals <span class="math inline">\(\epsilon_i\)</span> and the fitted values <span class="math inline">\(\hat{Y}_i\)</span>, or possibly to run a regression using all of the original predictors instead of just <span class="math inline">\(\hat{Y}_i\)</span>.<a href="#fn129" class="footnote-ref" id="fnref129"><sup>129</sup></a> Using the default settings, the <code>ncvTest()</code> looks for a relationship between <span class="math inline">\(\hat{Y}_i\)</span> and the variance of the residuals, making it a straightforward analogue of Figure <a href="regression.html#fig:regressionplot3">8.24</a>. So if we run it for our model,</p>
<div class="sourceCode" id="cb729"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb729-1"><a href="regression.html#cb729-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ncvTest</span>( regression<span class="fl">.2</span> )</span></code></pre></div>
<pre><code>## Non-constant Variance Score Test 
## Variance formula: ~ fitted.values 
## Chisquare = 0.09317511, Df = 1, p = 0.76018</code></pre>
<p>We see that our original impression was right: there’s no violations of homogeneity of variance in this data.</p>
<p><strong>Dave note:</strong> According to <span class="citation"><a href="#ref-Cohen2013" role="doc-biblioref">B. H. Cohen</a> (<a href="#ref-Cohen2013" role="doc-biblioref">2013</a>)</span>, ANOVA is robust to violations of homogeneity of variance so long as sample sizes are roughly equal. If your model is comparing groups, and those groups are unequal such that one group is 1.5x or more of another, then you may need to address your violation of homogeneity of variance. In a design with balanced groups, I would not go to the trouble of the correction described next. I leave it included here in case you ever need it.</p>
<p>It’s a bit beyond the scope of this chapter to talk too much about how to deal with violations of homogeneity of variance, but I’ll give you a quick sense of what you need to consider. The <em>main</em> thing to worry about, if homogeneity of variance is violated, is that the standard error estimates associated with the regression coefficients are no longer entirely reliable, and so your <span class="math inline">\(t\)</span> tests for the coefficients aren’t quite right either. A simple fix to the problem is to make use of a “heteroscedasticity corrected covariance matrix” when estimating the standard errors. These are often called <strong><em>sandwich estimators</em></strong>, for reasons that only make sense if you understand the maths at a low level<a href="#fn130" class="footnote-ref" id="fnref130"><sup>130</sup></a> have implemented as the default in the <code>hccm()</code> function is a tweak on this, proposed by <span class="citation"><a href="#ref-Long2000" role="doc-biblioref">Long and Ervin</a> (<a href="#ref-Long2000" role="doc-biblioref">2000</a>)</span>. This version uses <span class="math inline">\(\Sigma = \mbox{diag}(\epsilon_i^2/(1-h_i^2))\)</span>, where <span class="math inline">\(h_i\)</span> is the <span class="math inline">\(i\)</span>th hat value. Gosh, regression is <em>fun</em>, isn’t it?] You don’t need to understand what this means (not for an introductory class), but it might help to note that there’s a <code>hccm()</code> function in the <code>car()</code> package that does it. Better yet, you don’t even need to use it. You can use the <code>coeftest()</code> function in the <code>lmtest</code> package, but you need the <code>car</code> package loaded:</p>
<div class="sourceCode" id="cb731"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb731-1"><a href="regression.html#cb731-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(lmtest)</span>
<span id="cb731-2"><a href="regression.html#cb731-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(car)</span>
<span id="cb731-3"><a href="regression.html#cb731-3" aria-hidden="true" tabindex="-1"></a><span class="fu">coeftest</span>( regression<span class="fl">.2</span>, <span class="at">vcov=</span> hccm )</span></code></pre></div>
<pre><code>## 
## t test of coefficients:
## 
##               Estimate Std. Error  t value Pr(&gt;|t|)    
## (Intercept) 125.965566   3.247285  38.7910   &lt;2e-16 ***
## dan.sleep    -8.950250   0.615820 -14.5339   &lt;2e-16 ***
## baby.sleep    0.010524   0.291565   0.0361   0.9713    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Not surprisingly, these <span class="math inline">\(t\)</span> tests are pretty much identical to the ones that we saw when we used the <code>summary(regression.2)</code> command earlier; because the homogeneity of variance assumption wasn’t violated. But if it had been, we might have seen some more substantial differences.</p>
</div>
<div id="regressioncollinearity" class="section level3" number="8.13.6">
<h3><span class="header-section-number">8.13.6</span> Checking for collinearity</h3>
<p>The last kind of regression diagnostic that I’m going to discuss in this chapter is the use of <strong><em>variance inflation factors</em></strong> (VIFs), which are useful for determining whether or not the predictors in your regression model are too highly correlated with each other. There is a variance inflation factor associated with each predictor <span class="math inline">\(X_k\)</span> in the model, and the formula for the <span class="math inline">\(k\)</span>-th VIF is:
<span class="math display">\[
\mbox{VIF}_k = \frac{1}{1-{R^2_{(-k)}}}
\]</span>
where <span class="math inline">\(R^2_{(-k)}\)</span> refers to <span class="math inline">\(R\)</span>-squared value you would get if you ran a regression using <span class="math inline">\(X_k\)</span> as the outcome variable, and all the other <span class="math inline">\(X\)</span> variables as the predictors. The idea here is that <span class="math inline">\(R^2_{(-k)}\)</span> is a very good measure of the extent to which <span class="math inline">\(X_k\)</span> is correlated with all the other variables in the model. Better yet, the square root of the VIF is pretty interpretable: it tells you how much wider the confidence interval for the corresponding coefficient <span class="math inline">\(b_k\)</span> is, relative to what you would have expected if the predictors are all nice and uncorrelated with one another. If you’ve only got two predictors, the VIF values are always going to be the same, as we can see if we use the <code>vif()</code> function (<code>car</code> package)…</p>
<div class="sourceCode" id="cb733"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb733-1"><a href="regression.html#cb733-1" aria-hidden="true" tabindex="-1"></a><span class="fu">vif</span>( <span class="at">mod =</span> regression<span class="fl">.2</span> )</span></code></pre></div>
<pre><code>##  dan.sleep baby.sleep 
##   1.651038   1.651038</code></pre>
<p>And since the square root of 1.65 is 1.28, we see that the correlation between our two predictors isn’t causing much of a problem.</p>
<p>To give a sense of how we could end up with a model that has bigger collinearity problems, suppose I were to run a much less interesting regression model, in which I tried to predict the <code>day</code> on which the data were collected, as a function of all the other variables in the data set. To see why this would be a bit of a problem, let’s have a look at the correlation matrix for all four variables:</p>
<div class="sourceCode" id="cb735"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb735-1"><a href="regression.html#cb735-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>( parenthood )</span></code></pre></div>
<pre><code>##              dan.sleep  baby.sleep   dan.grump         day
## dan.sleep   1.00000000  0.62794934 -0.90338404 -0.09840768
## baby.sleep  0.62794934  1.00000000 -0.56596373 -0.01043394
## dan.grump  -0.90338404 -0.56596373  1.00000000  0.07647926
## day        -0.09840768 -0.01043394  0.07647926  1.00000000</code></pre>
<p>We have some fairly large correlations between some of our predictor variables! When we run the regression model and look at the VIF values, we see that the collinearity is causing a lot of uncertainty about the coefficients. First, run the regression…</p>
<div class="sourceCode" id="cb737"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb737-1"><a href="regression.html#cb737-1" aria-hidden="true" tabindex="-1"></a>regression<span class="fl">.3</span> <span class="ot">&lt;-</span> <span class="fu">lm</span>( day <span class="sc">~</span> baby.sleep <span class="sc">+</span> dan.sleep <span class="sc">+</span> dan.grump, parenthood )</span></code></pre></div>
<p>and second, look at the VIFs…</p>
<div class="sourceCode" id="cb738"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb738-1"><a href="regression.html#cb738-1" aria-hidden="true" tabindex="-1"></a><span class="fu">vif</span>( regression<span class="fl">.3</span> )</span></code></pre></div>
<pre><code>## baby.sleep  dan.sleep  dan.grump 
##   1.651064   6.102337   5.437903</code></pre>
<p>Yep, that’s some mighty fine collinearity you’ve got there.</p>
<p><strong>Dave note:</strong> A VIF cutoff of 4 is commonly used, where VIF &lt;= 4 is not concerning, but VIF &gt; 4 suggests redudnancy of variables in the model. The statistical solution would be to eliminate redundant variables from the model.</p>
</div>
</div>
<div id="modelselreg" class="section level2" number="8.14">
<h2><span class="header-section-number">8.14</span> Model selection</h2>
<p>One fairly major problem that remains is the problem of “model selection.” That is, if we have a data set that contains several variables, which ones should we include as predictors, and which ones should we not include? In other words, we have a problem of <strong><em>variable selection</em></strong>. In general, model selection is a complex business, but it’s made somewhat simpler if we restrict ourselves to the problem of choosing a subset of the variables that ought to be included in the model. Nevertheless, I’m not going to try covering even this reduced topic in a lot of detail. Instead, I’ll talk about two broad principles that you need to think about; and then discuss one concrete tool that R provides to help you select a subset of variables to include in your model. Firstly, the two principles:</p>
<ul>
<li>It’s nice to have an actual substantive basis for your choices. That is, in a lot of situations you the researcher have good reasons to pick out a smallish number of possible regression models that are of theoretical interest; these models will have a sensible interpretation in the context of your field. Never discount the importance of this. Statistics serves the scientific process, not the other way around.</li>
<li>To the extent that your choices rely on statistical inference, there is a trade off between simplicity and goodness of fit. As you add more predictors to the model, you make it more complex; each predictor adds a new free parameter (i.e., a new regression coefficient), and each new parameter increases the model’s capacity to “absorb” random variations. So the goodness of fit (e.g., <span class="math inline">\(R^2\)</span>) continues to rise as you add more predictors no matter what. If you want your model to be able to generalise well to new observations, you need to avoid throwing in too many variables.</li>
</ul>
<p>This latter principle is often referred to as <strong><em>Ockham’s razor</em></strong>, and is often summarised in terms of the following pithy saying: <em>do not multiply entities beyond necessity</em>. We also calls this <strong>parsimony.</strong> In this context, it means: don’t chuck in a bunch of largely irrelevant predictors just to boost your <span class="math inline">\(R^2\)</span>. Hm. Yeah, the original was better.</p>
<p>In any case, what we need is an actual mathematical criterion that will implement the qualitative principle behind Ockham’s razor in the context of selecting a regression model. As it turns out there are several possibilities. The one that I’ll talk about is the <strong><em>Akaike information criterion</em></strong> [AIC; <span class="citation"><a href="#ref-Akaike1974" role="doc-biblioref">Akaike</a> (<a href="#ref-Akaike1974" role="doc-biblioref">1974</a>)</span>] simply because it’s the default one used in the R function <code>step()</code>. In the context of a linear regression model (and ignoring terms that don’t depend on the model in any way!), the AIC for a model that has <span class="math inline">\(K\)</span> predictor variables plus an intercept is:<a href="#fn131" class="footnote-ref" id="fnref131"><sup>131</sup></a>
<span class="math display">\[
\mbox{AIC} = \displaystyle\frac{\mbox{SS}_{res}}{\hat{\sigma}}^2+ 2K
\]</span>
The smaller the AIC value, the better the model performance is. If we ignore the low level details, it’s fairly obvious what the AIC does: on the left we have a term that increases as the model predictions get worse; on the right we have a term that increases as the model complexity increases. The best model is the one that fits the data well (low residuals; left hand side) using as few predictors as possible (low <span class="math inline">\(K\)</span>; right hand side). In short, this is a simple implementation of Ockham’s razor.</p>
<p><strong>Dave note:</strong> Please consider the following sections to have a big “Warning!” sign next to them when used in confirmatory research. They are inherently exploratory techniques. One assumption common to all automated model selection strategies is that they assume that your sample data are perfect representations of their population distributions. The computer is effectively testing many possible models and picking the one that fits best. The resulting model is always exploratory, tentative, and highly dependent on the sample. It would need to be tested in a confirmatory study. It would be highly inappropriate and misleading to present a computer-generated model as if it was developed <em>a priori</em> or without explaining the process used to create the model.</p>
<div id="backward-elimination" class="section level3" number="8.14.1">
<h3><span class="header-section-number">8.14.1</span> Backward elimination</h3>
<p>Okay, let’s have a look at the <code>step()</code> function at work. In this example I’ll keep it simple and use only the basic <strong><em>backward elimination</em></strong> approach. That is, start with the complete regression model, including all possible predictors. Then, at each “step” we try all possible ways of removing one of the variables, and whichever of these is best (in terms of lowest AIC value) is accepted. This becomes our new regression model; and we then try all possible deletions from the new model, again choosing the option with lowest AIC. This process continues until we end up with a model that has a lower AIC value than any of the other possible models that you could produce by deleting one of its predictors. Let’s see this in action. First, I need to define the model from which the process starts.</p>
<div class="sourceCode" id="cb740"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb740-1"><a href="regression.html#cb740-1" aria-hidden="true" tabindex="-1"></a>full.model <span class="ot">&lt;-</span> <span class="fu">lm</span>( <span class="at">formula =</span> dan.grump <span class="sc">~</span> dan.sleep <span class="sc">+</span> baby.sleep <span class="sc">+</span> day,  </span>
<span id="cb740-2"><a href="regression.html#cb740-2" aria-hidden="true" tabindex="-1"></a>                   <span class="at">data =</span> parenthood  </span>
<span id="cb740-3"><a href="regression.html#cb740-3" aria-hidden="true" tabindex="-1"></a> )</span></code></pre></div>
<p>That’s nothing terribly new: yet another regression. Booooring. Still, we do need to do it: the <code>object</code> argument to the <code>step()</code> function will be this regression model. With this in mind, I would call the <code>step()</code> function using the following command:</p>
<div class="sourceCode" id="cb741"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb741-1"><a href="regression.html#cb741-1" aria-hidden="true" tabindex="-1"></a> <span class="fu">step</span>( <span class="at">object =</span> full.model,     <span class="co"># start at the full model</span></span>
<span id="cb741-2"><a href="regression.html#cb741-2" aria-hidden="true" tabindex="-1"></a>       <span class="at">direction =</span> <span class="st">&quot;backward&quot;</span>   <span class="co"># allow it remove predictors but not add them</span></span>
<span id="cb741-3"><a href="regression.html#cb741-3" aria-hidden="true" tabindex="-1"></a> )</span></code></pre></div>
<pre><code>## Start:  AIC=299.08
## dan.grump ~ dan.sleep + baby.sleep + day
## 
##              Df Sum of Sq    RSS    AIC
## - baby.sleep  1       0.1 1837.2 297.08
## - day         1       1.6 1838.7 297.16
## &lt;none&gt;                    1837.1 299.08
## - dan.sleep   1    4909.0 6746.1 427.15
## 
## Step:  AIC=297.08
## dan.grump ~ dan.sleep + day
## 
##             Df Sum of Sq    RSS    AIC
## - day        1       1.6 1838.7 295.17
## &lt;none&gt;                   1837.2 297.08
## - dan.sleep  1    8103.0 9940.1 463.92
## 
## Step:  AIC=295.17
## dan.grump ~ dan.sleep
## 
##             Df Sum of Sq    RSS    AIC
## &lt;none&gt;                   1838.7 295.17
## - dan.sleep  1    8159.9 9998.6 462.50</code></pre>
<pre><code>## 
## Call:
## lm(formula = dan.grump ~ dan.sleep, data = parenthood)
## 
## Coefficients:
## (Intercept)    dan.sleep  
##     125.956       -8.937</code></pre>
<p>although in practice I didn’t need to specify <code>direction</code> because <code>"backward"</code> is the default. The output is somewhat lengthy, so I’ll go through it slowly. Firstly, the output reports the AIC value for the current best model:</p>
<pre><code>Start:  AIC=299.08
dan.grump ~ dan.sleep + baby.sleep + day</code></pre>
<p>That’s our starting point. Since small AIC values are good, we want to see if we can get a value smaller than 299.08 by deleting one of those three predictors. So what R does is try all three possibilities, calculate the AIC values for each one, and then print out a short table with the results:</p>
<pre><code>             Df Sum of Sq    RSS    AIC
- baby.sleep  1       0.1 1837.2 297.08
- day         1       1.6 1838.7 297.16
&lt;none&gt;                    1837.1 299.08
- dan.sleep   1    4909.0 6746.1 427.15</code></pre>
<p>To read this table, it helps to note that the text in the left hand column is telling you what <em>change</em> R made to the regression model. So the line that reads <code>&lt;none&gt;</code> is the actual model we started with, and you can see on the right hand side that this still corresponds to an AIC value of 299.08 (obviously). The other three rows in the table correspond to the other three models that it looked at: it tried removing the <code>baby.sleep</code> variable, which is indicated by <code>- baby.sleep</code>, and this produced an AIC value of 297.08. That was the best of the three moves, so it’s at the top of the table. So, this move is accepted, and now we start again. There are two predictors left in the model, <code>dan.sleep</code> and <code>day</code>, so it tries deleting those:</p>
<pre><code>Step:  AIC=297.08
dan.grump ~ dan.sleep + day

            Df Sum of Sq    RSS    AIC
- day        1       1.6 1838.7 295.17
&lt;none&gt;                   1837.2 297.08
- dan.sleep  1    8103.0 9940.1 463.92</code></pre>
<p>Okay, so what we can see is that removing the <code>day</code> variable lowers the AIC value from 297.08 to 295.17. So R decides to keep that change too, and moves on:</p>
<pre><code>Step:  AIC=295.17
dan.grump ~ dan.sleep

            Df Sum of Sq    RSS    AIC
&lt;none&gt;                   1838.7 295.17
- dan.sleep  1    8159.9 9998.6 462.50</code></pre>
<p>This time around, there’s no further deletions that can actually improve the AIC value. So the <code>step()</code> function stops, and prints out the result of the best regression model it could find:</p>
<pre><code>Call:
lm(formula = dan.grump ~ dan.sleep, data = parenthood)

Coefficients:
(Intercept)    dan.sleep  
    125.956       -8.937  </code></pre>
<p>which is (perhaps not all that surprisingly) the <code>regression.1</code> model that we started with at the beginning of the chapter.</p>
</div>
<div id="forward-selection" class="section level3" number="8.14.2">
<h3><span class="header-section-number">8.14.2</span> Forward selection</h3>
<p>As an alternative, you can also try <strong><em>forward selection</em></strong>. This time around we start with the smallest possible model as our start point, and only consider the possible additions to the model. However, there’s one complication: you also need to tell <code>step()</code> what the largest possible model you’re willing to entertain is, using the <code>scope</code> argument. The simplest usage is like this:</p>
<div class="sourceCode" id="cb749"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb749-1"><a href="regression.html#cb749-1" aria-hidden="true" tabindex="-1"></a> null.model <span class="ot">&lt;-</span> <span class="fu">lm</span>( dan.grump <span class="sc">~</span> <span class="dv">1</span>, parenthood )   <span class="co"># intercept only.</span></span>
<span id="cb749-2"><a href="regression.html#cb749-2" aria-hidden="true" tabindex="-1"></a> <span class="fu">step</span>( <span class="at">object =</span> null.model,     <span class="co"># start with null.model</span></span>
<span id="cb749-3"><a href="regression.html#cb749-3" aria-hidden="true" tabindex="-1"></a>       <span class="at">direction =</span> <span class="st">&quot;forward&quot;</span>,   <span class="co"># only consider &quot;addition&quot; moves</span></span>
<span id="cb749-4"><a href="regression.html#cb749-4" aria-hidden="true" tabindex="-1"></a>       <span class="at">scope =</span>  dan.grump <span class="sc">~</span> dan.sleep <span class="sc">+</span> baby.sleep <span class="sc">+</span> day  <span class="co"># largest model allowed</span></span>
<span id="cb749-5"><a href="regression.html#cb749-5" aria-hidden="true" tabindex="-1"></a> )</span></code></pre></div>
<pre><code>## Start:  AIC=462.5
## dan.grump ~ 1
## 
##              Df Sum of Sq    RSS    AIC
## + dan.sleep   1    8159.9 1838.7 295.17
## + baby.sleep  1    3202.7 6795.9 425.89
## &lt;none&gt;                    9998.6 462.50
## + day         1      58.5 9940.1 463.92
## 
## Step:  AIC=295.17
## dan.grump ~ dan.sleep
## 
##              Df Sum of Sq    RSS    AIC
## &lt;none&gt;                    1838.7 295.17
## + day         1   1.55760 1837.2 297.08
## + baby.sleep  1   0.02858 1838.7 297.16</code></pre>
<pre><code>## 
## Call:
## lm(formula = dan.grump ~ dan.sleep, data = parenthood)
## 
## Coefficients:
## (Intercept)    dan.sleep  
##     125.956       -8.937</code></pre>
<p>If I do this, the output takes on a similar form, but now it only considers addition (<code>+</code>) moves rather than deletion (<code>-</code>) moves:</p>
<pre><code>Start:  AIC=462.5
dan.grump ~ 1

             Df Sum of Sq    RSS    AIC
+ dan.sleep   1    8159.9 1838.7 295.17
+ baby.sleep  1    3202.7 6795.9 425.89
&lt;none&gt;                    9998.6 462.50
+ day         1      58.5 9940.1 463.92

Step:  AIC=295.17
dan.grump ~ dan.sleep

             Df Sum of Sq    RSS    AIC
&lt;none&gt;                    1838.7 295.17
+ day         1   1.55760 1837.2 297.08
+ baby.sleep  1   0.02858 1838.7 297.16

Call:
lm(formula = dan.grump ~ dan.sleep, data = parenthood)

Coefficients:
(Intercept)    dan.sleep  
    125.956       -8.937  </code></pre>
<p>As you can see, it’s found the same model. In general though, forward and backward selection don’t always have to end up in the same place.</p>
</div>
<div id="a-caveat" class="section level3" number="8.14.3">
<h3><span class="header-section-number">8.14.3</span> A caveat</h3>
<p>Automated variable selection methods are seductive things, especially when they’re bundled up in (fairly) simple functions like <code>step()</code>. They provide an element of objectivity to your model selection, and that’s kind of nice. Unfortunately, they’re sometimes used as an excuse for thoughtlessness. No longer do you have to think carefully about which predictors to add to the model and what the theoretical basis for their inclusion might be… everything is solved by the magic of AIC. And if we start throwing around phrases like Ockham’s razor, well, it sounds like everything is wrapped up in a nice neat little package that no-one can argue with.</p>
<p>Or, perhaps not. Firstly, there’s very little agreement on what counts as an appropriate model selection criterion. When I was taught backward elimination as an undergraduate, we used <span class="math inline">\(F\)</span>-tests to do it, because that was the default method used by the software. The default in the <code>step()</code> function is AIC, and since this is an introductory text that’s the only method I’ve described, but the AIC is hardly the Word of the Gods of Statistics. It’s an approximation, derived under certain assumptions, and it’s guaranteed to work only for large samples when those assumptions are met. Alter those assumptions and you get a different criterion, like the BIC for instance. Take a different approach again and you get the NML criterion. Decide that you’re a Bayesian and you get model selection based on posterior odds ratios. Then there are a bunch of regression specific tools that I haven’t mentioned. And so on. All of these different methods have strengths and weaknesses, and some are easier to calculate than others (AIC is probably the easiest of the lot, which might account for its popularity). Almost all of them produce the same answers when the answer is “obvious” but there’s a fair amount of disagreement when the model selection problem becomes hard.</p>
<p>What does this mean in practice? Well, you <em>could</em> go and spend several years teaching yourself the theory of model selection, learning all the ins and outs of it; so that you could finally decide on what you personally think the right thing to do is. Speaking as someone who actually did that, I wouldn’t recommend it: you’ll probably come out the other side even more confused than when you started. A better strategy is to show a bit of common sense… if you’re staring at the results of a <code>step()</code> procedure, and the model that makes sense is close to having the smallest AIC, but is narrowly defeated by a model that doesn’t make any sense… trust your instincts. Statistical model selection is an inexact tool, and as I said at the beginning, <em>interpretability matters</em>.</p>
</div>
<div id="comparing-two-regression-models" class="section level3" number="8.14.4">
<h3><span class="header-section-number">8.14.4</span> Comparing two regression models</h3>
<p>An alternative to using automated model selection procedures is for the researcher to explicitly select two or more regression models to compare to each other. You can do this in a few different ways, depending on what research question you’re trying to answer. Suppose we want to know whether or not the amount of sleep that my son got has any relationship to my grumpiness, over and above what we might expect from the amount of sleep that I got. We also want to make sure that the day on which we took the measurement has no influence on the relationship. That is, we’re interested in the relationship between <code>baby.sleep</code> and <code>dan.grump</code>, and from that perspective <code>dan.sleep</code> and <code>day</code> are nuisance variable or <strong><em>covariates</em></strong> that we want to control for. In this situation, what we would like to know is whether <code>dan.grump ~ dan.sleep + day + baby.sleep</code> (which I’ll call Model 1, or <code>M1</code>) is a better regression model for these data than <code>dan.grump ~ dan.sleep + day</code> (which I’ll call Model 0, or <code>M0</code>). There are two different ways we can compare these two models, one based on a model selection criterion like AIC, and the other based on an explicit hypothesis test. I’ll show you the AIC based approach first because it’s simpler, and follows naturally from the <code>step()</code> function that we saw in the last section. The first thing I need to do is actually run the regressions:</p>
<div class="sourceCode" id="cb753"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb753-1"><a href="regression.html#cb753-1" aria-hidden="true" tabindex="-1"></a>M0 <span class="ot">&lt;-</span> <span class="fu">lm</span>( dan.grump <span class="sc">~</span> dan.sleep <span class="sc">+</span> day, parenthood )</span>
<span id="cb753-2"><a href="regression.html#cb753-2" aria-hidden="true" tabindex="-1"></a>M1 <span class="ot">&lt;-</span> <span class="fu">lm</span>( dan.grump <span class="sc">~</span> dan.sleep <span class="sc">+</span> day <span class="sc">+</span> baby.sleep, parenthood )</span></code></pre></div>
<p>Now that I have my regression models, I could use the <code>summary()</code> function to run various hypothesis tests and other useful statistics, just as we have discussed throughout this chapter. However, since the current focus on model comparison, I’ll skip this step and go straight to the AIC calculations. Conveniently, the <code>AIC()</code> function in R lets you input several regression models, and it will spit out the AIC values for each of them:<a href="#fn132" class="footnote-ref" id="fnref132"><sup>132</sup></a></p>
<div class="sourceCode" id="cb754"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb754-1"><a href="regression.html#cb754-1" aria-hidden="true" tabindex="-1"></a><span class="fu">AIC</span>( M0, M1 )</span></code></pre></div>
<pre><code>##    df      AIC
## M0  4 582.8681
## M1  5 584.8646</code></pre>
<p>Since Model 0 has the smaller AIC value, it is judged to be the better model for these data.</p>
<p>A somewhat different approach to the problem comes out of the hypothesis testing framework. Suppose you have two regression models, where one of them (Model 0) contains a <em>subset</em> of the predictors from the other one (Model 1). That is, Model 1 contains all of the predictors included in Model 0, plus one or more additional predictors. When this happens we say that Model 0 is <strong><em>nested</em></strong> within Model 1, or possibly that Model 0 is a <strong><em>submodel</em></strong> of Model 1. Regardless of the terminology what this means is that we can think of Model 0 as a null hypothesis and Model 1 as an alternative hypothesis. And in fact we can construct an <span class="math inline">\(F\)</span> test for this in a fairly straightforward fashion. We can fit both models to the data and obtain a residual sum of squares for both models. I’ll denote these as SS<span class="math inline">\(_{res}^{(0)}\)</span> and SS<span class="math inline">\(_{res}^{(1)}\)</span> respectively. The superscripting here just indicates which model we’re talking about. Then our <span class="math inline">\(F\)</span> statistic is
<span class="math display">\[
F = \frac{(\mbox{SS}_{res}^{(0)} - \mbox{SS}_{res}^{(1)})/k}{(\mbox{SS}_{res}^{(1)})/(N-p-1)}
\]</span>
where <span class="math inline">\(N\)</span> is the number of observations, <span class="math inline">\(p\)</span> is the number of predictors in the full model (not including the intercept), and <span class="math inline">\(k\)</span> is the difference in the number of parameters between the two models.<a href="#fn133" class="footnote-ref" id="fnref133"><sup>133</sup></a> The degrees of freedom here are <span class="math inline">\(k\)</span> and <span class="math inline">\(N-p-1\)</span>. Note that it’s often more convenient to think about the difference between those two SS values as a sum of squares in its own right. That is:
<span class="math display">\[
\mbox{SS}_\Delta = \mbox{SS}_{res}^{(0)} - \mbox{SS}_{res}^{(1)}
\]</span>
The reason why this his helpful is that we can express <span class="math inline">\(\mbox{SS}_\Delta\)</span> a measure of the extent to which the two models make different predictions about the the outcome variable. Specifically:
<span class="math display">\[
\mbox{SS}_\Delta  = \sum_{i} \left( \hat{y}_i^{(1)} - \hat{y}_i^{(0)} \right)^2
\]</span>
where <span class="math inline">\(\hat{y}_i^{(0)}\)</span> is the fitted value for <span class="math inline">\(y_i\)</span> according to model <span class="math inline">\(M_0\)</span> and <span class="math inline">\(\hat{y}_i^{(1)}\)</span> is the is the fitted value for <span class="math inline">\(y_i\)</span> according to model <span class="math inline">\(M_1\)</span>.</p>
<p>Okay, so that’s the hypothesis test that we use to compare two regression models to one another. Now, how do we do it in R? The answer is to use the <code>anova()</code> function. All we have to do is input the two models that we want to compare (null model first):</p>
<div class="sourceCode" id="cb756"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb756-1"><a href="regression.html#cb756-1" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>( M0, M1 )</span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Model 1: dan.grump ~ dan.sleep + day
## Model 2: dan.grump ~ dan.sleep + day + baby.sleep
##   Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)
## 1     97 1837.2                           
## 2     96 1837.1  1  0.063688 0.0033 0.9541</code></pre>
<p>Note that, just like we saw with the output from the <code>step()</code> function, R has used the acronym <code>RSS</code> to refer to the residual sum of squares from each model. That is, RSS in this output corresponds to SS<span class="math inline">\(_{res}\)</span> in the formula above. Since we have <span class="math inline">\(p&gt;.05\)</span> we retain the null hypothesis (<code>M0</code>). This approach to regression, in which we add all of our covariates into a null model, and then <em>add</em> the variables of interest into an alternative model, and then compare the two models in hypothesis testing framework, is often referred to as <strong><em>hierarchical regression</em></strong>.</p>
</div>
</div>
<div id="practical-issues-in-correlation-and-regression" class="section level2" number="8.15">
<h2><span class="header-section-number">8.15</span> Practical Issues in Correlation and Regression</h2>
<p>Text by David Schuster</p>
<div id="correlation-is-not-causation" class="section level3" number="8.15.1">
<h3><span class="header-section-number">8.15.1</span> Correlation is not causation</h3>
<p>Remember that causality can be inferred when the cause comes before the effect, a relationship is shown between cause and effect, and we can find no other plausible explanations for the effect other than the cause. Our statistical analysis (regardless of which GLM analysis is used) can help with the middle criterion; we can use statistics to provide evidence of a relationship between two variables. However, no statistical analysis can, by itself, allow us to make causal inferences. Our best tool for making causal inferences is an experimental design. Designs that are quasi-experimental or non-experimental will not let us make inferences about causality.</p>
<p>In this statement, correlation refers to the evidence of a relationship between two variables. Therefore, it would be a bit clearer if it was, “quasi- and non-experimental research designs are not causation.” but I suppose the original version is catchier.</p>
</div>
<div id="interpreting-nhst-in-big-data" class="section level3" number="8.15.2">
<h3><span class="header-section-number">8.15.2</span> Interpreting NHST in Big Data</h3>
<p>Sample size must be sufficiently large to detect a linear relationship. Larger samples increase power. Power is especially important when your measures are not reliable or when there is a weak linear relationship you are trying to find evidence for. You can also have too large of a sample. Extremely large samples with small effect sizes will be significant because it is assumed that your sample is an almost perfect representation of the population. Thus, even the tiniest effects will be significant. Take care to examine effect size and sample size when conducting NHST for correlations. Besides issues of power, a sufficiently large sample size is important to increase the stability of your estimates of the strength and direction of the correlation.</p>
</div>
<div id="outliers" class="section level3" number="8.15.3">
<h3><span class="header-section-number">8.15.3</span> Outliers</h3>
<p>If either of your variables includes an outlier, it will reduce the strength of your correlation. Because of this, you want to consider whether the extreme score belongs as part of your data set. The most common cause of an outlier in real world data is a mistake during data entry. As previously stated, being an outlier is not generally, itself, enough criteria for a researcher to exclude a case from a sample without concern that the sample will be biased.</p>
</div>
<div id="restriction-of-range" class="section level3" number="8.15.4">
<h3><span class="header-section-number">8.15.4</span> Restriction of Range</h3>
<p>When scores that are very high or very low on the IV, range restriction can occur. Range restriction lowers the strength of the correlation. <strong>Floor and ceiling effects</strong> can cause range restriction. Imagine a very easy math test as a measure of math skill. Students at the highest levels of math ability will score 100% on the test. Because the test does not measure past 100%, some information is lost about individuals who could have scored higher than 100% if it were possible. This is an example of a ceiling effect. Similarly, a very challenging test can result in 0% scores with the test similarly lacking diagnosticity between low-scorers at the bottom of the distribution.</p>
<p>This can be illustrated easily by drawing scatterplots. First, we start with two variables with a high correlation:</p>
<div class="sourceCode" id="cb758"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb758-1"><a href="regression.html#cb758-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Two variables with a known correlation of r = .9.</span></span>
<span id="cb758-2"><a href="regression.html#cb758-2" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">0.169062295</span>,<span class="fl">0.053822267</span>,<span class="fl">0.527217327</span>,<span class="fl">0.07727407</span>,<span class="fl">0.722027061</span>,<span class="sc">-</span><span class="fl">0.28132826</span>,<span class="sc">-</span><span class="fl">0.893490299</span>,<span class="fl">0.79516792</span>,<span class="fl">0.175189753</span>,<span class="sc">-</span><span class="fl">0.733235739</span>)</span>
<span id="cb758-3"><a href="regression.html#cb758-3" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.648162835</span>, <span class="fl">0.964985264</span>,<span class="fl">1.160828115</span>,<span class="fl">1.403311403</span>,<span class="fl">1.692166183</span>,<span class="fl">0.26407813</span>,<span class="sc">-</span><span class="fl">0.393116812</span>,<span class="fl">2.110632623</span>,<span class="fl">1.684217475</span>,<span class="fl">0.464734783</span>)</span>
<span id="cb758-4"><a href="regression.html#cb758-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb758-5"><a href="regression.html#cb758-5" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(x, y) <span class="co"># r =.9</span></span></code></pre></div>
<pre><code>## [1] 0.9</code></pre>
<div class="sourceCode" id="cb760"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb760-1"><a href="regression.html#cb760-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x,y,<span class="at">xlab=</span><span class="st">&quot;x&quot;</span>,<span class="at">ylab=</span><span class="st">&quot;y&quot;</span>) <span class="co"># generate a scatterplot and label the axes</span></span>
<span id="cb760-2"><a href="regression.html#cb760-2" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">reg =</span> <span class="fu">lm</span>(y <span class="sc">~</span> x)) <span class="co"># draw the line of best fit for the regression equation</span></span></code></pre></div>
<p><img src="schuster-statistics-remix_files/figure-html/unnamed-chunk-354-1.png" width="672" />
We will create a floor effect by saying that 0 is now the lowest possible score on this measure. Any negative values become zero in our new variable, <code>x_floor</code>. Notice the change in the line, fit, and <span class="math inline">\(r\)</span>.</p>
<div class="sourceCode" id="cb761"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb761-1"><a href="regression.html#cb761-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Two variables with a known correlation of r = .9, changing all scores where x &lt; 0 to 0.</span></span>
<span id="cb761-2"><a href="regression.html#cb761-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb761-3"><a href="regression.html#cb761-3" aria-hidden="true" tabindex="-1"></a>x_floor <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="fl">0.053822267</span>,<span class="fl">0.527217327</span>,<span class="fl">0.07727407</span>,<span class="fl">0.722027061</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="fl">0.79516792</span>,<span class="fl">0.175189753</span>,<span class="dv">0</span>)</span>
<span id="cb761-4"><a href="regression.html#cb761-4" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.648162835</span>, <span class="fl">0.964985264</span>,<span class="fl">1.160828115</span>,<span class="fl">1.403311403</span>,<span class="fl">1.692166183</span>,<span class="fl">0.26407813</span>,<span class="sc">-</span><span class="fl">0.393116812</span>,<span class="fl">2.110632623</span>,<span class="fl">1.684217475</span>,<span class="fl">0.464734783</span>)</span>
<span id="cb761-5"><a href="regression.html#cb761-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb761-6"><a href="regression.html#cb761-6" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(x_floor, y) <span class="co"># Correlation r = .74</span></span></code></pre></div>
<pre><code>## [1] 0.7353465</code></pre>
<div class="sourceCode" id="cb763"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb763-1"><a href="regression.html#cb763-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x_floor,y,<span class="at">xlab=</span><span class="st">&quot;x&quot;</span>,<span class="at">ylab=</span><span class="st">&quot;y&quot;</span>) <span class="co"># generate a scatterplot and label the axes</span></span>
<span id="cb763-2"><a href="regression.html#cb763-2" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">reg =</span> <span class="fu">lm</span>(y <span class="sc">~</span> x)) <span class="co"># draw the line of best fit for the regression equation</span></span></code></pre></div>
<p><img src="schuster-statistics-remix_files/figure-html/unnamed-chunk-355-1.png" width="672" />
A floor/ceiling effect can cause this problem but is not the only cause. Simply excluding low scores can have this effect. Next, we will eliminate any case with a negative value of x instead of removing it. Notice that although no scores are changed, the correlation is weaker. Whenever we start with a linear relationship and “zoom in” to a portion of it, the subset of the data will be a weaker correlation than the overall relationship. Therefore, restriction of range can result in a weaker correlation.</p>
<div class="sourceCode" id="cb764"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb764-1"><a href="regression.html#cb764-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Two variables with a known correlation of r = .9, excluding all scores where x &lt; 0.</span></span>
<span id="cb764-2"><a href="regression.html#cb764-2" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.053822267</span>,<span class="fl">0.527217327</span>,<span class="fl">0.07727407</span>,<span class="fl">0.722027061</span>,<span class="fl">0.79516792</span>,<span class="fl">0.175189753</span>)</span>
<span id="cb764-3"><a href="regression.html#cb764-3" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.964985264</span>,<span class="fl">1.160828115</span>,<span class="fl">1.403311403</span>,<span class="fl">1.692166183</span>,<span class="fl">2.110632623</span>,<span class="fl">1.684217475</span>)</span>
<span id="cb764-4"><a href="regression.html#cb764-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb764-5"><a href="regression.html#cb764-5" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(x, y) <span class="co"># r =.9</span></span></code></pre></div>
<pre><code>## [1] 0.6347461</code></pre>
<div class="sourceCode" id="cb766"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb766-1"><a href="regression.html#cb766-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x,y,<span class="at">xlab=</span><span class="st">&quot;x&quot;</span>,<span class="at">ylab=</span><span class="st">&quot;y&quot;</span>) <span class="co"># generate a scatterplot and label the axes</span></span>
<span id="cb766-2"><a href="regression.html#cb766-2" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">reg =</span> <span class="fu">lm</span>(y <span class="sc">~</span> x)) <span class="co"># draw the line of best fit for the regression equation</span></span></code></pre></div>
<p><img src="schuster-statistics-remix_files/figure-html/unnamed-chunk-356-1.png" width="672" /></p>
</div>
<div id="regression-toward-the-mean" class="section level3" number="8.15.5">
<h3><span class="header-section-number">8.15.5</span> Regression Toward the Mean</h3>
<p><span class="citation"><a href="#ref-Galton1886" role="doc-biblioref">Galton</a> (<a href="#ref-Galton1886" role="doc-biblioref">1886</a>)</span> came up with the concept of regression toward mediocrity which he applied to heredity. This phrase is designed to be pejorative (suggesting that inheritance is losing something of value) and Galton’s explanation for why it mattered was eugenics (see also the <a href="https://www.youtube.com/watch?v=JeCKftkNKJ0">Video: Eugenics and Francis Galton - 13 min</a> on this topic). We now understand that it has nothing to do with heredity and more to do with the properties of the mean. In present use, regression toward the mean refers to mistaking an extreme score as evidence of an effect. Imagine you are playing a challenging level of a video game. After a lot of practice, your score tends to hover around 10,000 points. Sometimes you score a bit more, sometimes a bit less, but if you were to have all the data available, you would find a mean of 10,000 points. Regression toward the mean would occur if you played this level one day and scored 20,000 points, then concluded that you were a video game superstar, left school, and went to be a pro gamer. Playing the level again, you are dismayed that your score has fallen back to around 10,000 points. What happened? How could you gain and lose such an ability? What is happening is that your performance has variability, and it is unlikely but possible to observe an extreme score. Researchers need to be mindful that extreme events will tend to be less extreme with repeated sampling.</p>
</div>
<div id="report-effect-size" class="section level3" number="8.15.6">
<h3><span class="header-section-number">8.15.6</span> Report Effect Size</h3>
<p>Interpretation of the correlation coefficient <span class="math inline">\(r\)</span> (or <span class="math inline">\(R^2\)</span>). The statistic <span class="math inline">\(r\)</span> measures both strength (i.e., it is the measure of effect size) and direction of a correlation. A strong correlation means that the data points of the scatterplot lie close to the line. When you report a correlation, give the proper interpretation of its effect size. Also note that, depending on the research question, small effect sizes can be important and meaningful.</p>
<p>How do you interpret the direction of an effect in multiple regression? <span class="math inline">\(R^2\)</span> will always be positive. The answer is to look at the direction of the coefficients. If they are positive, it suggests a positive relationship with the DV. If negative, it suggests a negative relationship with the DV.</p>
<div id="rsquared" class="section level4" number="8.15.6.1">
<h4><span class="header-section-number">8.15.6.1</span> Interpretation of <span class="math inline">\(\eta^2\)</span> and <span class="math inline">\(r^2\)</span> (Cohen, 1988)</h4>
<p>These are reference points, not firm cutoffs. For example, .056 is a medium effect size.</p>
<table style="width:54%;">
<colgroup>
<col width="30%" />
<col width="23%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Effect Size</th>
<th align="left">Interpretation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(\eta^2 = r^2 = .01\)</span></td>
<td align="left">Small effect</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\eta^2 = r^2 = .06\)</span></td>
<td align="left">Medium effect</td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(\eta^2 = r^2 = .14\)</span></td>
<td align="left">Large effect</td>
</tr>
</tbody>
</table>
</div>
<div id="interpr" class="section level4" number="8.15.6.2">
<h4><span class="header-section-number">8.15.6.2</span> Interpretation of <span class="math inline">\(r\)</span> (Cohen, 1988; Note: This is not <span class="math inline">\(r^2\)</span>)</h4>
<p>These are reference points, not firm cutoffs. For example, .056 is a medium effect size.</p>
<table style="width:54%;">
<colgroup>
<col width="30%" />
<col width="23%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Effect Size</th>
<th align="left">Interpretation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(r\pm.10\)</span> Small effe</td>
<td align="left">ct</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(r\pm.30\)</span> Medium e</td>
<td align="left">ffect</td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(r\pm.50\)</span> Large effe</td>
<td align="left">ct</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="what-are-degrees-of-freedom-again" class="section level3" number="8.15.7">
<h3><span class="header-section-number">8.15.7</span> What are Degrees of Freedom, again?</h3>
<p>This is more of a footnote. The calculation of variance is a fraction. The numerator of the fraction is a sum of squared deviations. The denominator of the fraction is degrees of freedom. I find it helpful to think about degrees of freedom in any of three ways (pick which makes the most sense to you):</p>
<ol style="list-style-type: decimal">
<li><p>Degrees of freedom (df), are the number of values free to vary given some constraint. The calculation of a mean has <span class="math inline">\(N - 1\)</span> degrees of freedom because if the mean is known and <span class="math inline">\(N - 1\)</span> of the values are known, then the last value is defined. Thus, only <span class="math inline">\(N - 1\)</span> values are free to vary. This is my least favorite explanation.</p></li>
<li><p>Degrees of freedom (df) are the number of values that provide information about variability. If you know only your score on the last exam, you could use that as an estimate of the class mean, but you would have no way of estimating variability of test scores in class (df = 0). If you know your score and another classmate’s, you can now estimate variability, albeit crudely (df = 1). With your score and seven others, you can make a better estimate of the variability in the class (df = 7). This is my favorite definition and it comes from <span class="citation"><a href="#ref-Cohen2013" role="doc-biblioref">B. H. Cohen</a> (<a href="#ref-Cohen2013" role="doc-biblioref">2013</a>)</span>.</p></li>
<li><p>Because degrees of freedom (df) are directly related to sample size, think of degrees of freedom as sample size. I think of the sum of squared deviations as adding up all of the variation that has been observed, and degrees of freedom as being the sample size. Therefore, I think of variance as being roughly the variation per unit, or the average variation. This is the least precise.</p></li>
</ol>
</div>
</div>
<div id="summary-5" class="section level2" number="8.16">
<h2><span class="header-section-number">8.16</span> Summary</h2>
<ul>
<li>Basic ideas in linear regression and how regression models are estimated (Sections <a href="regression.html#introregression">8.5</a> and <a href="regression.html#regressionestimation">8.6</a>).</li>
<li>Multiple linear regression (Section <a href="regression.html#multipleregression">8.7</a>).</li>
<li>Measuring the overall performance of a regression model using <span class="math inline">\(R^2\)</span> (Section <a href="regression.html#r2">8.8</a>)</li>
<li>Hypothesis tests for regression models (Section <a href="regression.html#regressiontests">8.9</a>)</li>
<li>Calculating confidence intervals for regression coefficients, and standardised coefficients (Section <a href="regression.html#regressioncoefs">8.11</a>)</li>
<li>The assumptions of regression (Section <a href="regression.html#regressionassumptions">8.12</a>) and how to check them (Section <a href="regression.html#regressiondiagnostics">8.13</a>)</li>
<li>Selecting a regression model (Section <a href="regression.html#modelselreg">8.14</a>)</li>
</ul>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Akaike1974" class="csl-entry">
Akaike, H. 1974. <span>“A New Look at the Statistical Model Identification.”</span> <em>IEEE Transactions on Automatic Control</em> 19: 716–23.
</div>
<div id="ref-Anscombe1973" class="csl-entry">
Anscombe, F. J. 1973. <span>“Graphs in Statistical Analysis.”</span> <em>American Statistician</em> 27: 17–21.
</div>
<div id="ref-Cohen2013" class="csl-entry">
Cohen, Barry H. 2013. <em>Explaining Psychological Statistics</em>. John Wiley; Sons.
</div>
<div id="ref-Cook1983" class="csl-entry">
Cook, R. D., and S. Weisberg. 1983. <span>“Diagnostics for Heteroscedasticity in Regression.”</span> <em>Biometrika</em> 70: 1–10.
</div>
<div id="ref-Fox2011" class="csl-entry">
Fox, J., and S. Weisberg. 2011. <em>An <span>R</span> Companion to Applied Regression</em>. 2nd ed. Los Angeles: Sage.
</div>
<div id="ref-Galton1886" class="csl-entry">
Galton, Francis. 1886. <span>“Regression Towards Mediocrity in Hereditary Stature.”</span> <a href="https://doi.org/10.2307/2841583">https://doi.org/10.2307/2841583</a>.
</div>
<div id="ref-Kozma1983" class="csl-entry">
Kozma, A., and M. J. Stones. 1983. <span>“Predictors of Happiness.”</span> <em>Journal of Gerentology</em> 38. <a href="https://doi.org/10.1093/geronj/38.5.626">https://doi.org/10.1093/geronj/38.5.626</a>.
</div>
<div id="ref-Long2000" class="csl-entry">
Long, J. S., and L. H. Ervin. 2000. <span>“Using Heteroscedasticity Consistent Standard Errors in Thee Linear Regression Model.”</span> <em>The American Statistician</em> 54: 217–24.
</div>
<div id="ref-Navarro2018" class="csl-entry">
Navarro, D. 2018. <em>Learning Statistics with r: A Tutorial for Psychology Students and Other Beginners (Version 0.6)</em>. <a href="https://learningstatisticswithr.com">https://learningstatisticswithr.com</a>.
</div>
<div id="ref-White1980" class="csl-entry">
White, H. 1980. <span>“A Heteroskedasticity-Consistent Covariance Matrix Estimator and a Direct Test for Heteroskedasticity.”</span> <em>Econometrika</em> 48: 817–38.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="111">
<li id="fn111"><p>Actually, even that table is more than I’d bother with. In practice most people pick <em>one</em> measure of central tendency, and <em>one</em> measure of variability only.<a href="regression.html#fnref111" class="footnote-back">↩︎</a></p></li>
<li id="fn112"><p>Just like we saw with the variance and the standard deviation, in practice we divide by <span class="math inline">\(N-1\)</span> rather than <span class="math inline">\(N\)</span>.<a href="regression.html#fnref112" class="footnote-back">↩︎</a></p></li>
<li id="fn113"><p>This is an oversimplification, but it’ll do for our purposes.<a href="regression.html#fnref113" class="footnote-back">↩︎</a></p></li>
<li id="fn114"><p>If you are reading this after having already completed Chapter <a href="hypothesistesting.html#hypothesistesting">5</a> you might be wondering about hypothesis tests for correlations. R has a function called <code>cor.test()</code> that runs a hypothesis test for a single correlation, and the <code>psych</code> package contains a version called <code>corr.test()</code> that can run tests for every correlation in a correlation matrix; hypothesis tests for correlations are discussed in more detail in Section <a href="regression.html#corrhyp">8.10</a>.<a href="regression.html#fnref114" class="footnote-back">↩︎</a></p></li>
<li id="fn115"><p>An alternative usage of <code>cor()</code> is to correlate one set of variables with another subset of variables. If <code>X</code> and <code>Y</code> are both data frames with the same number of rows, then <code>cor(x = X, y = Y)</code> will produce a correlation matrix that correlates all variables in <code>X</code> with all variables in <code>Y</code>.<a href="regression.html#fnref115" class="footnote-back">↩︎</a></p></li>
<li id="fn116"><p>It’s worth noting that, even though we have missing data for each of these variables, the output doesn’t contain any <code>NA</code> values. This is because, while <code>describe()</code> also has an <code>na.rm</code> argument, the default value for this function is <code>na.rm = TRUE</code>.<a href="regression.html#fnref116" class="footnote-back">↩︎</a></p></li>
<li id="fn117"><p>The technical term here is “missing completely at random” (often written MCAR for short). Makes sense, I suppose, but it does sound ungrammatical to me.<a href="regression.html#fnref117" class="footnote-back">↩︎</a></p></li>
<li id="fn118"><p>The <span class="math inline">\(\epsilon\)</span> symbol is the Greek letter epsilon. It’s traditional to use <span class="math inline">\(\epsilon_i\)</span> or <span class="math inline">\(e_i\)</span> to denote a residual.<a href="regression.html#fnref118" class="footnote-back">↩︎</a></p></li>
<li id="fn119"><p>Or at least, I’m assuming that it doesn’t help most people. But on the off chance that someone reading this is a proper kung fu master of linear algebra (and to be fair, I always have a few of these people in my intro stats class), it <em>will</em> help <em>you</em> to know that the solution to the estimation problem turns out to be <span class="math inline">\(\hat{b} = (X^TX)^{-1} X^T y\)</span>, where <span class="math inline">\(\hat{b}\)</span> is a vector containing the estimated regression coefficients, <span class="math inline">\(X\)</span> is the “design matrix” that contains the predictor variables (plus an additional column containing all ones; strictly <span class="math inline">\(X\)</span> is a matrix of the regressors, but I haven’t discussed the distinction yet), and <span class="math inline">\(y\)</span> is a vector containing the outcome variable. For everyone else, this isn’t exactly helpful, and can be downright scary. However, since quite a few things in linear regression can be written in linear algebra terms, you’ll see a bunch of footnotes like this one in this chapter. If you can follow the maths in them, great. If not, ignore it.<a href="regression.html#fnref119" class="footnote-back">↩︎</a></p></li>
<li id="fn120"><p>And by “sometimes” I mean “almost never.” In practice everyone just calls it “<span class="math inline">\(R\)</span>-squared.”<a href="regression.html#fnref120" class="footnote-back">↩︎</a></p></li>
<li id="fn121"><p>Note that, although R has done multiple tests here, it hasn’t done a Bonferroni correction or anything. These are standard one-sample <span class="math inline">\(t\)</span>-tests with a two-sided alternative. If you want to make corrections for multiple tests, you need to do that yourself.<a href="regression.html#fnref121" class="footnote-back">↩︎</a></p></li>
<li id="fn122"><p>You can change the kind of correction it applies by specifying the <code>p.adjust.method</code> argument.<a href="regression.html#fnref122" class="footnote-back">↩︎</a></p></li>
<li id="fn123"><p>Strictly, you standardise all the <em>regressors</em>: that is, every “thing” that has a regression coefficient associated with it in the model. For the regression models that I’ve talked about so far, each predictor variable maps onto exactly one regressor, and vice versa. However, that’s not actually true in general: we’ll see some examples of this in Chapter <a href="#anova2"><strong>??</strong></a>. But for now, we don’t need to care too much about this distinction.<a href="regression.html#fnref123" class="footnote-back">↩︎</a></p></li>
<li id="fn124"><p>Or have no hope, as the case may be.<a href="regression.html#fnref124" class="footnote-back">↩︎</a></p></li>
<li id="fn125"><p>Again, for the linear algebra fanatics: the “hat matrix” is defined to be that matrix <span class="math inline">\(H\)</span> that converts the vector of observed values <span class="math inline">\(y\)</span> into a vector of fitted values <span class="math inline">\(\hat{y}\)</span>, such that <span class="math inline">\(\hat{y} = H y\)</span>. The name comes from the fact that this is the matrix that “puts a hat on <span class="math inline">\(y\)</span>.” The hat <em>value</em> of the <span class="math inline">\(i\)</span>-th observation is the <span class="math inline">\(i\)</span>-th diagonal element of this matrix (so technically I should be writing it as <span class="math inline">\(h_{ii}\)</span> rather than <span class="math inline">\(h_{i}\)</span>). Oh, and in case you care, here’s how it’s calculated: <span class="math inline">\(H = X(X^TX)^{-1} X^T\)</span>. Pretty, isn’t it?<a href="regression.html#fnref125" class="footnote-back">↩︎</a></p></li>
<li id="fn126"><p>Though special mention should be made of the <code>influenceIndexPlot()</code> and <code>influencePlot()</code> functions in the <code>car</code> package. These produce somewhat more detailed pictures than the default plots that I’ve shown here. There’s also an <code>outlierTest()</code> function that tests to see if any of the Studentised residuals are significantly larger than would be expected by chance.<a href="regression.html#fnref126" class="footnote-back">↩︎</a></p></li>
<li id="fn127"><p>An alternative is to run a “robust regression”; I’ll discuss robust regression in a later version of this book.<a href="regression.html#fnref127" class="footnote-back">↩︎</a></p></li>
<li id="fn128"><p>And, if you take the time to check the <code>residualPlots()</code> for <code>regression.1</code>, it’s pretty clear that this isn’t some wacky distortion being caused by the fact that <code>baby.sleep</code> is a useless predictor variable. It’s an actual nonlinearity in the relationship between <code>dan.sleep</code> and <code>dan.grump</code>.<a href="regression.html#fnref128" class="footnote-back">↩︎</a></p></li>
<li id="fn129"><p>Note that the underlying mechanics of the test aren’t the same as the ones I’ve described for regressions; the goodness of fit is assessed using what’s known as a score-test not an <span class="math inline">\(F\)</span>-test, and the test statistic is (approximately) <span class="math inline">\(\chi^2\)</span> distributed if there’s no relationship<a href="regression.html#fnref129" class="footnote-back">↩︎</a></p></li>
<li id="fn130"><p>Again, a footnote that should be read only by the two readers of this book that love linear algebra (mmmm… I love the smell of matrix computations in the morning; smells like… nerd). In these estimators, the covariance matrix for <span class="math inline">\(b\)</span> is given by <span class="math inline">\((X^T X)^{-1}\  X^T \Sigma X \ (X^T X)^{-1}\)</span>. See, it’s a “sandwich?” Assuming you think that <span class="math inline">\((X^T X)^{-1} = \mbox{&quot;bread&quot;}\)</span> and <span class="math inline">\(X^T \Sigma X = \mbox{&quot;filling&quot;}\)</span>, that is. Which of course everyone does, right? In any case, the usual estimator is what you get when you set <span class="math inline">\(\Sigma = \hat\sigma^2 I\)</span>. The corrected version that I learned originally uses <span class="math inline">\(\Sigma = \mbox{diag}(\epsilon_i^2)\)</span> <span class="citation">(<a href="#ref-White1980" role="doc-biblioref">White 1980</a>)</span>. However, the version that <span class="citation"><a href="#ref-Fox2011" role="doc-biblioref">Fox and Weisberg</a> (<a href="#ref-Fox2011" role="doc-biblioref">2011</a>)</span><a href="regression.html#fnref130" class="footnote-back">↩︎</a></p></li>
<li id="fn131"><p>Note, however, that the <code>step()</code> function computes the full version of AIC, including the irrelevant constants that I’ve dropped here. As a consequence this equation won’t correctly describe the AIC values that you see in the outputs here. However, if you calculate the AIC values using my formula for two different regression models and take the difference between them, this will be the same as the differences between AIC values that <code>step()</code> reports. In practice, this is all you care about: the actual value of an AIC statistic isn’t very informative, but the differences between two AIC values <em>are</em> useful, since these provide a measure of the extent to which one model outperforms another.<a href="regression.html#fnref131" class="footnote-back">↩︎</a></p></li>
<li id="fn132"><p>While I’m on this topic I should point out that there is also a function called <code>BIC()</code> which computes the Bayesian information criterion (BIC) for the models. So you could type <code>BIC(M0,M1)</code> and get a very similar output. In fact, while I’m not particularly impressed with either AIC or BIC as model selection methods, if you do find yourself using one of these two, the empirical evidence suggests that BIC is the better criterion of the two. In most simulation studies that I’ve seen, BIC does a much better job of selecting the correct model.<a href="regression.html#fnref132" class="footnote-back">↩︎</a></p></li>
<li id="fn133"><p>It’s worth noting in passing that this same <span class="math inline">\(F\)</span> statistic can be used to test a much broader range of hypotheses than those that I’m mentioning here. Very briefly: notice that the nested model M0 corresponds to the full model M1 when we constrain some of the regression coefficients to zero. It is sometimes useful to construct submodels by placing other kinds of constraints on the regression coefficients. For instance, maybe two different coefficients might have to sum to zero, or something like that. You can construct hypothesis tests for those kind of constraints too, but it is somewhat more complicated and the sampling distribution for <span class="math inline">\(F\)</span> can end up being something known as the non-central <span class="math inline">\(F\)</span> distribution, which is waaaaay beyond the scope of this book! All I want to do is alert you to this possibility.<a href="regression.html#fnref133" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="data-cleaning-and-missing-values-analysis.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ttest.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/08-glm.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["schuster-statistics-remix.pdf", "schuster-statistics-remix.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
