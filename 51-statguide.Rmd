## One-Sample _t_-Test

### Definition

The one-sample t-test tests the null hypothesis that a mean is equivalent to the mean of a known population. Unlike the $z$-test, the $t$-test uses the sample deviation as an estimator for the population standard deviation.

### Test Statistic

The test statistic is $t$, which measures the distance between two means. In this case, one mean is from our sample and the other mean is a known constant. 

${t}=\frac{\bar{X}-\mu_0}{\frac{s}{\sqrt{n}}}$

$\sigma_{\bar{X}}=\frac{\sigma}{\sqrt{n}}$ (standard error of the mean)

Confidence interval (results in a lower and upper bound): ${CI}=\bar{X}\pm t_{critical} * \sigma_{\bar{X}}$

Note that the confidence interval is given as a minimum and maximum value.

$df=n-1$

```{r onetnull, fig.cap="The null hypothesis distribution of $t$ with values of df between 2 and 10. Notice how the curve is starting to converge at the higher values of df.", echo=FALSE}
curve(dt(x, df=10), from=-4, to=4)
curve(dt(x, df=2), from=-4, to=4, add=TRUE)
curve(dt(x, df=4), from=-4, to=4, add=TRUE)
curve(dt(x, df=6), from=-4, to=4, add=TRUE)
curve(dt(x, df=8), from=-4, to=4, add=TRUE)

```

The $t$ distribution is actually a family of distributions defined by degrees of freedom. **Degrees of freedom** is a concept that can be interpreted multiple ways. For now, it is sufficient to say that it is based on sample size. The value of degrees of freedom grows by 1 with each additional unit increased in the sample. In other words, the specific sampling distribution used in the hypothesis test depends on the sample size (and degrees of freedom).



### Assumptions & Required Data

- 1 variable measured using a quantitative, continuous scale
- The variable was measured for a sample that was taken randomly, with replacement, from a population
- The normality assumption, meaning at least one of these:
  - $N \ge 30$
  - The variable is normally distributed in the population
- The population mean, $\mu$, is known.
- The population standard deviation, $\sigma$, is estimated from the sample standard deviation, $s$.

### When to use it

Use a $t$-test when you are comparing a single sample mean to a known population parameter and can meet the assumptions. This is nearly every situation in which you would use a $z$-test. In fact, as sample size surpasses 30, the $t$ and $z$ distributions converge.

### Example

Imagine a high school has a graduation test with $M = .80$ with a standard deviation ($\sigma$) of $\sigma = .10$. A random sample of $N = 35$ students at the high school participate in an after-school program aimed at increasing performance on the graduation test. 

#### Data

The data are test scores from 35 students.

````{r onetdata, echo=FALSE}
sample <-  round(rnorm(35, 0.89, 0.10), 2) # Generate a random, sample for this exercise
# cat(paste(sample), sep="\n")
sample
mean(sample)
````

The students in the program took the test and performed higher than the population average ($M=$```` mean(sample) ````). Is there evidence that the after school program is effective?

#### Hypotheses

Because researchers are interested in detecting higher performance on the test, a one-tailed test is used to increase statistical power. If, instead, researchers wanted to see if the sample had higher or lower performance, a two-tailed test should be used.



$$
\begin{array}{ll}
H_0 = \mu \le .80  \\
H_a = \mu > .80
\end{array}
$$


#### Analysis

Set the alpha level. By convention, an alpha level of $\alpha = .05$ will be used. 

Assume the null hypothesis is true. Assuming the null hypothesis is true means that we need to determine the probability of obtaining a sample mean this distance from the population mean. We will determine this using the sampling distribution of the null hypothesis for $t$.


````{r tanalysis, echo=TRUE}

t.test(sample, mu = .8, alternative = "greater") # two-sided, greater, or less

````

Note that in the syntax for the t-test, you can specify an alternative hypothesis of "two-sided" (two-tailed), "greater," or "less."

#### Effect Size
Cohen’s d is a measure of effect size for a t-test. 

$d=(\mu- \mu_0)/s$

#### Decision

Because $p < \alpha$, you reject the null hypothesis and concluded that there was a difference between your sample mean and the population.

#### Variations

- This was a one-tailed test on the right side of the distribution. The use of ````alternative = "greater"```` would need to be adapted if the one-tailed test was on the left side of the distribution (to detect if scores were lower than the population). Simply change ````alternative = "greater"```` to have ````"less"```` calculate from the left side (lower tail). 

- In a two-tailed test, the shading would need to be repeated on the left side, and the shaded area on both sides would need to be added together. Simply change ````alternative = "greater"```` to have ````"two-sided"````. If you omit the ````alternative```` argument completely, the default is a two-tailed test. 

- If $p>{\alpha}$, the null hypothesis would have been retained and the results inconclusive.

## Paired Samples T-Test

### Definition

A paired samples t-test measures the difference between two means collected from a within-subjects (also called repeated measures) design. In a within-subjects design, each unit is measured exactly twice. A longitudinal study that measured participants at two points in time could be analyzed with a paired-samples t-test. The hypothesis test for this t-test tests the null hypothesis that there is no difference between the two measurements. 

### Test Statistic

The test statistic is $t$, which measures the difference between the two measurements. 

$$
t = \frac{\bar{D}}{\hat\sigma_D / \sqrt{N}}
$$
```{r pairedtnull, fig.cap="The null hypothesis distribution of $t$ with values of df between 2 and 10. Notice how the curve is starting to converge at the higher values of df.", echo=FALSE}
curve(dt(x, df=10), from=-4, to=4)
curve(dt(x, df=2), from=-4, to=4, add=TRUE)
curve(dt(x, df=4), from=-4, to=4, add=TRUE)
curve(dt(x, df=6), from=-4, to=4, add=TRUE)
curve(dt(x, df=8), from=-4, to=4, add=TRUE)

```

The $t$ distribution is actually a family of distributions defined by degrees of freedom. **Degrees of freedom** is a concept that can be interpreted multiple ways. For now, it is sufficient to say that it is based on sample size. The value of degrees of freedom grows by 1 with each additional unit increased in the sample. In other words, the specific sampling distribution used in the hypothesis test depends on the sample size (and degrees of freedom).

### Assumptions & Required Data

- One variable measured twice, stored in a dataframe as two quantitative variables, each one reflecting one measurement (e.g., time1 and time2).
- The normality assumption, meaning at least one of these:
  - $N \ge 30$
  - The variable is normally distributed in the population
- Independence of observations. Participants do not affect each others' scores.
- Homogeneity of variance (aka "homoscedasticity"). Population standard deviation is equivalent across the conditions.

### When to use it

Use a paired-samples $t$-test when you are comparing exactly two related observations or have a within-subjects design with exactly two levels of the independent variable.

### Example


#### Data

Did students' test scores significantly differ between exam 1 and exam 2?

````{r pairedtdata, echo=FALSE}
exam1 <-  round(rnorm(35, 0.89, 0.10), 2) # Generate a random, sample for this exercise
exam2 <-  round(rnorm(35, 0.93, 0.10), 2) # Generate a random, sample for this exercise
exams <- data.frame(exam1, exam2)
# cat(paste(sample), sep="\n")
exams
````

#### Hypotheses

$t$-tests can be one-tailed or two-tailed. A one-tailed $t$-test could be conducted if researchers predict a direction for the effect.

$$
\begin{array}{ll}
H_0: & \mu_D = 0  \\
H_1: & \mu_D \neq 0
\end{array}
$$

#### Analysis


Set the alpha level. By convention, an alpha level of $\alpha = .05$ will be used. 

Assume the null hypothesis is true. Assuming the null hypothesis is true means that we need to determine the probability of obtaining a sample mean this distance from the population mean. We will determine this using the sampling distribution of the null hypothesis for $t$.

We need to take our wide format data and make it long format data.

````{r pairedtanalysis, echo=TRUE}
# exams <- wideToLong(exams, within="time")
# exams <- sortFrame(exams, id)
head(exams)
````

Then, we can run the $t$-test:

````{r pairedtanalysis2, echo=TRUE}
t.test(x = exams$exam1, y = exams$exam2, paired = TRUE)

````

#### Decision

The $t$-test was not significant, $p$ = 0.6589, t(34) = 0.44. The results are inconclusive.

#### Effect Size

Paired samples $t$-tests have their own version of Cohen's $d$ that is called $d_z$. The calculation is different, so it is important to label it as $d_z$.

$d_z = \frac{\mbox{mean difference}}{\mbox{standard deviation}}$

$d = d_z * \sqrt{2}$

#### Power Analysis

In G*Power, select "t-tests" and then "Means: Difference between two dependent means (matched pairs)." Compute the required sample size by entering the effect size ($d_z$), tail(s), alpha, and desired power. Clicking "Determine" will give you a variety of ways to estimate $d_z$.

## Independent Samples $t$-Test

This section is still under construction. For now, please refer to the regular chapter on $t$-tests.

### Definition

An independent samples $t$-test measures the difference between two means collected from a **between-subjects design**. In a between-subjects design, the two samples are compared, often from two levels of a manipulation. The key difference from the within-subjects design is that the units (usually, that means participants) in a between-subjects design are independent (different people) and only measured once. When different groups in a study contain different units, a between-subjects design is being used.


### Test Statistic


```{r indtnull, fig.cap="The null hypothesis distribution of $t$ with values of df between 2 and 10. Notice how the curve is starting to converge at the higher values of df.", echo=FALSE}
curve(dt(x, df=10), from=-4, to=4)
curve(dt(x, df=2), from=-4, to=4, add=TRUE)
curve(dt(x, df=4), from=-4, to=4, add=TRUE)
curve(dt(x, df=6), from=-4, to=4, add=TRUE)
curve(dt(x, df=8), from=-4, to=4, add=TRUE)

```

The $t$ distribution is actually a family of distributions defined by degrees of freedom. **Degrees of freedom** is a concept that can be interpreted multiple ways. For now, it is sufficient to say that it is based on sample size. The value of degrees of freedom grows by 1 with each additional unit increased in the sample. In other words, the specific sampling distribution used in the hypothesis test depends on the sample size (and degrees of freedom).


### Assumptions & Required Data

- Independence of observations 
– Normality as previously discussed; violations are a bigger problem if you also violate homogeneity of variance or have unequal sample sizes; the result is lower power
- Homogeneity of variance aka equality of variances aka homoscedasticity: Each group has same variance. This can be tested with Levene's test.

### When to use it


### Example


#### Data


````{r indtdata, echo=FALSE}

````

#### Hypotheses


#### Analysis

Set the alpha level. By convention, an alpha level of $\alpha = .05$ will be used. 

Assume the null hypothesis is true. Assuming the null hypothesis is true means that we need to determine the probability of obtaining an effect size (in this case, the predictive power of our model over the null model with only the mean, measured using $R^2$) this strong at our sample size through random sampling from a population with no effect (a null model). We will determine this using an analysis of variance using the sampling distribution of the null hypothesis for $F$.

````{r indtanalysis, echo=TRUE}

````


#### Decision

#### Effect Size

d using weighted (allows unequal sample size) and pooled (requires equal variance) standard deviation: different group sizes but same SDs - weighted average of sample variances. Same two variances but different means

#### Power Analysis



## One-Way ANOVA

### Definition

The analysis of variance (ANOVA) is an expansion of t-test to allow comparison of a categorical IV with more than two levels.

As with the other tests of group comparisons, ANOVA requires a continuous, quantitative DV variable.

ANOVA has two stages. The **omnibus test** tests the null that two or more groups have the same mean. If the omnibus test is significant, **pairwise comparisons** are the second step. There are two strategies for pairwise comparisons. **Planned comparisons** maximizes statistical power by only testing hypothesized pairwise comparisons. **Post-hoc tests** run statistical tests for every possible pairwise comparison and then makes some (or no) adjustment for the number of tests that have been run.

ANOVA solves the problem of inflated type I error that arises when running multiple t-tests. Every t-test has a **testwise error rate** equal to alpha. In other words, if we assume the null is true, what is the probability of rejecting the null on one t-test? It is alpha. If we assume the null is true, what is the probability of rejecting the null on *at least one of a series* of t-tests? It is higher than alpha. ANOVA solves the problem of a **family** or **experimentwise error rate** (the overall alpha level for a series of tests) by keeping the experimentwise error rate equal to .05. If you are curious, the experimentwise error rate is equal to:

$EER = 1-(1-\alpha)^m$ where m is the number of tests.

$m = \frac{k(k-1)}{2}$ where k is the number of levels of the IV.


### Test Statistic

The test statistic for ANOVA is $F$, which is equivalent to $t^2$. Where $t$-tests measure variability using standard deviation/standard error, $F$-tests measure variability using variance.

F is a ratio of two kinds of variance, explained and unexplained. The variance formula could be described as the sum of squared mean differences (sum of squares) over degrees of freedom (df). Another name for a variance calculation is mean square. The explained variance is called **mean square between-groups**, and the unexplained variance is **mean square within-groups**. Because between and within are used to describe variance in ANOVA, it is helpful to memorize that between-groups variance is good/explained and within-groups variance is bad/unexplained. The denominator/within-groups variance is pooled variance; it is just the average variance of the groups. The variance of the group means is the numerator/between-groups variance. Think of the F ratio as a competition between these two kinds of variance.

$F_{observed} = \frac{MS_{between}}{MS_{within}}$

If the null is true, and all group means are the same, then the value of the F ratio will be near 1. As group differences increase, the value of F will increase.

$F$ is a one-tailed distribution, meaning that the omnibus F test does not include direction (i.e., all ANOVA tests are two-tailed). Notice how the ratio of two kinds of variance can never be negative.

```{r anovanull, fig.cap="The null hypothesis distribution of $F$ with values of df between F(4, 2) and F(10, 2).", echo=FALSE}
curve(df(x, df1 =10, df2=4), from=-0.5, to=4)
curve(df(x, df1=2, df2=4), from=-0.5, to=4, add=TRUE)
curve(df(x, df1 =4, df2=4), from=-0.5, to=4, add=TRUE)
curve(df(x, df1=6, df2=4), from=-0.5, to=4, add=TRUE)
curve(df(x, df1=8, df2=4), from=-0.5, to=4, add=TRUE)

```

Pairwise comparisons, whether planned or as post-hoc tests, are essentially just $t$-tests.

#### Repeated Measures ANOVA

Repeated measures ANOVA is an extension of the paired samples $t$-test to allow for within-subjects designs. 

In a repeated measures ANOVA, variance due to individual differences is computed separately (i.e., how different were participants scores from each other?). The individual differences variance is subtracted from the error term. In other words, variability across different participants is removed from the denominator, thus raising the F value Therefore, when you observe large differences across participants, power is increased relative to the between-subjects version. 

Practically, repeated measures ANOVA (and within-subjects design) requires fewer cases, but more time per case. You need fewer people, but more observations of them. Within subjects designs also are subject to:

- Simple order effects. Which condition is first affects participants scores; practice effects are an example. The solution is counterbalancing.

- Differential carryover effects are when the effect of one condition bleeds into the second, and the effect is asymmetrical because it happens more in one order of conditions than the other. Imagine a study of shocking images versus neutral images (@Cohen2013). Participants may be more affected by the shocking images and that could alter their scores in the neutral image trial. In this situation, counterbalancing will not help.


### Assumptions & Required Data

#### Between-Subjects ANOVA

-	Independence of observations. Participants do not affect the measurement of each other. For example, if you conduct a study on workplace satisfaction with some participants from the same office, independence of observations is a vioalted assumption. Other techniques, such as hierarchical linear modeling (a series of regression equations) would be more appropriate. Aside from these types of situations, it is relatively easy to meet this assumption through careful research design.
- The normality assumption, meaning at least one of these:
  - $N \ge 30$
  - The variable is normally distributed in the population
- Independence of observations. Participants do not affect each others' scores.
- Homogeneity of variance (aka "homoscedasticity"). Population standard deviation is equivalent across the conditions. This can be tested using Levene's test. If this assumption is violated *and* group sizes are unequal, then a parametric ANOVA may not be appropriate. Alternatives for this case are Welch or Brown-Forsythe ANOVA. The syntax for Welch is ````oneway.test(dataframe$dv ~ dataframe$iv, data = ex, na.action = na.exclude)````.
- No outliers. Note our earlier guidance on balancing the impact of outliers with the need to maintain the sampling method.

#### Repeated Measures ANOVA

- Random sampling. Although this is always a concern for external validity, it is more important for a repated measures ANOVA because each unit is in both conditions. Thus, units act as their own control group. 
- The normality assumption, meaning at least one of these:
  - $N \ge 30$
  - The variable is normally distributed in the population
- No outliers. Note our earlier guidance on balancing the impact of outliers with the need to maintain the sampling method.
- No subject x treatment interaction. This occurs when some participants respond differently to a treatment. Imagine a drug trial for people at various stages of an illness. If the pill is more effective at earlier stages and not at later stages, there will be an inflated Type II error rate. Repeated measures ANOVA assumes this is does not happen.
- Sphericity. 
  - It only applies when 3+ levels ($k>2$). 
  - Imagine you found the difference between a pair of levels (e.g., time1 - time 2) and then found the variance of those difference scores. If you did that for every pair of two levels, you would want each variance calculation to be equal.
  - In case you are ever asked, sphericity guarantees no compound symmetry (homogeneity of variance and of covariance) and no additivity (a treatment x subject interaction).
  - **Sphericity is an assumption you need to care about, because violating it can inflate the F-ratio and Type I error rate**.
  - Use Wauchly's $W$ to test for sphericity. It gives a value of epsilon. 1.0 is perfect sphericity. If the significance test for $W$ is not significant, you conclude you have met the assumption. However, this test is not powerful when sample size is small, so it can be helpful to also look at the value of epsilon.
  - If sphericity is violated, you need to use an alternative statistic. Greenhouse Geisser is more common but conservative; Huynh and Feldt is more powerful when you are close to sphericity.
  - Do not try different statistics and only test for sphericity after you have failed to reject the null. That is p-hacking.


### When to use it

If you have a quantitative DV and a single, discrete IV with two or more levels, you can run an ANOVA.

### Example 1: Between-Subjects

Researchers are interested in participants' reaction times to three types of alert displays: a visual alert, an auditory alert, and a speech alert. They randomly assign participants to one of these three conditions. Therefore, this is a between-subjects experimental design.

#### Data

Complicating matters, our data in this example are starting off in the wrong format. This format is called wide. We have one column per condition. This means that each row contains the data for three participants. We need this reshaped into one row per participant, called long.

````{r anovadata1, echo=TRUE}
RT_visual <- c(218.24, 218.34, 229.05, 219.12, 218.42)
RT_auditory <- c(238.37, 249.06, 238.86, 248.76, 239.34)
RT_speech <- c(258.34, 228.62, 239.31, 259.14, 258.13)
id <- c(1:5)
dataframe <- data.frame(RT_visual, RT_auditory, RT_speech, id)
dataframe
````

These data are in wide format (one column per condition). To use these data in a between-subjects ANOVA, we need them in long format (one row per participant). 

The secret to this is naming the variables correctly. First, you need an id variable. Second, you should name all of your condition variables in this format: DV_level. In this example, our DV is reaction time (RT) and the levels are visual, auditory, and speech. ````wideToLong()```` is looking for this format or it won't work. 

````{r anovavis1, echo=TRUE}
library(lsr)
longdata <- wideToLong(dataframe, "condition")
longdata
````
All of this was to illustrate that for a between-subjects ANOVA, we need long format data. To avoid having to reshape your data, it is helpful to collect between-subjects data already in long format. The condition variable is a hint that the data are in long format.

With that out of the way, we can now use our dataframe, ````longdata````. First, we will descriptively look at the group means:

````{r anovavis2, echo=TRUE}
library(pastecs)
by(longdata$RT, longdata$condition, stat.desc) # show descriptive stats on our RT DV grouped by condition
````

````{r anovavis3, echo=TRUE}
library(ggplot2)
ggplot(longdata, aes(condition, RT)) + stat_summary(fun.y = mean, geom = "point") + labs(x ="Condition", y="Outcome") + stat_summary(fun.data = mean_cl_normal, geom = "errorbar", width= 0.2) + labs(x ="Condition", y="Outcome")
````

And, we can create an interactive graph of the means using a package called Plotly. The error bars are 95% confidence intervals. I cannot embed Plotly in this book, but I can show you the syntax:

````{r anovavis5, echo=TRUE}
# install.packages("plotly")
# library(plotly)
# ggplotly(ggplot(longdata, aes(condition, RT)) + stat_summary(fun.y = mean, geom = "point") + labs(x ="Condition", y="Outcome") + stat_summary(fun.data = mean_cl_normal, geom = "errorbar", width= 0.2) + labs(x ="Condition", y="Outcome")) # Plotly shows hoverable data points on mouseover
````

A visual interpretation of this graph suggests that visual had the fastest reaction time, followed by auditory, and then speech. But are these significantly different from each other? We need to run the ANOVA to find out.

#### Hypotheses

$$
\begin{array}{ll}
H_0: & \mu_{auditory} = \mu_{speech} = \mu_{visual}  \\
H_1: & \mbox{the null is false}
\end{array}
$$
#### Assumption Checking

````{r anovaassumption, echo=TRUE}
library(car)
leveneTest(longdata$RT, longdata$condition, center = median)
````

Levene's test is not significant, so we assume homogeneity of variance. However, I would be cautious about this interpretation because our sample size is small ($N = 15$). Further, the error bars in the graph looked different to me (and the confidence intervals were numerically different in the table). But because our sample sizes are equal, we will continue with this example.

#### Analysis

Set the alpha level. By convention, an alpha level of $\alpha = .05$ will be used. 

Assume the null hypothesis is true. Assuming the null hypothesis is true means that we need to determine the probability of obtaining an effect size (in this case, the ratio of explained to unexplained variance) this strong at our sample size through random sampling from a population with no effect (a null model). We will determine this using an analysis of variance using the sampling distribution of the null hypothesis for $F$.

````{r anovaanalysis, echo=TRUE}

anovaModel <- lm(RT ~ condition, data = longdata) # note there is an na.action argument if you need to exclude missing data
summary(anovaModel)

anovaModel <- aov(RT ~ condition, data = longdata)
summary(anovaModel)

````


#### Omnibus Decision

The omnibus ANOVA was significant, $F$(2, 12) = 13.26, $p$ < .001, $\eta^2$ = .689. We can conclude there is some difference among these conditions. To determine which conditions are significantly different from the others, we need to perform post-hoc tests.

#### Multiple Comparisons

A significant $F$-test only tells us that at least one group is significantly different. Which groups are different from which groups? What are the directions of the effects (i.e., which mean is larger, and which is smaller)? This is purpose of multiple comparisons; we try to identify which mean differences reflect true effects.

There are multiple ways to do this; they differ in balance of Type I error control and power, but fundamentally are $t$-tests (as they always compare two means). You’ll need to consider multiple comparisons whenever you have more than one comparison of means. You do not need multiple comparisons with only two levels of the IV, as there is only a single comparison to make, and that one comparison is accomplished with the onmibus test.

There are two approaches to multiple comparisons:

1. Post-hoc tests (a posteriori): Post-hoc tests are **protected** in that they are only run after a significant $F$ test. All are variations on a t-test with different adjustments to protect against the multiple t-test problem. Post-hoc tests are used whenever researchers want to examine all possible mean comparisons.

2. Planned comparisons (a priori): In a planned comparisons approach, the omnibus ANOVA can be skipped (although it usually is run anyway), and only test a subset of all possible comparisons are tested. Because you examine fewer comparisons, and your comparisons are not at all influenced by the data, you will examine fewer means and have the most power. For this to be meaningful, the researchers must declare their tested comparisons ahead of time and not alter the approach if undeclared comparisons turn out to be significant. Readers must either trust that researchers have done this faithfully, or preregistration would need to be used.

#### Post-Hoc Tests 

Let's explore the post-hoc test options, starting with the most **liberal** (least control of Type I error to maximize power) and ending with the most **conservative** (most control of Type I error at the cost of power).

##### Fisher's Least Significant Difference (LSD)

Fisher’s protected $t$ Tests is also known as Fisher’s Least Significant Difference (LSD). This involves running follow up t-tests using the $MS_wn$ as a variance estimate. Essentially, you are running protected $t$-tests. The problem of inflated Type I error exists when your omnibus $F$ is significant and there are many groups to compare (i.e., more than 3). It is the most liberal approach but maintains the most statistical power with 3 or fewer groups.

My recommendation is to use this approach if you have three or fewer groups.


##### Tukey method

**Tukey's Honestly Significant Difference (HSD) test** or Tukey's range test is a method that compares the size of mean differences on the logic that the largest mean differences are real effects. Tukey's HSD is still just a t-test, but it has a correction for familywise error rate. Instead of $t$, a similar statistic $q_k$ (called the studentized range statistic) is computed, and a significance test is done using the $q_k$ distribution. The $q_k$ distribution depends on degrees of freedom (just like the t-test) but also incorporates $k$, the number of groups that are being compared.

In all, Tukey gives you comparisons at a familywise error rate of .05. However, because you are peanlized for the number of comparisons you make, it can end up being too conservative, especially with many groups.

My recommendation is to use this approach for post-hoc testsing in between-subjects designs with more than 3 groups.

Tukey also has the homogeneity of variance assumption, so if that is violated and you have unequal group sizes, you may need to use an alternative statistic, such as Games-Howell or Fisher-Hayter. The larger point is that this list is not every possible approach to post-hoc testing. You can employ any post-hoc testing method you wish so long as you decide on your approach before starting your analysis and can make an argument for why you chose that particular method.

````{r anovatukey, echo=TRUE}
library(multcomp)
# install.packages("multcomp")

hsdModel <- glht(anovaModel, linfct = mcp(condition = "Tukey"))
summary(hsdModel)
````

The Tukey post-hoc tests show that there was a significant difference between visual ($M$ = 220.63, $SD$ = 4.72) and auditory ($M$ = 242.87, $SD$ = 5.52) alerts, and between speech ($M$ = 248.71, $SD$ = 13.98) and visual alerts, but not between auditory and speech alerts.

##### Bonferroni method

This is the most conservative method of post-hoc tests. To keep the familywise error rate equal to alpha, we just divide alpha by the number of comparisons. If you make two comparisons, your new alpha level is $\alpha = .025$. As you may imagine, this becomes *very* conservative as you increase the number of comparisons you need to make. But, in cases where you want the best control of Type I error, are not concerned about the loss of power, and/or are making this adjustment on a small number of tests, it can be simple to implement and useful. In statistical software, Bonferroni corrections are usually implemented by altering the $p$-value, not alpha, so you can still interpret the output with an alpha level of $\alpha = .05$.

````{r anovabonferroni, echo=TRUE}
pairwise.t.test(longdata$RT, longdata$condition, paired = FALSE, p.adjust.method = "bonferroni")
````

The bonferroni post-hoc tests show that there was a significant difference between visual ($M$ = 220.63, $SD$ = 4.72) and auditory ($M$ = 242.87, $SD$ = 5.52) alerts, and between speech ($M$ = 248.71, $SD$ = 13.98) and visual alerts, but not between auditory and speech alerts.

##### Alternatives to Bonferroni method

Instead of "bonferroni", you can choose another p.adjust.method argument of "holm”, “hochberg”, “hommel”, “BH”, “BY”, “fdr”, or “none”. The [R documentation](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/p.adjust) gives a nice summary of the choices and provides citations for further reading.

#### Planned Comparisons

As you have more groups in your study, you start to lose power by testing every possible combination. Sometimes, you are only interested in one or a few of the contrasts, or you might want to compare groups of means. For example, if you measure people's mood in each month of the year (with 12 levels), you might want to compare spring measurements to fall measurements. Planned comparisons gives you more flexibility in the means you can test without a loss of statistical power for making all of the comparisons. There are caveats, though:

1. You need to specify the comparisons ahead of time. Your readers and reviewers need to be satisfied that you did specify the comparisons ahead of time.

2. You need to be interested in a subset of all possible comparisons. There is little point in a planned comparisons approach that tests every possible comparison.

3. You have to program the comparisons in R using **user-defined contrast coding**. Dummy coding is an example of contrast coding. User-defined contrast coding means you define each comparison using numerical codes. I will try to explain this briefly here, but we will give this topic more attention when we do factorial ANOVA. For now, just be aware that planned comparisons require contrast coding.

   Imagine we have four levels of our IV: A, B, C, and D. We are interested a planned comparison of A vs B and C vs D. We give each group a coefficient (a weight) that defines if it is used in the comparison. We use a 0 to exclude the level from the comparison. We use a positive number to put it on one side of the comparison and a negative number to put it on the other side. An example may help:

----------------------------
Comparison    A   B   C   D
------------ --- --- --- ---
A vs B       1   -1  0   0

C vs D       0   0   1   -1
----------------------------

   The table above defines two comparisons, one between A and B and a seond one between C and D.

   There is one rule about writing these comparisons; they must be **orthogonal** (independent). You can verify that your comparisons are orthogonal by multiplying the coefficients *down*. That is, muliply the first A coefficient (1) by the second A coefficient (0). Then, add the products. Based on the table, that is $1 * 0 + -1 * 0 + 0 * 1 + 0 * -1$, which gives a total of 0. The total should be zero for the comparisons to be orthogonal. If you have three coomparisons, you have to do this multiplication for every pair. Yes, it's a bit of work, but this is the planning we sign up for when we do planned comparisons.

4. The convention seems to be to ignore familywise error rate, which is only appropriate if you have a small number of planned comparisons. You may consider one of the adjustment methods, like Bonferroni, if you have many planned comparisons to test.

#### Effect Size

Because $\eta^2 = R^2$, you can find and report $\eta^2$ from the ````lm()```` function.


#### Power Analysis

In G*Power, select the F tests family, and then choose "ANOVA: Fixed effects, omnibus, one-way."

The best way to estimate effect size is to rely on effect sizes in similar past research. You will commonly find eta squared ($\eta^2$) in published work, not effect size $f$. However, you can convert eta squared to $f$ to use in G*Power:

1. Click "Determine."
2. Select the procedure of "Effect size from variance."
3. Select "Direct."
4. Enter a value for partial eta squared.
5. Click "Calculate and transfer to main window." **Don't miss this step**. It is easy to forget to update the value of f in the input parameters. If you do not update that value, your power analysis will not use this new effect size.

One more (less common) option is to select "Effect size from means" in the "Determine" drawer to fill in the anticipated means to estimate effect size.





### Example 2: Within-Subjects

This example is the same, but in this study, the researchers randomly assigned participants to one of these three condition orders. Participants completed a visual, auditory, and speech trial. Therefore, this is a within-subjects experimental design.

#### Data

If you run this analysis in SPSS or another package, it is more common to see it in wide format, with one row per participant, and each measurement represented by a separate column. That is how our data in this example started out:

````{r anovadata2, echo=TRUE}
RT_visual <- c(218.24, 218.34, 229.05, 219.12, 218.42)
RT_auditory <- c(238.37, 249.06, 238.86, 248.76, 239.34)
RT_speech <- c(258.34, 228.62, 239.31, 259.14, 258.13)
id <- c(1:5)
dataframe <- data.frame(RT_visual, RT_auditory, RT_speech, id)
dataframe
````

These data are in wide format (one column per condition) in R, however, we still have to use the long format (one row per participant). 

````{r anovavis4, echo=TRUE}
library(lsr)
longdata <- wideToLong(dataframe, "condition")
longdata
````

I will omit the parts of the analysis that are the same in the within-subjects design. All the descriptive analysis is the same.

#### Hypotheses

Hypotheses are written the same as for a between-subjects design.


#### Analysis

We are using ezANOVA() in the ez package.

````{r anovaanalysis2, echo=TRUE}

# install.packages("ez") # you may need to install the ez package
library(ez)
withinAnovaModel <- ezANOVA(data = longdata, dv = .(RT), wid = .(id), within = .(condition), type = 3)
withinAnovaModel
````


#### Sphericity Checking

We will need to diagnose sphericity. Fortunately, this is provided in the default output. The value of Mauchly's $W$ in this example is 0.59, which is not significant ($p$ > .05), and I WILL report the statistics in the first table. Had this been significant, I could have reported Greenhouse and Geisser (GG; 1959) or Huynh and Feldt (HF; 1976) versions of the statistics. Remember, it is important to decide which approach you will take before examining the p-values. 

#### Omnibus Decision

The omnibus ANOVA was significant, $F$(2, 8) = 10.74, $p$ = .005, $\eta^2_G$ = .689. We can conclude there is some difference among these conditions. To determine which conditions are significantly different from the others, we need to perform post-hoc tests.


#### Post-hoc Tests

The post-hoc test methods are the same, except set paired = TRUE, so that paired samples t-tests are used.

Tukey's HSD assumes independence of observations and is not available for within-subjects designs.

#### Effect Size

The effect size statistic is called generalized eta squared (see also [Bakeman, 2005](https://link.springer.com/content/pdf/10.3758%2FBF03192707.pdf)). Note that this differs in its calculation from eta squared, so you should take care to label it with the $_G$.

#### Power Analysis

In G*Power, select the F tests family, and then choose "ANOVA: Repeated measures, within factors."

As this is a one-way, repeated measures ANOVA, you can enter effect size, alpha, and power. Then, set number of groups to 1 (as there is no between-subjects factor) and the number of measurements to the number of levels of the IV. The corr among rep measures is the estimated correlation between your measures (e.g., how well can you predict a participant's time 2 score if you know their time 1 score). This can be hard to estimate, but it's a good exercise to think about how large you expect these individual differences to be. The larger the correlation among measures, the more you gain by using a within-subjects design.
