# Statistics Reference

## One-Sample _z_-Test

[Video: The one-sample Z-test](https://youtu.be/TE61rB6ajTY)

### Definition

The one-sample z-test tests the null hypothesis that a mean is equivalent to the mean of a known population.

### Test Statistic

The test statistic is $z$, which measures the distance between two means. In this case, one mean is from our sample and the other mean is a known constant. The sampling distribution of $z$ is the normal distribution with a standard deviation defined by the formula for standard error ($\sigma_{\bar{X}}$).

$z = \frac{\bar{X}-\mu}{\sigma_{\bar{X}}} = \frac{\bar{X}-\mu_{hyp}}{\frac{\sigma}{\sqrt{N}}}$

```{r znull, fig.cap="The null hypothesis distribution of $z$", echo=FALSE}

library(ggplot2)
ggplot(NULL, aes(c(-4,4))) +
  geom_area(stat = "function", fun = dnorm, xlim = c(-4, 4), alpha = 0.75,fill=alpha("grey",0)) 

```

### Assumptions & Required Data

- 1 variable measured using a quantitative, continuous scale
- The variable was measured for a sample that was taken randomly, with replacement, from a population
- The normality assumption, meaning at least one of these:
  - $N \ge 30$
  - The variable is normally distributed in the population
- The population mean, $\mu$, is known.
- The population standard deviation, $\sigma$, is known

### When to use it

Use a $z$-test when you are comparing a single sample mean to a known population parameter and can meet the assumptions.

If the population standard deviation is unknown, or if the normality assumption cannot be met, consider a $t$-test.

### Example

Imagine a high school has a graduation test with $M = .80$ with a standard deviation ($\sigma$) of $\sigma = .10$. A random sample of $N = 35$ students at the high school participate in an after-school program aimed at increasing performance on the graduation test. 

#### Data

The data are test scores from 35 students.

````{r zdata, echo=FALSE}
sample <-  round(rnorm(35, 0.89, 0.10), 2) # Generate a random, sample for this exercise
# cat(paste(sample), sep="\n")
sample
mean(sample)
````

The students in the program took the test and performed higher than the population average ($M=$````print(mean(sample))````). Is there evidence that the after school program is effective?

#### Hypotheses

Because researchers are interested in detecting higher performance on the test, a one-tailed test is used to increase statistical power. If, instead, researchers wanted to see if the sample had higher or lower performance, a two-tailed test should be used.

````
$H_0=\mu\le.80$

$H_a=\mu\gt.80$
````

#### Analysis

Set the alpha level. By convention, an alpha level of $\alpha = .05$ will be used. 

Assume the null hypothesis is true. Assuming the null hypothesis is true means that we need to determine the probability of obtaining a sample mean this distance from the population mean. We will determine this using the sampling distribution of the null hypothesis for $z$ (the normal distribution).

Unlike later statistical tests, R does not provide a built-in $z$-test. This is actually a feature, as it lets us demonstrate the steps in more detail.

The most challenging part is the function ````pnorm()````, which gives the area to the left of a score on the a standard normal distribution. By using the argument ````lower.tail = FALSE```, the function will give the area to the right of the score.


````{r zanalysis, echo=TRUE}

mu <- .80
sigma <- .10
n <- length(data)
z <- (mean(sample) - mu) / (sigma / sqrt(n))
z
p_value <- pnorm(z, mean = 0, sd = 1, lower.tail = FALSE) # gives area to the right of the score, which is the p-value
p_value

````

To visualize this result, we can graph the location of the test statistic in the sampling distribution, shading everything beyond the test statistic in one tail:

````{r zvisualization, echo=TRUE}

library(ggplot2)
ggplot(NULL, aes(c(-4,4))) +
  geom_area(stat = "function", fun = dnorm, xlim = c(-4, z), alpha = 0.5,fill=alpha("grey",0)) +
  geom_area(stat = "function", fun = dnorm, fill="blue", xlim = c(z, 4), alpha = 0.7)

````

The shaded area is well over 5 percent, showing visually that $p>\alpha$.

#### Decision

Because $p>{\alpha}$, the null hypothesis is retained and the results are inconclusive. These data do not provide evidence of effectiveness of the program.

#### Variations

- This was a one-tailed test on the right side of the distribution. The use of ````rnorm()```` would need to be adapted if the one-tailed test was on the left side of the distribution (to detect if scores were lower than the population). Simply omit ````lower.tail = FALSE```` to have ````rnorm()```` calculate from the left side (lower tail). 

- In a two-tailed test, the shading would need to be repeated on the left side, and the shaded area on both sides would need to be added together. You can save a step by knowing that each tail is always the same area. To convert this one-tailed p-value into a two-tailed p-value, you would need to double it, giving you a two-tailed p-value of ````{r, echo=FALSE} print(p_value*2)````. When doing a two-tailed test, check to make sure you are calculating in the correct tail; if your two-tailed test had a sample mean lower than the population mean, you would want to shade/calculate to the left.

- If $p < \alpha$, you would have rejected the null hypothesis and concluded that there was a difference between your sample mean and the population.


## Correlation

### Definition

A correlation analysis measures the strength and direction of a relationship between two variables. The hypothesis test for a correlation tests the null hypothesis that there is no linear relationship between two variables. This is also called a bivariate correlation (because it involves two variables) and the Pearson correlation coefficient.

### Test Statistic

The test statistic is actually a $t$ distribution calculated from the observed value of $r$, which measures the strength and direction of the relationship. The statistic $r$ has values $-1\le{r}\le 1$, with -1 indicating a perfect negative relationship and +1 indicating a perfect positive relationship. $r = 0$ indicates no relationship between the variables and is rarely observed to be exactly 0 in practice. To conduct a hypothesis test, $r$ is converted to a value of $t$ because this function of the sampling distribution of $r$ follows a $t$-distribution:

$t = r\sqrt{\frac{n-2}{1-r^2}}$

This function can be reversed as $r=\frac{t}{\sqrt{n-2+t^2}}$

```{r corrnull, fig.cap="The null hypothesis distribution of $t$ with values of df between 2 and 10. Notice how the curve is starting to converge at the higher values of df.", echo=FALSE}
curve(dt(x, df=10), from=-4, to=4)
curve(dt(x, df=2), from=-4, to=4, add=TRUE)
curve(dt(x, df=4), from=-4, to=4, add=TRUE)
curve(dt(x, df=6), from=-4, to=4, add=TRUE)
curve(dt(x, df=8), from=-4, to=4, add=TRUE)

```

The $t$ distribution is actually a family of distributions defined by degrees of freedom. **Degrees of freedom** is a concept that can be interpreted multiple ways. For now, it is sufficient to say that it is based on sample size. The value of degrees of freedom grows by 1 with each additional unit increased in the sample. In other words, the specific sampling distribution used in the hypothesis test depends on the sample size (and degrees of freedom).

### Assumptions & Required Data

For more detail, see \@ref(regressionassumptions).

- 2 quantitative variables (interval or ratio), or one quantitative variable with a dichotomous (two possible values) variable. The later version is called a point-biserial correlation and is mathematically the same as the Pearson correlation coefficient. Note that this procedure is not appropriate for ordinal variables, and Spearman's rank correlation coefficient should be used instead.
- Normality, meaning normal distribution of residuals.
- Linearity
- Homogeneity of variance (also called homoscedasiticty and equality of variance)
- No outliers


### When to use it

Use a correlation when you want to detect a linear relationship between two variables.

### Example

@Navarro2018 wanted to see if there was a relationship between hours slept in a night and their rating of grumpiness the next day.

#### Data

The data are two variables, one indicating sleep and the other indicating grumpiness. Data were collected for 100 nights.

````{r correldata, echo=FALSE}
rm(list=ls(all=TRUE))

load( "./data/parenthood.Rdata" )
head(parenthood,100)
````

#### Hypotheses

Given their often-exploratory use, correlations are typically conducted as two-tailed tests. However, a one-tailed correlation could be conducted if researchers predict a direction for the effect.

Hypotheses are always written with population parameters (since we are making hypotheses about truth in the population, not what we have observed in our sample). The population parameter corresponding to the test statistic $r$ is $\rho$ (rho). The null is that there is no relationship. The alternative hypothesis is that there is a relationship.

$H_0=\rho=0$

$H_a=\rho\ne0$

#### Analysis

Set the alpha level. By convention, an alpha level of $\alpha = .05$ will be used. 

Assume the null hypothesis is true. Assuming the null hypothesis is true means that we need to determine the probability of obtaining an effect size ($r$) this strong at our sample size through random sampling from a population with effect size $\rho=0$. We will determine this using the sampling distribution of the null hypothesis for $t$.

There are several ways to generate correlations in R:

- ````cor()```` will output just the correlation coefficient
- ````cor.test()```` will perform NHST and give a p-value

````{r corranalysis, echo=TRUE}

cor( x = parenthood$dan.sleep, y = parenthood$dan.grump )
cor.test(x = parenthood$dan.sleep, y = parenthood$dan.grump)
````

To visualize this result, we can graph a scatterplot, with observed values of one variable plotted against the observed value of the second variable:

````{r corrvisualization, echo=TRUE}

oneCorPlot <- function(x,y,...) {
		
		plot(x,y,pch=19,col=("black"),...)
		
	}
	
	oneCorPlot( parenthood$dan.sleep, parenthood$dan.grump, 
		xlab="Sleep (hours)", ylab="Grumpiness"
	)
````

The magnitude of r (it's value) indicates the strength of the relationship. The closer the value of $r$ to $\pm1$, the more closely the points hug a straight line. The line will have a positive slope if the relationship is positive and a negative slope if the relationship is negative.

#### Decision

Because $p<{\alpha}$, the null hypothesis is rejected and we conclude that there is a relationship between sleep and grumpiness. Further, because the value of $r$ is negative, the relationship between grumpiness and sleep is that higher amounts of sleep are associated with lower levels of grumpiness.

#### Variations

- If data are ordinal, Spearman's rank order correlation can be calculated by adding the argument method = "separman" using the following syntax:

```` cor( x, y, method = "spearman") ````


- In a two-tailed test, the shading would need to be repeated on the left side, and the shaded area on both sides would need to be added together. You can save a step by knowing that each tail is always the same area. To convert this one-tailed p-value into a two-tailed p-value, you would need to double it, giving you a two-tailed p-value of ````{r, echo=FALSE} print(p_value*2)````. When doing a two-tailed test, check to make sure you are calculating in the correct tail; if your two-tailed test had a sample mean lower than the population mean, you would want to shade/calculate to the left.

- If $p \ge \alpha$, you would have retained the null hypothesis and made no conclusion.

- You can calculate a **correlation matrix** that shows all possible bivariate correlations from a dataframe. Simply include the entire dataframe as the argument instead of two variables, like this:


````{r corrvariation, echo=TRUE}

cor( x = parenthood)

````

When using a correlation matrix, p-values are not interpretable because the probability of a type I error on one or more of these correlations is higher than .05 (because the alpha level of .05 is used on each one, and many tests are being conducted).

## Linear Regression

### Definition

Linear regression creates a linear model that can be used to predict an outcome variable from one or more predictor variables. The hypothesis test for a linear regression occurs in two stages. The first stage, called the **omnibus test**, is a hypothesis test of the entire model. If this test is significant, researchers can conclude that the collection of variables significantly predicts the outcome. Assuming significance, the second stage follows in which each predictor variable is tested with a against the null hypothesis that its regression coefficient is zero. 

### Test Statistic

The omnibus test uses an $F$-test to test the null hypothesis that there is no linear relationship between the outcome variable and the linear combination of the predictor variables. Or, that the variance explained by the predictor variables is no more than the variance explained by the mean. Tests of each coefficient use $t$-tests. The statistics $F$ and $t$ are related such that $t=\sqrt{F}$ and $F=t^2$.

$$
F =  \frac{\mbox{MS}_{model}}{\mbox{MS}_{residual}}
$$
$$
t = \frac{\hat{b}}{\mbox{SE}({\hat{b})}}
$$

As with $t$, $F$ is a family of distributions defined around two degrees of freedom.

```{r regressionnull, fig.cap="Graph of the sampling distribution of F used under Creative Commons License from Hartmann, K., Krois, J., Waske, B. (2018): E-Learning Project SOGA: Statistics and Geospatial Data Analysis. Department of Earth Sciences, Freie Universitaet Berlin.", echo=FALSE}
x <- rf(100000, df1 = 10, df2 = 20)
hist(x, 
     breaks = 'fd', 
     freq = FALSE, 
     xlim = c(0,3), 
     ylim = c(0,1),
     xlab = '', 
     main = 'Sampling Distribution of F(10,20)',
          cex.main=0.9)
curve(df(x, df1 = 10, df2 = 20), from = 0, to = 4, n = 5000, col= 'pink', lwd=2, add = T)

```

### Assumptions & Required Data

For more detail, see \@ref(regressionassumptions).

- A single quantitative outcome variable (interval or ratio), and one or more quantitative or  dichotomous (two possible values) variables. Note that this procedure is not appropriate for ordinal variables.
- Normality of residuals 
- Linearity
- Homogeneity of variance (also called homoscedasiticty and equality of variance)
- Uncorrelated predictors
- Residuals that are independent of each other
- No outliers

### When to use it

Use a regression when you want to formulate and test a linear model, including mediating and moderating variables. Regression can also be used to run ANOVAs.

### Example

The same data from the correlation guide will be used. @Navarro2018 wanted to see if  hours slept in a night and hours the baby slept in a night could predict their rating of grumpiness the next day.

#### Data

The data are three variables, one indicating hours slept (predictor 1), one indicating the hours slept by the baby (predictor 2), and the other (the outcome) indicating grumpiness. Data were collected for 100 nights.

````{r regressiondata, echo=FALSE}
rm(list=ls(all=TRUE))

load( "./data/parenthood.Rdata" )
head(parenthood,100)
````

#### Hypotheses

All $F$-tests have two-tailed, directionless hypotheses. Because the omnibus test uses the $F$-distribution, all regression omnibus tests are two-tailed.

The null is that the mean (which is the y-intercept, or $b_0$) predicts the outcome variable ($Y_i$) as well as the model. The alternative hypothesis is that the predictors add explained variance.

$$
H_0: Y_i = b_0 + \epsilon_i
$$
$$
H_a: Y_i = \left( \sum_{k=1}^K b_{k} X_{ik} \right) + b_0 + \epsilon_i
$$

#### Analysis

Set the alpha level. By convention, an alpha level of $\alpha = .05$ will be used. 

Assume the null hypothesis is true. Assuming the null hypothesis is true means that we need to determine the probability of obtaining an effect size (in this case, the predictive power of our model over the null model with only the mean, measured using $R^2$) this strong at our sample size through random sampling from a population with no effect (a null model). We will determine this using an analysis of variance using the sampling distribution of the null hypothesis for $F$.

````{r regressionnalysis, echo=TRUE}

regression.2 <- lm( formula = dan.grump ~ dan.sleep + baby.sleep,  
                     data = parenthood )
summary(regression.2)
confint(regression.2, level = 0.95)
````

The omnibus test can be found in the last row as F-statistic. Here, the omnibus test was significant, $F$(2, 97) = 215.2, $p$ < .001, $R^2=0.82$, suggesting that the model significantly predicts grumpiness.

While a multiple regression with more than one predictor cannot be visualized easily, you could generate [added variable plots](https://www.statology.org/plot-multiple-linear-regression-in-r/) for each predictor.

#### Decision

##### Omnibus Test

Because $p<{\alpha}$, the null hypothesis is rejected and we conclude that grumpiness can be predicted by the linear combination of hours slept and hours the baby slept. However, we do not know which variable(s) significantly explain variance. Because we have a significant omnibus test, we are now justified in testing the coefficients.

##### Test of Coefficients


$t$-tests are conducted for each coefficient in the ````summary()```` table. The hours slept coefficient was a significant predictor, $t$(97) = -16.17, $p$ < .001, 95% CI [-10.05, -7.85]. As hours slept increased, grumpiness decreased. The hours the baby slept was not a significant predictor, $t$(97) = -16.17, $p$ < .001, 95% CI [-0.53, 0.55]. 

Note that the degrees of freedom are the residual degrees of freedom (the second number). $F$ tests are always reported with the model degrees of freedom first (2) and the residual degrees of freedom second (97).

#### Variations

This guide will be updated to add assumption checking and diagnostics procedures for regression.
